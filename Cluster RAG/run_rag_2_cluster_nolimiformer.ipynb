{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "indexes = []\n",
    "GROUP_NUM = 1 # 1, 2, or 3\n",
    "\n",
    "for i in range(GROUP_NUM+1):\n",
    "    path_to_index = \".ragatouille/colbert/indexes/GROUP{}_cluster{}\".format(GROUP_NUM, i)\n",
    "    RAG = RAGPretrainedModel.from_index(path_to_index)\n",
    "    indexes.append(RAG.as_langchain_retriever(k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever, RetrieverLike, RetrieverOutputLike\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "\n",
    "\n",
    "    vectorstore : List[RetrieverLike]\n",
    "\n",
    "    def flatten_extend(self, matrix):\n",
    "        flat_list = []\n",
    "        for row in matrix:\n",
    "            flat_list.extend(row)\n",
    "        return flat_list\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "\n",
    "        all_docs = []\n",
    "        for i in self.vectorstore:\n",
    "            all_docs.append(i.get_relevant_documents(query, k=3))\n",
    "\n",
    "        all_docs = self.flatten_extend(all_docs)\n",
    "        return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customRetriever = CustomRetriever(vectorstore=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "n_gpu_layers = -1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# rag_prompt.messages\n",
    "prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use one sentence maximum and keep the answer CONCISE. Keep the answer CONCISE.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))\n",
    "rag_prompt.messages = [prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "retriever = customRetriever\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"FINALQUESTIONS.txt\", \"r\")\n",
    "questions = f.readlines()\n",
    "f.close()\n",
    "\n",
    "questions = [i.strip() for i in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "answer_file = \"2cluster_nolimiformer.txt\"\n",
    "\n",
    "\n",
    "f = open(answer_file, \"w\")\n",
    "f.close()\n",
    "\n",
    "answers = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "  response = qa_chain.invoke(questions[i])\n",
    "\n",
    "  f = open(answer_file, \"a\")\n",
    "  f.write(response.replace(\"\\n\", \"\") + \"\\n\")\n",
    "  f.close()\n",
    "  answers.append(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
