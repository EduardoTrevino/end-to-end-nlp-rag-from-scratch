{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade --quiet  langchain langchain-community langchainhub gpt4all chromadb \n",
    "# !pip3 install unstructured\n",
    "# !pip3 install sentence-transformers\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\"Web Scholar PDFs\", \"Data/About Scottie\", \"Data/Buggy News\", \"academic_calendars\", \"Data/history_of_cmu\", \"Data/history_of_scs\", \"Data/Kiltie Band\", \"Data/lti_faculty\", \"Data/lti_programs\", \"program_handbooks\", \"Data/Tartan Facts\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i in directories:\n",
    "    loader = DirectoryLoader(i, glob=\"*\", exclude=\"annotation.txt\", show_progress=True)\n",
    "    docs.append(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = []\n",
    "for i in docs:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    all_splits.append(text_splitter.split_documents(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# with open('noWebScholarSplitDocuments.pkl', \"wb\") as f:\n",
    "#     pkl.dump(all_splits[1:], f)\n",
    "\n",
    "\n",
    "with open('splitDocuments.pkl','rb') as f: \n",
    "  all_splits = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 2, 6\n",
    "\n",
    "preciseSplits = [all_splits[i] for i in [1,2,6]] #scotty, mascot, terrier, scottish\n",
    "nonPrecise = [all_splits[i] for i in range(len(all_splits)) if i not in [1,2,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# model_name = \"Salesforce/SFR-Embedding-Mistral\"\n",
    "# model_kwargs = {\"device\": \"cuda\"}\n",
    "# embedding_function = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# import chromadb.utils.embedding_functions as embedding_functions\n",
    "# huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "#     model_name=\"BAAI/llm-embedder\"\n",
    "# )\n",
    "\n",
    "model_name = \"BAAI/llm-embedder\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "\n",
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "for i in all_splits:\n",
    "    vectorstore = Chroma.from_documents(documents=i, embedding=embedding_function, persist_directory=\"llm-embedder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nonPrecise:\n",
    "    vectorstore = Chroma.from_documents(documents=i, embedding=embedding_function, persist_directory=\"nonPrecise/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(persist_directory=\"nonPrecise\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from langchain.retrievers import BM25Retriever\n",
    "\n",
    "with open('splitDocuments.pkl','rb') as f:\n",
    "  all_splits = pkl.load(f)\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(flatten_extend(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What is classification? What is its purpose and its impact in different institutional contexts? What can those impacts add up to in our material realities? Sorting Sexualities encourages us to ask these questions when we think about and describe sexuality. This slim but dense volume delves deeply into the ways that sexuality is seen and understood in different legal settings. In doing so, it leads the reader to confront the contradic- tion between our common social understanding of sexuality as', metadata={'source': 'Web Scholar PDFs/eea8803365fcbfdca8ba9ef0ec4e90cfd7620bf2.pdf'}),\n",
       " Document(page_content='In the experiment section, we aim to answer the following research questions: 1) What is the per- formance across various tasks within the English language? 2) What is the performance across dif- ferent domains within the same task? 3) What is the performance across different evaluation dimen- sions? 4) What is the performance at unseen tasks? 5) Given that LLaMA is predominantly trained in English texts, can it effectively evaluate generations in other languages? 6) Can we align the diagnostic', metadata={'source': 'Web Scholar PDFs/460609e217fd59eaa34f5e11a820661f8ec8d7b6.pdf'}),\n",
       " Document(page_content='As our final system is an encoder-decoder model (WavLM + mBART50), adapting the LM alone is less straightforward. We create pseudo ASR training data with ACL data on the transcript side. Specifically, we use our TTS model to synthesize speech from the ACL dev and test abstracts. As the amount of ACL abstract data is very limited (less than 100 sentences in total), we heavily upsampled them, so that they consist of 60% of the training data. As shown in the lower section of Table 6, this leads to', metadata={'source': 'Web Scholar PDFs/610d9958390ab83515d0d81e19f8e5264faf8e9b.pdf'}),\n",
       " Document(page_content='2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2) What is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1) John Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in The Breakfast Club.', metadata={'source': 'Web Scholar PDFs/88884b8806262a4095036041e3567d450dba39f7.pdf'})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retriever.get_relevant_documents(\"What is the purpose of the ACL 60/60 evaluation sets?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
