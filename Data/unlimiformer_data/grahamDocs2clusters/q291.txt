Question: What are the names of the 15.5B parameter models introduced by The BigCode community?

Context: In this tech report, we summarize the learnings of the BigCode community in developing the Santa models, a set of 1.1B-parameter models trained on the Java, JavaScript, and Python subsets of The Stack and evaluated on MultiPL-E (Cassano et al., 2022). We describe the ﬁrst steps of the commu- nity towards developing larger code models and report experiments to de-risk the model architecture and the data processing pipeline. Speciﬁcally, the contributions of this report can be summarized as
Using the ﬁndings from these experiments, we train a ﬁnal 1.1B parameter model, dubbed SantaCoder, on Python, JavaScript, and Java. This model obtains comparable or stronger performance than previous open-source multilingual models, InCoder-6.7B and CodeGen- Multi-2.7B, on code generation and inﬁlling tasks on the MultiPL-E benchmark for these three languages, despite being substantially smaller.

2 RELATED WORK
Findings We ﬁnd all BigCode Santa variants with 1.1B parameters to outperform the 6.7B In- Coder model (Fried et al., 2022), which we attribute to differences in the training datasets. Among BigCode models, variants trained on more Python perform better: The stars variant with 32% of Python in its training corpus outperforms the tokenizer fertility variant with only 28.5% of Python (see proportions in Table 3). The bfloat16 is the same as the no-ﬁm variant, except for the lat- ter being trained
10-708, Probabilistic Graphical Models

16-720, Computer Vision

17-631 Information Security, Privacy & Policy

17-781, Mobile and IoT Computing Services

4.4.1 Breadth Courses:  Human Language

11-611, Natural Language Processing

11-624, Human Language for Artificial Intelligence

11-711, Advanced NLP

11-722, Grammar Formalisms

11-724, Human Language for Artificial Intelligence

11-727, Computational Semantics for NLP

11-737, Multilingual NLP
11-663, Applied Machine Learning

11-747, Neural Networks for NLP

11-755, Machine Learning for Signal Processing

11-761, Language and Statistics

11-777, Multimodal Machine Learning  MIIS Graduate Student Handbook  Page 16

11-785, Introduction to Deep Learning

10-601, Introduction to Machine Learning (Master’s)

10-605, Machine Learning with Large Datasets

10-701, Introduction to Machine Learning (PhD)

10-707, Advanced Deep Learning

10-708, Probabilistic Graphical Models
15. Model checking, 1994 CMU professor Edmund Clarke had long stressed the importance of verifying computer hardware and software through a formal problem-solving technique called “model checking.” In 1994, his arguments gained new weight with the discovery that Intel’s amazing new Pentium chip made errors on certain math problems. Clarke would go on to receive the Turing Award for his role in the development of model checking.


Answer: 