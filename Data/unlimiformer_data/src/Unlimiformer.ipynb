{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/llm-embedder\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"llm-embedder\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the BERTScore achieved by BASS-a\n"
     ]
    }
   ],
   "source": [
    "res = vectorstore.similarity_search(\"What is the BERTScore achieved by BASS-adapt on the How-2 test set?\", k=10)\n",
    "p = \"Question: What is the BERTScore achieved by BASS-adapt on the How-2 test set?\\nContext: \"\n",
    "\n",
    "for i in res:\n",
    "  p += i.page_content + \"\\n\"\n",
    "\n",
    "p += \"\\n Answer: \"\n",
    "print(p[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ·····································\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import inspect\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import  Tuple, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "normal_repr = torch.Tensor.__repr__\n",
    "torch.Tensor.__repr__ = lambda self: f\"{self.shape}_{normal_repr(self)}\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BloomForCausalLM,\n",
    "    BloomTokenizerFast,\n",
    "    CTRLLMHeadModel,\n",
    "    CTRLTokenizer,\n",
    "    GenerationMixin,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPTJForCausalLM,\n",
    "    HfArgumentParser,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    OPTForCausalLM,\n",
    "    TransfoXLLMHeadModel,\n",
    "    TransfoXLTokenizer,\n",
    "    XLMTokenizer,\n",
    "    XLMWithLMHeadModel,\n",
    "    XLNetLMHeadModel,\n",
    "    XLNetTokenizer,\n",
    "    TextStreamer,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from unlimiformer import Unlimiformer\n",
    "from random_training_unlimiformer import RandomTrainingUnlimiformer\n",
    "\n",
    "@dataclass\n",
    "class UnlimiformerArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    test_unlimiformer: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"whether to use KNN.\"\n",
    "        },\n",
    "    )\n",
    "    unlimiformer_verbose: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"whether to print KNN intermediate predictions (mostly for debugging).\"\n",
    "        },\n",
    "    )\n",
    "    layer_begin: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"The layer to begin applying KNN to. KNN will be applied to layers[knn_layer_begin:layer_end]. \"\n",
    "                          \"By default, it will be applied to all layers: [0:None]]\"}, \n",
    "    )\n",
    "    layer_end: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The layer to end applying KNN to. KNN will be applied to layers[knn_layer_begin:layer_end]. \"\n",
    "                          \"By default, it will be applied to all layers: [0:None]]\"}, \n",
    "    )\n",
    "    unlimiformer_chunk_overlap: Optional[float] = field(\n",
    "        default=0.5,\n",
    "        metadata={\"help\": \"The fraction of overlap between input chunks\"},\n",
    "    )\n",
    "    unlimiformer_chunk_size: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The size of each input chunk\"},\n",
    "    )\n",
    "    unlimiformer_head_num: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The head to apply KNN to (if None, apply to all heads)\"},\n",
    "    )\n",
    "    unlimiformer_exclude: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"If True, prioritize the inputs that are **not** in the standard attention window.\"\n",
    "        },\n",
    "    )\n",
    "    random_unlimiformer_training: Optional[bool] = field(\n",
    "        default=False,\n",
    "    )\n",
    "    unlimiformer_training: Optional[bool] = field(\n",
    "        default=False,\n",
    "    )\n",
    "    index_devices: Optional[List[int]] = field(\n",
    "        default_factory=lambda: (0,),\n",
    "    )\n",
    "    datastore_device: Optional[int] = field(\n",
    "        default=0,\n",
    "    )\n",
    "    use_datastore: Optional[bool] = field(default=True)\n",
    "    flat_index: Optional[bool] = field(default=True)\n",
    "    test_datastore: Optional[bool] = field(default=False)\n",
    "    reconstruct_embeddings: Optional[bool] = field(default=False)\n",
    "    gpu_datastore: Optional[bool] = field(default=True)\n",
    "    gpu_index: Optional[bool] = field(default=True)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    \"ctrl\": (CTRLLMHeadModel, CTRLTokenizer),\n",
    "    \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    \"xlnet\": (XLNetLMHeadModel, XLNetTokenizer),\n",
    "    \"transfo-xl\": (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
    "    \"xlm\": (XLMWithLMHeadModel, XLMTokenizer),\n",
    "    \"gptj\": (GPTJForCausalLM, AutoTokenizer),\n",
    "    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n",
    "    \"llama\": (LlamaForCausalLM, LlamaTokenizer),\n",
    "    \"opt\": (OPTForCausalLM, GPT2Tokenizer),\n",
    "}\n",
    "\n",
    "# Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia\n",
    "# in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "# and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
    "PREFIX = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "#\n",
    "# Functions to prepare models' input\n",
    "#\n",
    "\n",
    "\n",
    "def prepare_ctrl_input(args, _, tokenizer, prompt_text):\n",
    "    if args.temperature > 0.7:\n",
    "        logger.info(\"CTRL typically works better with lower temperatures (and lower top_k).\")\n",
    "\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
    "    if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):\n",
    "        logger.info(\"WARNING! You are not starting your generation from a control code so you won't get good results\")\n",
    "    return prompt_text\n",
    "\n",
    "\n",
    "def prepare_xlm_input(args, model, tokenizer, prompt_text):\n",
    "    # kwargs = {\"language\": None, \"mask_token_id\": None}\n",
    "\n",
    "    # Set the language\n",
    "    use_lang_emb = hasattr(model.config, \"use_lang_emb\") and model.config.use_lang_emb\n",
    "    if hasattr(model.config, \"lang2id\") and use_lang_emb:\n",
    "        available_languages = model.config.lang2id.keys()\n",
    "        if args.xlm_language in available_languages:\n",
    "            language = args.xlm_language\n",
    "        else:\n",
    "            language = None\n",
    "            while language not in available_languages:\n",
    "                language = input(\"Using XLM. Select language in \" + str(list(available_languages)) + \" >>> \")\n",
    "\n",
    "        model.config.lang_id = model.config.lang2id[language]\n",
    "        # kwargs[\"language\"] = tokenizer.lang2id[language]\n",
    "\n",
    "    # TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers\n",
    "    # XLM masked-language modeling (MLM) models need masked token\n",
    "    # is_xlm_mlm = \"mlm\" in args.model_name_or_path\n",
    "    # if is_xlm_mlm:\n",
    "    #     kwargs[\"mask_token_id\"] = tokenizer.mask_token_id\n",
    "\n",
    "    return prompt_text\n",
    "\n",
    "\n",
    "def prepare_xlnet_input(args, _, tokenizer, prompt_text):\n",
    "    prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX\n",
    "    prompt_text = prefix + prompt_text\n",
    "    return prompt_text\n",
    "\n",
    "\n",
    "def prepare_transfoxl_input(args, _, tokenizer, prompt_text):\n",
    "    prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX\n",
    "    prompt_text = prefix + prompt_text\n",
    "    return prompt_text\n",
    "\n",
    "\n",
    "PREPROCESSING_FUNCTIONS = {\n",
    "    \"ctrl\": prepare_ctrl_input,\n",
    "    \"xlm\": prepare_xlm_input,\n",
    "    \"xlnet\": prepare_xlnet_input,\n",
    "    \"transfo-xl\": prepare_transfoxl_input,\n",
    "}\n",
    "\n",
    "\n",
    "def adjust_length_to_model(length, max_sequence_length):\n",
    "    if length < 0 and max_sequence_length > 0:\n",
    "        length = max_sequence_length\n",
    "    elif 0 < max_sequence_length < length:\n",
    "        length = max_sequence_length  # No generation bigger than model size\n",
    "    elif length < 0:\n",
    "        length = MAX_LENGTH  # avoid infinite loop\n",
    "    return length\n",
    "\n",
    "\n",
    "def sparse_model_config(model_config):\n",
    "    embedding_size = None\n",
    "    if hasattr(model_config, \"hidden_size\"):\n",
    "        embedding_size = model_config.hidden_size\n",
    "    elif hasattr(model_config, \"n_embed\"):\n",
    "        embedding_size = model_config.n_embed\n",
    "    elif hasattr(model_config, \"n_embd\"):\n",
    "        embedding_size = model_config.n_embd\n",
    "\n",
    "    num_head = None\n",
    "    if hasattr(model_config, \"num_attention_heads\"):\n",
    "        num_head = model_config.num_attention_heads\n",
    "    elif hasattr(model_config, \"n_head\"):\n",
    "        num_head = model_config.n_head\n",
    "\n",
    "    if embedding_size is None or num_head is None or num_head == 0:\n",
    "        raise ValueError(\"Check the model config\")\n",
    "\n",
    "    num_embedding_size_per_head = int(embedding_size / num_head)\n",
    "    if hasattr(model_config, \"n_layer\"):\n",
    "        num_layer = model_config.n_layer\n",
    "    elif hasattr(model_config, \"num_hidden_layers\"):\n",
    "        num_layer = model_config.num_hidden_layers\n",
    "    else:\n",
    "        raise ValueError(\"Number of hidden layers couldn't be determined from the model config\")\n",
    "\n",
    "    return num_layer, num_head, num_embedding_size_per_head\n",
    "\n",
    "\n",
    "def generate_past_key_values(model, batch_size, seq_len):\n",
    "    num_block_layers, num_attention_heads, num_embedding_size_per_head = sparse_model_config(model.config)\n",
    "    if model.config.model_type == \"bloom\":\n",
    "        past_key_values = tuple(\n",
    "            (\n",
    "                torch.empty(int(num_attention_heads * batch_size), num_embedding_size_per_head, seq_len)\n",
    "                .to(model.dtype)\n",
    "                .to(model.device),\n",
    "                torch.empty(int(num_attention_heads * batch_size), seq_len, num_embedding_size_per_head)\n",
    "                .to(model.dtype)\n",
    "                .to(model.device),\n",
    "            )\n",
    "            for _ in range(num_block_layers)\n",
    "        )\n",
    "    else:\n",
    "        past_key_values = tuple(\n",
    "            (\n",
    "                torch.empty(batch_size, num_attention_heads, seq_len, num_embedding_size_per_head)\n",
    "                .to(model.dtype)\n",
    "                .to(model.device),\n",
    "                torch.empty(batch_size, num_attention_heads, seq_len, num_embedding_size_per_head)\n",
    "                .to(model.dtype)\n",
    "                .to(model.device),\n",
    "            )\n",
    "            for _ in range(num_block_layers)\n",
    "        )\n",
    "    return past_key_values\n",
    "\n",
    "\n",
    "def prepare_jit_inputs(inputs, model, tokenizer):\n",
    "    batch_size = len(inputs)\n",
    "    dummy_input = tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\")\n",
    "    dummy_input = dummy_input.to(model.device)\n",
    "    if model.config.use_cache:\n",
    "        dummy_input[\"past_key_values\"] = generate_past_key_values(model, batch_size, 1)\n",
    "    dummy_input[\"attention_mask\"] = torch.cat(\n",
    "        [\n",
    "            torch.zeros(dummy_input[\"attention_mask\"].shape[0], 1)\n",
    "            .to(dummy_input[\"attention_mask\"].dtype)\n",
    "            .to(model.device),\n",
    "            dummy_input[\"attention_mask\"],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "    return dummy_input\n",
    "\n",
    "\n",
    "class _ModelFallbackWrapper(GenerationMixin):\n",
    "    __slots__ = (\"_optimized\", \"_default\")\n",
    "\n",
    "    def __init__(self, optimized, default):\n",
    "        self._optimized = optimized\n",
    "        self._default = default\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if kwargs[\"past_key_values\"] is None and self._default.config.use_cache:\n",
    "            kwargs[\"past_key_values\"] = generate_past_key_values(self._default, kwargs[\"input_ids\"].shape[0], 0)\n",
    "        kwargs.pop(\"position_ids\", None)\n",
    "        for k in list(kwargs.keys()):\n",
    "            if kwargs[k] is None or isinstance(kwargs[k], bool):\n",
    "                kwargs.pop(k)\n",
    "        outputs = self._optimized(**kwargs)\n",
    "        lm_logits = outputs[0]\n",
    "        past_key_values = outputs[1]\n",
    "        fixed_output = CausalLMOutputWithPast(\n",
    "            loss=None,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=past_key_values,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "        return fixed_output\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self._default, item)\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, inputs_embeds=None, use_cache=None, **kwargs\n",
    "    ):\n",
    "        return self._default.prepare_inputs_for_generation(\n",
    "            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, **kwargs\n",
    "        )\n",
    "\n",
    "    def _reorder_cache(\n",
    "        self, past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n",
    "    ) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n",
    "        [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n",
    "        beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return self._default._reorder_cache(past_key_values, beam_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb88c8a24b84491b6a7ff8dcd7192df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_type = \"llama\"\n",
    "model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "prefix = \"<s>[INST] <<SYS>>\\n You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use one sentence maximum and keep the answer CONCISE. Keep the answer CONCISE. \\n<</SYS>>\\n\\n\"\n",
    "prompt = \"CMU Holds a buggy race where people build carts and pushed them around campus.\"\n",
    "suffix = \" [/INST]\"\n",
    "test_unlimiformer=True\n",
    "fp16 = True\n",
    "length=200\n",
    "layer_begin=22\n",
    "index_devices=0\n",
    "datastore_device=0\n",
    "device=\"cuda\"\n",
    "n_gpu=torch.cuda.device_count()\n",
    "jit=False\n",
    "temperature=1.0\n",
    "repetition_penalty=1.0\n",
    "k=0\n",
    "p=0.9\n",
    "stream_output=False\n",
    "num_return_sequences=1\n",
    "seed = 42\n",
    "\n",
    "\n",
    "\n",
    "hf_parser = HfArgumentParser(UnlimiformerArguments)\n",
    "unlimiformer_args, unknown_unlimiformer_args = hf_parser.parse_known_args()\n",
    "\n",
    "# set_seed(42, n_gpu)\n",
    "\n",
    "\n",
    "try:\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "except KeyError:\n",
    "    raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model_kwargs = {}\n",
    "\n",
    "model = model_class.from_pretrained(model_name_or_path, **model_kwargs)\n",
    "\n",
    "if fp16:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "\n",
    "max_seq_length = getattr(model.config, \"max_position_embeddings\", 0)\n",
    "length = adjust_length_to_model(length, max_sequence_length=max_seq_length)\n",
    "# logger.info(args)\n",
    "\n",
    "if test_unlimiformer:\n",
    "    unlimiformer_kwargs = {\n",
    "        'layer_begin': unlimiformer_args.layer_begin,\n",
    "        'layer_end': unlimiformer_args.layer_end,\n",
    "        'unlimiformer_head_num': unlimiformer_args.unlimiformer_head_num,\n",
    "        'exclude_attention': unlimiformer_args.unlimiformer_exclude,\n",
    "        'chunk_overlap': unlimiformer_args.unlimiformer_chunk_overlap,\n",
    "        'model_encoder_max_len': unlimiformer_args.unlimiformer_chunk_size,\n",
    "        'verbose': unlimiformer_args.unlimiformer_verbose, 'tokenizer': tokenizer,\n",
    "        'unlimiformer_training': unlimiformer_args.unlimiformer_training,\n",
    "        'use_datastore': unlimiformer_args.use_datastore,\n",
    "        'flat_index': unlimiformer_args.flat_index,\n",
    "        'test_datastore': unlimiformer_args.test_datastore,\n",
    "        'reconstruct_embeddings': unlimiformer_args.reconstruct_embeddings,\n",
    "        'gpu_datastore': unlimiformer_args.gpu_datastore,\n",
    "        'gpu_index': unlimiformer_args.gpu_index,\n",
    "        'index_devices': unlimiformer_args.index_devices,\n",
    "        'datastore_device': unlimiformer_args.datastore_device,\n",
    "    }\n",
    "    if unlimiformer_args.random_unlimiformer_training:\n",
    "        model = RandomTrainingUnlimiformer.convert_model(model, **unlimiformer_kwargs)\n",
    "    else:\n",
    "        model = Unlimiformer.convert_model(model, **unlimiformer_kwargs)\n",
    "\n",
    "# Check if prompt_text is a valid file name:\n",
    "\n",
    "    # prefix = args.prefix if args.prefix else args.padding_text\n",
    "requires_preprocessing = model_type in PREPROCESSING_FUNCTIONS.keys()\n",
    "if requires_preprocessing:\n",
    "    prepare_input = PREPROCESSING_FUNCTIONS.get(model_type)\n",
    "    preprocessed_prompt_text = prepare_input(model, tokenizer, prompt_text)\n",
    "\n",
    "    if model.__class__.__name__ in [\"TransfoXLLMHeadModel\"]:\n",
    "        tokenizer_kwargs = {\"add_space_before_punct_symbol\": True}\n",
    "    else:\n",
    "        tokenizer_kwargs = {}\n",
    "\n",
    "    encoded_prompt = tokenizer.encode(\n",
    "        preprocessed_prompt_text, add_special_tokens=False, return_tensors=\"pt\", **tokenizer_kwargs\n",
    "    )\n",
    "else:\n",
    "    prompt_text = f'{prefix}{prompt_text}{suffix}'\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "if not test_unlimiformer:\n",
    "    encoded_prompt = encoded_prompt[:, -2048:]\n",
    "encoded_prompt = encoded_prompt.to(device)\n",
    "\n",
    "if encoded_prompt.size()[-1] == 0:\n",
    "    input_ids = None\n",
    "else:\n",
    "    input_ids = encoded_prompt\n",
    "\n",
    "if jit:\n",
    "    jit_input_texts = [\"enable jit\"]\n",
    "    jit_inputs = prepare_jit_inputs(jit_input_texts, model, tokenizer)\n",
    "    torch._C._jit_set_texpr_fuser_enabled(False)\n",
    "    model.config.return_dict = False\n",
    "    if hasattr(model, \"forward\"):\n",
    "        sig = inspect.signature(model.forward)\n",
    "    else:\n",
    "        sig = inspect.signature(model.__call__)\n",
    "    jit_inputs = tuple(jit_inputs[key] for key in sig.parameters if jit_inputs.get(key, None) is not None)\n",
    "    traced_model = torch.jit.trace(model, jit_inputs, strict=False)\n",
    "    traced_model = torch.jit.freeze(traced_model.eval())\n",
    "    traced_model(*jit_inputs)\n",
    "    traced_model(*jit_inputs)\n",
    "\n",
    "    model = _ModelFallbackWrapper(traced_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2024 17:51:22 - INFO - Unlimiformer - Encoding 0 to 265 out of 265\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m output_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_length=args.length + len(encoded_prompt[0]),\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTextStreamer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Remove the batch dimension when returning multiple sequences\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output_sequences\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/unlimiformer/src/unlimiformer.py:520\u001b[0m, in \u001b[0;36mUnlimiformer.pre_generate_hook\u001b[0;34m(self, input_ids, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    519\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(input_ids)\n\u001b[0;32m--> 520\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/Projects/unlimiformer/src/unlimiformer.py:382\u001b[0m, in \u001b[0;36mUnlimiformer.reset_memory\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    380\u001b[0m chunk_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, context_start_ind:context_end_ind]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 382\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_labels\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# , return_dict=True, output_hidden_states=True)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_datastore:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# TODO: verify with BART as well\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# hidden_states_to_index = [hidden_states.encoder_last_hidden_state] # list of length 1 of (batch, chunked_source_len, dim)\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     hidden_states_to_index \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    387\u001b[0m         layer_capturer\u001b[38;5;241m.\u001b[39mcaptured \u001b[38;5;28;01mfor\u001b[39;00m layer_capturer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_capturer\n\u001b[1;32m    388\u001b[0m     ] \n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/unlimiformer/src/unlimiformer.py:553\u001b[0m, in \u001b[0;36mUnlimiformer.pre_forward_hook\u001b[0;34m(self, input_ids, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerated_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerated_input_ids, kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 553\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_forward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_first_test_decoding_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1176\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1173\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1019\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1009\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1010\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         cache_position,\n\u001b[1;32m   1017\u001b[0m     )\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1019\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:740\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/unlimiformer/src/unlimiformer.py:577\u001b[0m, in \u001b[0;36mUnlimiformer.create_cross_attn_pre_forward_hook.<locals>.attention_pre_forward_hook\u001b[0;34m(hidden_states, attention_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m     result \u001b[38;5;241m=\u001b[39m (attn_output, attn_weights_reshaped, past_key_value)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_cross_attn_forward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;66;03m# Uri: this part adds the generated tokens to the prompt. \u001b[39;00m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# However it was commented out because currently we always keep the generated tokens in the attention window\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# if not self.is_encoder_decoder and not self.is_input_encoding_pass and \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;66;03m#             self.prompt_attention_mask, \u001b[39;00m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m#             torch.ones([self.prompt_attention_mask.shape[0], 1], dtype=self.prompt_attention_mask.dtype).to(self.device)], dim=-1)\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/unlimiformer/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:655\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(LLAMA_INPUTS_DOCSTRING)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;129m@replace_return_docstrings\u001b[39m(output_type\u001b[38;5;241m=\u001b[39mCausalLMOutputWithPast, config_class\u001b[38;5;241m=\u001b[39m_CONFIG_FOR_DOC)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    652\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    653\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m    654\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03m            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    ```python\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m    >>> from transformers import AutoTokenizer, LlamaForCausalLM\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m    >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m    >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03m    >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;124;03m    >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    >>> # Generate\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03m    >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m    \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n\u001b[1;32m    681\u001b[0m     output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    682\u001b[0m         output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    683\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'update'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output_sequences = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    # max_length=args.length + len(encoded_prompt[0]),\n",
    "    max_new_tokens=length,\n",
    "    temperature=temperature,\n",
    "    top_k=k,\n",
    "    top_p=p,\n",
    "    repetition_penalty=repetition_penalty,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True) if stream_output else None,\n",
    ")\n",
    "\n",
    "# Remove the batch dimension when returning multiple sequences\n",
    "if len(output_sequences.shape) > 2:\n",
    "    output_sequences.squeeze_()\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    print(f\"=== GENERATED SEQUENCE {generated_sequence_idx + 1} (input length: {input_ids.shape[-1]}) ===\")\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    # generated_sequence = generated_sequence[len(encoded_prompt[0]):] + tokenizer.encode(' <end_of_prompt> ') + generated_sequence[:len(encoded_prompt[0])]\n",
    "\n",
    "    # Decode text\n",
    "    # text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "    prompt_length = min(input_ids.shape[-1], model.unlimiformer.window_size()) if unlimiformer_args.test_unlimiformer else input_ids.shape[-1]\n",
    "    completion = tokenizer.decode(generated_sequence[prompt_length:])\n",
    "\n",
    "    # Remove all text after the stop token\n",
    "    # text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "\n",
    "    # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "    total_sequence = (\n",
    "        # prompt_text +\n",
    "        '|||' + completion\n",
    "    )\n",
    "\n",
    "    generated_sequences.append(total_sequence)\n",
    "    print(total_sequence)\n",
    "\n",
    "print(generated_sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlimiformer",
   "language": "python",
   "name": "unlimiformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
