{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/llm-embedder\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"llm-embedder\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the BERTScore achieved by BASS-a\n"
     ]
    }
   ],
   "source": [
    "res = vectorstore.similarity_search(\"What is the BERTScore achieved by BASS-adapt on the How-2 test set?\", k=10)\n",
    "p = \"Question: What is the BERTScore achieved by BASS-adapt on the How-2 test set?\\nContext: \"\n",
    "\n",
    "for i in res:\n",
    "  p += i.page_content + \"\\n\"\n",
    "\n",
    "p += \"\\n Answer: \"\n",
    "print(p[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ·····································\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/trevea/.local/lib/python3.7/site-packages/torch/__init__.py\", line 172, in _load_global_deps\n",
      "    ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n",
      "  File \"/apps/local/anaconda/3.7/lib/python3.7/ctypes/__init__.py\", line 364, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /home/trevea/.local/lib/python3.7/site-packages/torch/lib/../../nvidia/cublas/lib/libcublas.so.11: undefined symbol: cublasLtHSHMatmulAlgoInit, version libcublasLt.so.11\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"src/run_generation.py\", line 29, in <module>\n",
      "    import torch\n",
      "  File \"/home/trevea/.local/lib/python3.7/site-packages/torch/__init__.py\", line 217, in <module>\n",
      "    _load_global_deps()\n",
      "  File \"/home/trevea/.local/lib/python3.7/site-packages/torch/__init__.py\", line 178, in _load_global_deps\n",
      "    _preload_cuda_deps()\n",
      "  File \"/home/trevea/.local/lib/python3.7/site-packages/torch/__init__.py\", line 158, in _preload_cuda_deps\n",
      "    ctypes.CDLL(cublas_path)\n",
      "  File \"/apps/local/anaconda/3.7/lib/python3.7/ctypes/__init__.py\", line 364, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /home/trevea/.local/lib/python3.7/site-packages/nvidia/cublas/lib/libcublas.so.11: undefined symbol: cublasLtHSHMatmulAlgoInit, version libcublasLt.so.11\n"
     ]
    }
   ],
   "source": [
    "!python src/run_generation.py --model_type llama --model_name_or_path meta-llama/Llama-2-7b-chat-hf \\\n",
    "    --prefix \"<s>[INST] <<SYS>>\\n You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use one sentence maximum and keep the answer CONCISE. Keep the answer CONCISE. \\n<</SYS>>\\n\\n\" \\\n",
    "    --prompt example_inputs/rag.txt \\\n",
    "    --suffix \" [/INST]\" --test_unlimiformer --fp16 --length 200 --layer_begin 22 \\\n",
    "    --index_devices 0 --datastore_device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unlimiformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     AutoTokenizer,\n\u001b[1;32m     20\u001b[0m     BloomForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     TextStreamer,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CausalLMOutputWithPast\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munlimiformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Unlimiformer\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom_training_unlimiformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomTrainingUnlimiformer\n\u001b[1;32m     47\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUnlimiformerArguments\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unlimiformer'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import inspect\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import  Tuple, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "normal_repr = torch.Tensor.__repr__\n",
    "torch.Tensor.__repr__ = lambda self: f\"{self.shape}_{normal_repr(self)}\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BloomForCausalLM,\n",
    "    BloomTokenizerFast,\n",
    "    CTRLLMHeadModel,\n",
    "    CTRLTokenizer,\n",
    "    GenerationMixin,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPTJForCausalLM,\n",
    "    HfArgumentParser,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    OPTForCausalLM,\n",
    "    TransfoXLLMHeadModel,\n",
    "    TransfoXLTokenizer,\n",
    "    XLMTokenizer,\n",
    "    XLMWithLMHeadModel,\n",
    "    XLNetLMHeadModel,\n",
    "    XLNetTokenizer,\n",
    "    TextStreamer,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from unlimiformer import Unlimiformer\n",
    "from random_training_unlimiformer import RandomTrainingUnlimiformer\n",
    "\n",
    "@dataclass\n",
    "class UnlimiformerArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    test_unlimiformer: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"whether to use KNN.\"\n",
    "        },\n",
    "    )\n",
    "    unlimiformer_verbose: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"whether to print KNN intermediate predictions (mostly for debugging).\"\n",
    "        },\n",
    "    )\n",
    "    layer_begin: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"The layer to begin applying KNN to. KNN will be applied to layers[knn_layer_begin:layer_end]. \"\n",
    "                          \"By default, it will be applied to all layers: [0:None]]\"},\n",
    "    )\n",
    "    layer_end: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The layer to end applying KNN to. KNN will be applied to layers[knn_layer_begin:layer_end]. \"\n",
    "                          \"By default, it will be applied to all layers: [0:None]]\"},\n",
    "    )\n",
    "    unlimiformer_chunk_overlap: Optional[float] = field(\n",
    "        default=0.5,\n",
    "        metadata={\"help\": \"The fraction of overlap between input chunks\"},\n",
    "    )\n",
    "    unlimiformer_chunk_size: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The size of each input chunk\"},\n",
    "    )\n",
    "    unlimiformer_head_num: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The head to apply KNN to (if None, apply to all heads)\"},\n",
    "    )\n",
    "    unlimiformer_exclude: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"If True, prioritize the inputs that are **not** in the standard attention window.\"\n",
    "        },\n",
    "    )\n",
    "    random_unlimiformer_training: Optional[bool] = field(\n",
    "        default=False,\n",
    "    )\n",
    "    unlimiformer_training: Optional[bool] = field(\n",
    "        default=False,\n",
    "    )\n",
    "    index_devices: Optional[List[int]] = field(\n",
    "        default_factory=lambda: (0,),\n",
    "    )\n",
    "    datastore_device: Optional[int] = field(\n",
    "        default=0,\n",
    "    )\n",
    "    use_datastore: Optional[bool] = field(default=True)\n",
    "    flat_index: Optional[bool] = field(default=True)\n",
    "    test_datastore: Optional[bool] = field(default=False)\n",
    "    reconstruct_embeddings: Optional[bool] = field(default=False)\n",
    "    gpu_datastore: Optional[bool] = field(default=True)\n",
    "    gpu_index: Optional[bool] = field(default=True)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    \"ctrl\": (CTRLLMHeadModel, CTRLTokenizer),\n",
    "    \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    \"xlnet\": (XLNetLMHeadModel, XLNetTokenizer),\n",
    "    \"transfo-xl\": (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
    "    \"xlm\": (XLMWithLMHeadModel, XLMTokenizer),\n",
    "    \"gptj\": (GPTJForCausalLM, AutoTokenizer),\n",
    "    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n",
    "    \"llama\": (LlamaForCausalLM, LlamaTokenizer),\n",
    "    \"opt\": (OPTForCausalLM, GPT2Tokenizer),\n",
    "}\n",
    "\n",
    "def set_seed(seed, n_gpu):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "#\n",
    "# Functions to prepare models' input\n",
    "#\n",
    "\n",
    "\n",
    "def prepare_ctrl_input(args, _, tokenizer, prompt_text):\n",
    "    if args.temperature > 0.7:\n",
    "        logger.info(\"CTRL typically works better with lower temperatures (and lower top_k).\")\n",
    "\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
    "    if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):\n",
    "        logger.info(\"WARNING! You are not starting your generation from a control code so you won't get good results\")\n",
    "    return prompt_text\n",
    "\n",
    "\n",
    "def prepare_xlm_input(args, model, tokenizer, prompt_text):\n",
    "    # kwargs = {\"language\": None, \"mask_token_id\": None}\n",
    "\n",
    "    # Set the language\n",
    "    use_lang_emb = hasattr(model.config, \"use_lang_emb\") and model.config.use_lang_emb\n",
    "    if hasattr(model.config, \"lang2id\") and use_lang_emb:\n",
    "        available_languages = model.config.lang2id.keys()\n",
    "        if args.xlm_language in available_languages:\n",
    "            language = args.xlm_language\n",
    "        else:\n",
    "            language = None\n",
    "            while language not in available_languages:\n",
    "                language = input(\"Using XLM. Select language in \" + str(list(available_languages)) + \" >>> \")\n",
    "\n",
    "        model.config.lang_id = model.config.lang2id[language]\n",
    "        # kwargs[\"language\"] = tokenizer.lang2id[language]\n",
    "\n",
    "    # TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers\n",
    "    # XLM masked-language modeling (MLM) models need masked token\n",
    "    # is_xlm_mlm = \"mlm\" in args.model_name_or_path\n",
    "    # if is_xlm_mlm:\n",
    "    #     kwargs[\"mask_token_id\"] = tokenizer.mask_token_id\n",
    "\n",
    "    return prompt_text\n",
    "\n",
    "\n",
    "def prepare_xlnet_input(args, _, tokenizer, prompt_text):\n",
    "    prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX\n",
    "    prompt_text = prefix + prompt_text\n",
    "    return prompt_text\n",
    "\n",
    "\n",
    "def prepare_transfoxl_input(args, _, tokenizer, prompt_text):\n",
    "    prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX\n",
    "    prompt_text = prefix + prompt_text\n",
    "    return prompt_text\n",
    "\n",
    "\n",
    "PREPROCESSING_FUNCTIONS = {\n",
    "    \"ctrl\": prepare_ctrl_input,\n",
    "    \"xlm\": prepare_xlm_input,\n",
    "    \"xlnet\": prepare_xlnet_input,\n",
    "    \"transfo-xl\": prepare_transfoxl_input,\n",
    "}\n",
    "\n",
    "\n",
    "def adjust_length_to_model(length, max_sequence_length):\n",
    "    if length < 0 and max_sequence_length > 0:\n",
    "        length = max_sequence_length\n",
    "    elif 0 < max_sequence_length < length:\n",
    "        length = max_sequence_length  # No generation bigger than model size\n",
    "    elif length < 0:\n",
    "        length = MAX_LENGTH  # avoid infinite loop\n",
    "    return length\n",
    "\n",
    "\n",
    "def sparse_model_config(model_config):\n",
    "    embedding_size = None\n",
    "    if hasattr(model_config, \"hidden_size\"):\n",
    "        embedding_size = model_config.hidden_size\n",
    "    elif hasattr(model_config, \"n_embed\"):\n",
    "        embedding_size = model_config.n_embed\n",
    "    elif hasattr(model_config, \"n_embd\"):\n",
    "        embedding_size = model_config.n_embd\n",
    "\n",
    "    num_head = None\n",
    "    if hasattr(model_config, \"num_attention_heads\"):\n",
    "        num_head = model_config.num_attention_heads\n",
    "    elif hasattr(model_config, \"n_head\"):\n",
    "        num_head = model_config.n_head\n",
    "\n",
    "    if embedding_size is None or num_head is None or num_head == 0:\n",
    "        raise ValueError(\"Check the model config\")\n",
    "\n",
    "    num_embedding_size_per_head = int(embedding_size / num_head)\n",
    "    if hasattr(model_config, \"n_layer\"):\n",
    "        num_layer = model_config.n_layer\n",
    "    elif hasattr(model_config, \"num_hidden_layers\"):\n",
    "        num_layer = model_config.num_hidden_layers\n",
    "    else:\n",
    "        raise ValueError(\"Number of hidden layers couldn't be determined from the model config\")\n",
    "\n",
    "    return num_layer, num_head, num_embedding_size_per_head\n",
    "\n",
    "\n",
    "def generate_past_key_values(model, batch_size, seq_len):\n",
    "    num_block_layers, num_attention_heads, num_embedding_size_per_head = sparse_model_config(model.config)\n",
    "    if model.config.model_type == \"bloom\":\n",
    "        past_key_values = tuple(\n",
    "            (\n",
    "                torch.empty(int(num_attention_heads * batch_size), num_embedding_size_per_head, seq_len)\n",
    "                .to(model.dtype)\n",
    "                .to(model.device),\n",
    "                torch.empty(int(num_attention_heads * batch_size), seq_len, num_embedding_size_per_head)\n",
    "                .to(model.dtype)\n",
    "                .to(model.device),\n",
    "            )\n",
    "            for _ in range(num_block_layers)\n",
    "        )\n",
    "    else:\n",
    "        past_key_values = tuple(\n",
    "            (\n",
    "                torch.empty(batch_size, num_attention_heads, seq_len, num_embedding_size_per_head)\n",
    "                .to(model.dtype)\n",
    "                .to(model.device),\n",
    "                torch.empty(batch_size, num_attention_heads, seq_len, num_embedding_size_per_head)\n",
    "                .to(model.dtype)\n",
    "                .to(model.device),\n",
    "            )\n",
    "            for _ in range(num_block_layers)\n",
    "        )\n",
    "    return past_key_values\n",
    "\n",
    "\n",
    "def prepare_jit_inputs(inputs, model, tokenizer):\n",
    "    batch_size = len(inputs)\n",
    "    dummy_input = tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\")\n",
    "    dummy_input = dummy_input.to(model.device)\n",
    "    if model.config.use_cache:\n",
    "        dummy_input[\"past_key_values\"] = generate_past_key_values(model, batch_size, 1)\n",
    "    dummy_input[\"attention_mask\"] = torch.cat(\n",
    "        [\n",
    "            torch.zeros(dummy_input[\"attention_mask\"].shape[0], 1)\n",
    "            .to(dummy_input[\"attention_mask\"].dtype)\n",
    "            .to(model.device),\n",
    "            dummy_input[\"attention_mask\"],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "    return dummy_input\n",
    "\n",
    "\n",
    "class _ModelFallbackWrapper(GenerationMixin):\n",
    "    __slots__ = (\"_optimized\", \"_default\")\n",
    "\n",
    "    def __init__(self, optimized, default):\n",
    "        self._optimized = optimized\n",
    "        self._default = default\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if kwargs[\"past_key_values\"] is None and self._default.config.use_cache:\n",
    "            kwargs[\"past_key_values\"] = generate_past_key_values(self._default, kwargs[\"input_ids\"].shape[0], 0)\n",
    "        kwargs.pop(\"position_ids\", None)\n",
    "        for k in list(kwargs.keys()):\n",
    "            if kwargs[k] is None or isinstance(kwargs[k], bool):\n",
    "                kwargs.pop(k)\n",
    "        outputs = self._optimized(**kwargs)\n",
    "        lm_logits = outputs[0]\n",
    "        past_key_values = outputs[1]\n",
    "        fixed_output = CausalLMOutputWithPast(\n",
    "            loss=None,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=past_key_values,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "        return fixed_output\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self._default, item)\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, inputs_embeds=None, use_cache=None, **kwargs\n",
    "    ):\n",
    "        return self._default.prepare_inputs_for_generation(\n",
    "            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, **kwargs\n",
    "        )\n",
    "\n",
    "    def _reorder_cache(\n",
    "        self, past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n",
    "    ) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n",
    "        [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n",
    "        beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return self._default._reorder_cache(past_key_values, beam_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlimiformer",
   "language": "python",
   "name": "unlimiformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
