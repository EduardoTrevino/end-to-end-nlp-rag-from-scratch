Question: What model does SYNTACC use for multi-accent speech synthesis?

Context: In the speech area, Wang et al. (2023a) treated text- to-speech synthesis as a language modeling task. They use audio codec codes as an intermediate rep- resentation and propose the first TTS framework with strong in-context learning capability. Subse- quently, VALLE-X (Zhang et al., 2023b) extend the idea to multi-lingual scenarios, demonstrating su- perior performance in zero-shot cross-lingual text- to-speech synthesis and zero-shot speech-to-speech translation tasks.
model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain sufÔ¨Åcient information for speech reconstruction.
Intelligibility of Synthetic Speech using WER from Pre- trained ASR: We computed the WER for synthetic speech generated by three different systems using the Whisper medium multilingual. This model is pre-trained on real speech and evaluated on synthetic speech. This setting of training / testing demonstrates the traditional way that speech synthesis evaluation is performed. This evaluation was per- formed on both the test-clean and test-other datasets from LibriTTS.

5. EXPERIMENTAL RESULTS
Electives (Choose Three): - Natural Language Processing (11-411) - Machine Learning for Text and Graph-based Mining (11-441) - Search Engines (11-442) - Speech Processing (11-492) - Machine Learning in Practice (11-344) - Advanced Natural Language Processing (11-711) - Machine Translation and Sequence-to-Sequence Models (11-731) - Multilingual Natural Language Processing (11-737) - Neural Networks for NLP (11-747) - Speech Recognition and Understanding (11-751) - Language and Statistics
Language Technologies Institute - Faculty Name: Shinji Watanabe Email: swatanab@andrew.cmu.edu Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing Phone: 412-268-3687

Language Technologies Institute

Faculty

Name: Sean Welleck
11-727: Computational Semantics for NLP ( only if the course project was done as a

group project)

11-731: Machine Translation

11-747: Neural Networks for NLP

11-751:  Speech Recognition

11-775:  Large -Scale Multimedia

11-776: Multimodal Affective Computing

11-777: Multimodal Machine Learning

11-785:  Deep Learning

11-797: Question Answering

Students may request to have other  LTI course s with a group engineering project component to

be added to this list.


Answer: 