Question: What model does SYNTACC use for multi-accent speech synthesis?

Context: In the speech area, Wang et al. (2023a) treated text- to-speech synthesis as a language modeling task. They use audio codec codes as an intermediate rep- resentation and propose the first TTS framework with strong in-context learning capability. Subse- quently, VALLE-X (Zhang et al., 2023b) extend the idea to multi-lingual scenarios, demonstrating su- perior performance in zero-shot cross-lingual text- to-speech synthesis and zero-shot speech-to-speech translation tasks.
model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain sufﬁcient information for speech reconstruction.
Intelligibility of Synthetic Speech using WER from Pre- trained ASR: We computed the WER for synthetic speech generated by three different systems using the Whisper medium multilingual. This model is pre-trained on real speech and evaluated on synthetic speech. This setting of training / testing demonstrates the traditional way that speech synthesis evaluation is performed. This evaluation was per- formed on both the test-clean and test-other datasets from LibriTTS.

5. EXPERIMENTAL RESULTS
Course number: 11868

Title: Large Language Model Systems

Units: 12.0

Section: A

Days: MW

Start: 05:00PM

End: 06:20PM

Room: POS A35

Locations: Pittsburgh, Pennsylvania

Instructors: Li

Spring 2024

Course number: 11877

Title: Advanced Topics in Multimodal Machine Learning

Units: VAR

Section: A

Days: TR

Start: 02:00PM

End: 03:20PM

Room: WEH 4709

Locations: Pittsburgh, Pennsylvania

Instructors: Liang, Fried

Spring 2024

Course number: 11891

Title: Neural Code Generation:
Spring 2024

Course number: 48660

Title: Advanced Synthesis Options Studio II: M.Arch

Units: 18.0

Section: A,E,B,F,G,C,D,Lec

Days: TR

Start: 01:00PM

End: 04:50PM

Room: CFA 200,MM 312

Locations: Pittsburgh, Pennsylvania

Instructors: Hayes,Bista,Cupkova,Damiani,Anklesaria,Garofalo,Lee,Bizon

Spring 2024

Course number: 48667

Title: Material Histories

Units: 9.0

Section: A

Days: MW

Start: 11:00AM

End: 12:20PM

Room: MM 409

Locations: Pittsburgh, Pennsylvania

Instructors: Torello
Days: T

Start: 10:00AM

End: 11:50AM

Room: MM 103

Locations: Pittsburgh, Pennsylvania

Instructors: Vavasis

Spring 2024

Course number: 48410

Title: Advanced Synthesis Options Studio II

Units: 18.0

Section: A,E,B,F,G,C,D,Lec

Days: TR

Start: 01:00PM

End: 04:50PM

Room: CFA 200,POS 146,MM 312

Locations: Pittsburgh, Pennsylvania

Instructors: Hayes,Bista,Cupkova,Arscott,Damiani,Garofalo,Lee,Bizon

Spring 2024

Course number: 48425

Title: EX-CHANGE: Exhibition & Publication in Practice
Beta’s new design was the winner of the first post-war Design Competition. The buggy, designed by Rick Saxton, was shaped like a bug with welded tubular steel. The real design battle seemed to be between Beta and PiKA, and the Tartan noted that PiKA’s buggy had a better appearance. However, Beta’s was structurally superior and that allowed it to take home the design trophy.
1948 – Barry Rowles (DTD Hill 5) works hard to get DTD (left) across the finish line first, edging out KapSig (right) for the trophy (from the 1949 Thistle). First “Broadcast”. 1948 is the first indication that we have of a “broadcast” on Raceday. According to the May 18, 1948 Tartan, a jeep patrolled the course and carried both the starter and a PA system, allowing the occupants of the vehicle to broadcast the race to the spectators on the course (as long as you were in range to hear the PA
Tartan, Heat 1 saw a very tight race between KapSig, DTD, and PiKA, with KapSig winning in 3:24.5, DTD finishing in 3:25.1, and PiKA finishing in 3:26.3. Those were the top 3 times and they advanced to the finals. Prelim Heat 2 paired Beta, SAE, and DU, and SAE won the heat by default, as Beta was DQ’d for a pushbar violation and DU was DQ’d for a Hill 4-5 transition violation (the Tartan reports that they “shoved the buggy into the last zone instead of pushing it”). Prelims Heat 3 was a battle
Electives (Choose Three): - Natural Language Processing (11-411) - Machine Learning for Text and Graph-based Mining (11-441) - Search Engines (11-442) - Speech Processing (11-492) - Machine Learning in Practice (11-344) - Advanced Natural Language Processing (11-711) - Machine Translation and Sequence-to-Sequence Models (11-731) - Multilingual Natural Language Processing (11-737) - Neural Networks for NLP (11-747) - Speech Recognition and Understanding (11-751) - Language and Statistics
Language Technologies Institute - Faculty Name: Shinji Watanabe Email: swatanab@andrew.cmu.edu Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing Phone: 412-268-3687

Language Technologies Institute

Faculty

Name: Sean Welleck
11-727: Computational Semantics for NLP ( only if the course project was done as a

group project)

11-731: Machine Translation

11-747: Neural Networks for NLP

11-751:  Speech Recognition

11-775:  Large -Scale Multimedia

11-776: Multimodal Affective Computing

11-777: Multimodal Machine Learning

11-785:  Deep Learning

11-797: Question Answering

Students may request to have other  LTI course s with a group engineering project component to

be added to this list.


Answer: 