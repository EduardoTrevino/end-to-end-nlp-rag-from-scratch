Question: In the KALE paper, what evaluation metrics were reported on MSMARCO?

Context: Evaluation Metrics On XQUAD, MLQA and MI-EVAL, we follow Liu et al. (2023) and Wang et al. (2023a) to use ChatGPT for generation quality evaluation. On XQUAD, MLQA, we also report exact-matching results in Appendix D. For translation tasks, we use COMET (Rei et al., 2020), BLEURT (Sellam et al., 2020) and sentence-piece BLEU (Papineni et al., 2002) as metrics7. More evaluation details can be referred to Appendix D.

5 MAIN RESULTS

5.1 RESULTS ON CROSS-LINGUAL INSTRUCTION-TUNING
Table 2: Ablation study of FiT5. The evaluation metric is MRR@10 on MS MARCO and NDCG@10 on TREC DL.

Model All layers (l = 1) Top-6 layers (l = 7) Top-3 layers (l = 10) Top-2 layers (l = 11) Top-1 layer (l = 12) No global attention

FiT5 (w/o feature) 41.23 42.49 42.79 42.95 42.78 41.49

FiT5 40.83 43.36 43.93 43.43 43.07 40.95

Table 3: Performance on MS MARCO with global at- tention started to introduce at top-k transformer layers. The evaluation metric is MRR@10.

5.1 Overall Performance
Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely
Title: M.S. Research Final Report

Units: 0.0

Section: A

Days: TBA

Start:

End:

Room: TBA

Locations: Pittsburgh, Pennsylvania

Instructors: Rakach

Summer 2024

Course number: 42990

Title: Ph.D. Thesis Research

Units: 3-48

Section: R

Days: TBA

Start:

End:

Room: TBA

Locations: Pittsburgh, Pennsylvania

Instructors: Rakach

Summer 2024

Course number: 42997

Title: Ph.D. Qualifying Examination

Units: 0.0

Section: A

Days: TBA

Start:

End:

Room: TBA
Locations: Pittsburgh, Pennsylvania

Instructors: Kaess

Spring 2024

Course number: 16695

Title: MSR Independent Study

Units: VAR

Section: A

Days: TBA

Start:

End:

Room: DNM DNM

Locations: Pittsburgh, Pennsylvania

Instructors: Apostolopoulos

Spring 2024

Course number: 16697

Title: Introduction to Robotics Business

Units: 9.0

Section: A

Days: MW

Start: 11:00AM

End: 12:20PM

Room: NSH 1305

Locations: Pittsburgh, Pennsylvania

Instructors: Batavia

Spring 2024
Course number: 05610

Title: User-Centered Research and Evaluation:

Units: 12.0

Section: A,E,B,C,D,Lec,

Days:  ,MW

Start: 12:30PM, ,02:00PM

End:  ,01:50PM,03:20PM

Room: WEH 4708,PH 226B,PH 225B,DH 1212, ,GHC 4101,PH 125B

Locations:  ,Pittsburgh, Pennsylvania

Instructors: Musuraca, Shen

Spring 2024

Course number: 05615

Title: Persuasive Design:

Units: 12.0

Section: A,

Days:  ,MW

Start:  ,02:00PM

End:  ,03:20PM

Room:  ,3SC 172

Locations:  ,Pittsburgh, Pennsylvania
Tartan, Heat 1 saw a very tight race between KapSig, DTD, and PiKA, with KapSig winning in 3:24.5, DTD finishing in 3:25.1, and PiKA finishing in 3:26.3. Those were the top 3 times and they advanced to the finals. Prelim Heat 2 paired Beta, SAE, and DU, and SAE won the heat by default, as Beta was DQ’d for a pushbar violation and DU was DQ’d for a Hill 4-5 transition violation (the Tartan reports that they “shoved the buggy into the last zone instead of pushing it”). Prelims Heat 3 was a battle
We don’t have many details of the race itself, but DTD took the win, just barely edging out KapSig, who was less than 1 second behind. DTD’s goggled driver was Dick Stanley, and the Hill 5 pusher working very hard was Barry Rowles. As a consolation prize, KapSig’s streamlined aluminum buggy (which looks just like a slightly larger version of today’s designs, with an extra wheel) took home the Design Comp trophy.
was pretty uneventful, which leads to a boring story. Prelims were run in 5 heats, with DTD, KapSig, and PiKA advancing to the Finals (DTD put up a Prelim time of 2:51). There were no crashes. In fact, the only thing of note that we know about is that Beta Sigma Rho was DQ’d for a pushbar violation (the DQ was not protested, as their time wasn’t fast enough to make the Finals anyway). Finals. The Finals was a very tight race (see photo right).
because they explain the program sequence, it s Core and Knowledge co urses requirements and

how students will meet the program learning outcomes.

10.7 End of Semester Evaluation

The MSAII program conducts an academic progress re view at the conclusion of each semester in

order to monitor individual student progress towa rds graduation. Should a student’s effort fall

below the acceptable level of academic perfor mance and/or fail to meet the standards
contributions; other relevant research, including competing approaches; some preliminary results; the work that still must be completed; evaluation metrics for that work; and a LTI Ph.D.  Graduate Student Handbook  Page 17

projected timeline for completion. Before presenting the proposal, the student will also
established by the MSAII program, the stud ent may be dismissed from the program.

After each academic progress revi ew, a student may receive one or  more letters, indicating the

result of the review. See Section 3.1, Co urse Grade and GPA Requirements, above.

At the end of the semester, the faculty evaluate s each student's academic progress. If a student

seems to be having trouble, th e faculty determines whether it believes that the student can


Answer: 