Question: What is the DAE achieved by the CRL-COM (D) system from the paper Improving Factuality of Abstractive Summarization via Contrastive Reward Learning, on the XSUM dataset?

Context: (2020) proposed an unsupervised abstractive summarization model that generates summaries directly from source documents by optimizing coverage, ﬂuency, and brevity using reinforcement learning (RL)-based rewards. Yadav et al. (2021) introduced two novel question-aware semantic rewards for abstractive question summarization: (1) question-type identiﬁcation and (2) question-focus recognition. They integrated these rewards into an encoder- decoder-based ProphetNet transformer model (Qi et al.
With the resulting plot shown in Figure 3, PICL outperforms Incre-RSA along both dimensions. In comparison with E-S, PICL achieves better dis- criminativeness with a loss in fluency. For E-S and Incre-RSA, the trade-off patterns are different from that under ALBEF (Figure 2). While optimizing for ALBEF accuracy consistently induces more disfluent generation, the optimal informativeness under human judgment is likely to be achieved with a moderate level of disfluency.
In the evaluation process, the participants were given the news article and three generated summaries from the MLE-only, MLE + RL model with ROUGE-L reward and the bi-encoder or cross-encoder reward. They were asked to rate each predicted summary on a scale of 1 (very bad) to 5 (very good) in terms of relevance (selection of important content from the source), consis- tency (the factual alignment between the summary and the summarized source), and ﬂuency (the quality of individual sentences)
Start: 06:30PM

End: 09:20PM

Room: HBH 1005

Locations: Pittsburgh, Pennsylvania

Instructors: Dausey

Spring 2024

Course number: 90475

Title: Methods of Policy Analysis: Future of Work

Units: 12.0

Section: A

Days: TR

Start: 05:00PM

End: 06:20PM

Room: HBH 2003

Locations: Pittsburgh, Pennsylvania

Instructors: Branstetter

Spring 2024

Course number: 90489

Title: Resilient & Sustainable Communities

Units: 12.0

Section: A

Days: W

Start: 06:30PM

End: 09:20PM

Room: HBH 1004
Title: Foundations of Computational Data Science II:

Units: 6.0

Section:  ,A4

Days: TBA,

Start: ,

End: ,

Room:  ,DNM DNM

Locations:  ,Pittsburgh, Pennsylvania

Instructors: Rose

Spring 2024

Course number: 11681

Title: AI Venture Studio:

Units: 12.0

Section: A,

Days:  ,F

Start:  ,02:00PM

End:  ,04:50PM

Room:  ,POS 152

Locations:  ,Pittsburgh, Pennsylvania

Instructors: Ammirati, Paulisick

Spring 2024

Course number: 11685

Title: Introduction to Deep Learning

Units: 12.0
Course number: 17802

Title: Societal Computing Pre-Thesis

Units: 6.0

Section: A,M,E,I,K,B,F,G,P,C,D,J,H,L

Days: TBA

Start:

End:

Room: DNM DNM

Locations: Pittsburgh, Pennsylvania

Instructors: Herbsleb,Bauer,Pfeffer,Fredrikson,Christin,Sadeh,Vasilescu,Kolter,Cranor,Agarwal,Breaux,Goel,Carley,Fang

Spring 2024

Course number: 17803

Title: Empirical Methods

Units: 12.0

Section: A

Days: TR

Start: 03:30PM

End: 04:50PM

Room: WEH 4708

Locations: Pittsburgh, Pennsylvania
the 3 teams qualifying for Finals, broke the post-war record. In the finals, it was another close battle, with DTD squeaking it out over PiKA. DTD’s winning time of 2:42.5 was fast enough to set the course record, and PiKA’s 2:43.5 was only ½ second behind the prior record. SAE’s Best Design. SAE took home the trophy for Design Comp with their sleek aluminum buggy. Unfortunately, we haven’t found a photo. [Ed. Update: Based on the buggies from 1950, the 1950 Thistle’s actual description of
Tartan, Heat 1 saw a very tight race between KapSig, DTD, and PiKA, with KapSig winning in 3:24.5, DTD finishing in 3:25.1, and PiKA finishing in 3:26.3. Those were the top 3 times and they advanced to the finals. Prelim Heat 2 paired Beta, SAE, and DU, and SAE won the heat by default, as Beta was DQ’d for a pushbar violation and DU was DQ’d for a Hill 4-5 transition violation (the Tartan reports that they “shoved the buggy into the last zone instead of pushing it”). Prelims Heat 3 was a battle
SAE’s buggy as a “sleek aluminum auto”, and DTD’s letter to the editor in 1951 complaining that if a team had used a “Buick grill” they would have placed in Design Comp, we’ve determined that Buggy #4 (2nd from the left in the Design Comp photo below) is actually SAE’s design winner. For a better look at the design, check out the 1950 photos, which is a similar style (though with different wheels, a different pushbar, and likely some other changes).] 1949 Photos. Here are photos from 1949.
Language Technologies Institute - Faculty Name: Daniel Fried Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI

Language Technologies Institute - Faculty Name: Anatole Gershman Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Research Areas: Information Extraction, Summarization and Question Answering Phone: 412-268-8259
of this handbook. 1.1 The MCDS Degree The MCDS Degree The Master of Computational Data Science (MCDS) degree is a professional Master of Science degree offered by the Language Technologies Institute (LTI), a department in the School of Computer Science at Carnegie Mellon University. The MCDS degree offers students with a Bachelor's degree the opportunity to improve their training with advanced study in Computer Science and Machine Learning. We cater to students with basic analytic skills and a
development programs to support graduate students as teaching assistants or instructors of

record during their time at Carnegie Mellon University and as future faculty members at other

institutions. Regardless of one's current or future teaching context an d duties, Eberly’s goal is

to disseminate evidence -based teaching strategies in ways that are accessible and actionable.

Programs and services include campus -wide Graduate Student Instructor Orientation events


Answer: 