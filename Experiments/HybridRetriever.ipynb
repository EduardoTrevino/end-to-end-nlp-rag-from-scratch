{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('splitDocuments.pkl','rb') as f:\n",
    "  all_splits = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(flatten_extend(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(persist_directory=\"all-MiniLM-L6-v2\", embedding_function=embedding_function)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='(b) How do you interpret the term “promising”? (c) What do you think the research community prioritized when you started? (d) What was the scope of the work that your research group did? (e) What was your relationship to computing resources at the start of your career? (f) What did your software workflow look like when you first started doing research? (Prompt:\\n\\nWhat tools, frameworks, libraries did you use?)', metadata={'source': 'Web Scholar PDFs/1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4.pdf'}),\n",
       " Document(page_content='eassignei=TBegin(6)ei+1=...=ej=TIntermediate(7)ThepredictionofBIOtagsismodeledasamulti-classclassificationproblemwiththeobjectiveasLEE=EDhXi−logpθ(ei|[c,v,ρ])i(8)DocumentClassificationWeusetheembeddingofthestarting[CLS]tokenfordocumentclassifica-tion.ThelogitsarepredictedwithanMLPheadontopofthe[CLS]embedding.Letlbethecorrectclass,theobjectiveisLDC=EDh−logpθ(l|[c,v,ρ])i(9)BAdditionalRelatedWorksB.1DatasetsSmallerdocumentdatasetsTheFormUnder-standinginNoisyScannedDocuments(FUNSDdataset(Jaumeetal.,', metadata={'source': 'Web Scholar PDFs/8ccda6de0223bcd897d5dc0efc8f33222a899d0d.pdf'}),\n",
       " Document(page_content='tal.,2022).Ourmodel,METRO-T0,isaT0modelpretrainedwithM', metadata={'source': 'Web Scholar PDFs/38aaf8a29df6deeff0bf64cc835d242a25b10337.pdf'}),\n",
       " Document(page_content='4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens?', metadata={'source': 'Web Scholar PDFs/17fbffb05fa14e21d1c506fd5f0f568b955fe983.pdf'}),\n",
       " Document(page_content='(e) What would you define as the start of your NLP research career? (e.g. start of PhD, research as\\n\\nan undergrad, etc). (Prompt: When was this?)\\n\\n2. I’d like to hear your thoughts on what the field was like near the beginning of your career.\\n\\n(a) When you started in your field, what did people generally think were the most promising\\n\\ndirections? (Prompt: do you agree?)', metadata={'source': 'Web Scholar PDFs/1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4.pdf'}),\n",
       " Document(page_content='Table 2 Decision rules from the literature.\\n\\nAuthor\\n\\nRule\\n\\nMCC\\n\\nCarnero Rosell et al. (2019)\\n\\n(𝑖 − 𝑧) > 1.2, (𝑧 − 𝑌 ) > 0.15, (𝑌𝐴𝐵 − 𝐽𝑉 𝑒𝑔𝑎) > 1.6, z< 22\\n\\n0.935\\n\\nBurningham et al. (2013)\\n\\n(𝑧 − 𝐽 )𝑉 𝑒𝑔𝑎 > 2.5, 𝐽 < 17.5\\n\\n0.921\\n\\n\\x00\\x1b\\x00\\x15\\x00\\x18\\x00\\x14\\x00\\x18\\x00\\x17\\x00\\x13\\x00\\x1b\\x00\\x14\\x00\\x16\\n\\n\\x00\\x19\\x00\\x13\\x00\\x13\\n\\n\\x00\\x1b\\x00\\x13\\x00\\x13\\n\\n\\x00)\\x00D\\x00O\\x00V\\x00H\\x007\\x00U\\x00X\\x00H\\x003\\x00U\\x00H\\x00G\\x00L\\x00F\\x00W\\x00H\\x00G\\x00\\x03\\x00&\\x00O\\x00D\\x00V\\x00V\\x00)\\x00D\\x00O\\x00V\\x00H\\x007\\x00U\\x00X\\x00H\\x00$\\x00F\\x00W\\x00X\\x00D\\x00O\\x00\\x03\\x00&\\x00O\\x00D\\x00V\\x00V\\x00\\x03\\n\\n\\x00\\x17\\x00\\x13\\x00\\x13\\n\\n\\x00\\x18\\x00\\x13\\x00\\x13\\n\\n\\x00\\x15\\x00\\x13\\x00\\x13\\n\\n\\x00\\x14\\x00\\x13\\x00\\x13\\n\\n\\x00\\x1a\\x00\\x13\\x00\\x13\\n\\n\\x00\\x16\\x00\\x13\\x00\\x13\\n\\n2.5\\n\\n1\\n\\n1\\n\\n2i [mag]\\n\\n10.0\\n\\n2\\n\\n5.0\\n\\nBD\\n\\n0.0\\n\\nother classes\\n\\n2.5\\n\\n3\\n\\n0\\n\\n5.0z-y\\n\\n7.5\\n\\n\\x00\\x17\\x00\\x13\\x00\\x13', metadata={'source': 'Web Scholar PDFs/8fe90d2b761248689659bad9cf61c65c53568cc9.pdf'}),\n",
       " Document(page_content='in Epi-2.', metadata={'source': 'Web Scholar PDFs/e0401ca2d4fd6d0ed55130a4a24b33ed90111479.pdf'}),\n",
       " Document(page_content='Second-order logic is the union of all classes of the second-order hierarchy, and similarly local second-order logic is the union of all classes of the local second-order hierarchy. The classes Σfo 1 will be referred to as the the existential fragments of second-order logic and local second-order logic, respectively.\\n\\n1 and Σlfo', metadata={'source': 'Web Scholar PDFs/ce2e66604a71390a3f8759bb57da68ac66e13280.pdf'})]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retriever.get_relevant_documents(\"What number do all of the LTI classes start with\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is buggy?\"\n",
    "vectorstore.similarity_search(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting small-text\n",
      "  Downloading small_text-1.3.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dill in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (0.3.8)\n",
      "Requirement already satisfied: scipy in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (1.2.2)\n",
      "Requirement already satisfied: tqdm in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (4.66.1)\n",
      "Requirement already satisfied: packaging in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (23.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from scikit-learn>=0.24.1->small-text) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from scikit-learn>=0.24.1->small-text) (3.2.0)\n",
      "Downloading small_text-1.3.3-py3-none-any.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: small-text\n",
      "Successfully installed small-text-1.3.3\n"
     ]
    }
   ],
   "source": [
    "# 1. test hybrid retriever\n",
    "# 2. llm-embedder + bge reranker\n",
    "# 3. filco context filtering\n",
    "\n",
    "\n",
    "!pip install small-text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from small_text import TransformersDataset\n",
    "\n",
    "transformer_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    transformer_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"FINALQUESTIONS.txt\")\n",
    "targets = f.readlines()\n",
    "target = [i.strip() for i in targets]\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open(\"types.txt\")\n",
    "labels = f.readlines()\n",
    "labels = [i.strip() for i in labels]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Academic Calendar',\n",
       " 'Buggy News',\n",
       " 'Faculty',\n",
       " 'History of CMU',\n",
       " 'History of Drama',\n",
       " 'History of SCS',\n",
       " 'Kiltie Band',\n",
       " 'Programs',\n",
       " 'Research',\n",
       " 'Tartan Facts'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTargets = []\n",
    "finalLabels = []\n",
    "\n",
    "for count, i in enumerate(labels):\n",
    "    if \"kiltie\" in targets[count].lower() or i == \"Kiltie Band\":\n",
    "    #     finalLabels.append(\"kiltie\")\n",
    "\n",
    "        continue\n",
    "    \n",
    "\n",
    "    if i == \"History of CMU\" or i == \"History of Drama\" or i == \"History of SCS\":\n",
    "        finalLabels.append(0)\n",
    "\n",
    "    if i == \"Programs\" or i == \"Faculty\":\n",
    "        finalLabels.append(1)\n",
    "\n",
    "    if i == \"Academic Calendar\":\n",
    "        finalLabels.append(2)\n",
    "\n",
    "    if i == \"Research\":\n",
    "        finalLabels.append(3)\n",
    "\n",
    "    if i == \"Tartan Facts\" or \"Buggy News\":\n",
    "        finalLabels.append(4)\n",
    "\n",
    "    finalTargets.append(targets[count])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages/small_text/utils/annotations.py:67: ExperimentalWarning: The function from_arrays is experimental and maybe subject to change soon.\n",
      "  warnings.warn(f'The {subject} {func_or_class.__name__} is experimental '\n"
     ]
    }
   ],
   "source": [
    "from small_text import TransformersDataset\n",
    "import numpy as np\n",
    "\n",
    "# target_labels = np.arange(num_classes)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "transformer_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    transformer_model_name\n",
    ")\n",
    "\n",
    "train = TransformersDataset.from_arrays(finalTargets,\n",
    "                                        finalLabels,\n",
    "                                        tokenizer,\n",
    "                                        max_length=60,\n",
    "                                        target_labels=len(set(finalTargets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409fea82d1204540b40616afcb267ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411747df27d8401d852c5a5499d2c4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d134619ac55945b0acfa13ab71a52a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc225abed4ce49c2aef39258be04786e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0013dc420206405fa4f08325fa9a7318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from small_text import (\n",
    "    PoolBasedActiveLearner,\n",
    "    PredictionEntropy,\n",
    "    TransformerBasedClassificationFactory,\n",
    "    TransformerModelArguments,\n",
    "    random_initialization_balanced\n",
    ")\n",
    "\n",
    "\n",
    "# simulates an initial labeling to warm-start the active learning process\n",
    "def initialize_active_learner(active_learner, y_train):\n",
    "\n",
    "    indices_initial = random_initialization_balanced(y_train, n_samples=20)\n",
    "    active_learner.initialize_data(indices_initial, y_train[indices_initial])\n",
    "\n",
    "    return indices_initial\n",
    "\n",
    "\n",
    "transformer_model = TransformerModelArguments(transformer_model_name)\n",
    "clf_factory = TransformerBasedClassificationFactory(transformer_model, \n",
    "                                                    len(set(finalTargets)), \n",
    "                                                    kwargs=dict({'device': 'cpu', \n",
    "                                                                 'mini_batch_size': 32,\n",
    "                                                                 'class_weight': 'balanced'\n",
    "                                                                }))\n",
    "query_strategy = PredictionEntropy()\n",
    "\n",
    "active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train)\n",
    "indices_labeled = initialize_active_learner(active_learner, train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Iteration #0 (40 samples)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39my[indices_queried]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Return the labels for the current query to the active learner.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mactive_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m indices_labeled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([indices_queried, indices_labeled])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/active_learner.py:243\u001b[0m, in \u001b[0;36mPoolBasedActiveLearner.update\u001b[0;34m(self, y, indices_validation)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignored\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m concatenate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, y)\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices_validation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_queried \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/active_learner.py:393\u001b[0m, in \u001b[0;36mPoolBasedActiveLearner._retrain\u001b[0;34m(self, indices_validation)\u001b[0m\n\u001b[1;32m    390\u001b[0m dataset\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices_validation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_labeled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:378\u001b[0m, in \u001b[0;36mTransformerBasedClassification.fit\u001b[0;34m(self, train_set, validation_set, weights, early_stopping, model_selection, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_class_weights(sub_train)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_default_criterion(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights_,\n\u001b[1;32m    376\u001b[0m                                              use_sample_weights\u001b[38;5;241m=\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_train_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_scheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:401\u001b[0m, in \u001b[0;36mTransformerBasedClassification._fit_main\u001b[0;34m(self, sub_train, sub_valid, weights, early_stopping, model_selection, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mget_tmp_dir_base()) \u001b[38;5;28;01mas\u001b[39;00m tmp_dir:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_model_selection(optimizer, model_selection)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:432\u001b[0m, in \u001b[0;36mTransformerBasedClassification._train\u001b[0;34m(self, sub_train, sub_valid, weights, early_stopping, model_selection, optimizer, scheduler, tmp_dir)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop:\n\u001b[1;32m    430\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m--> 432\u001b[0m     train_acc, train_loss, valid_acc, valid_loss, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43msub_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43msub_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     timedelta \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_epoch(epoch, timedelta, sub_train, sub_valid, train_acc, train_loss,\n\u001b[1;32m    445\u001b[0m                     valid_acc, valid_loss)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:468\u001b[0m, in \u001b[0;36mTransformerBasedClassification._train_loop_epoch\u001b[0;34m(self, num_epoch, sub_train, sub_valid, weights, early_stopping, model_selection, optimizer, scheduler, tmp_dir)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     validate_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m train_loss, train_acc, valid_loss, valid_acc, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop_process_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_acc, train_loss, valid_acc, valid_loss, stop\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:502\u001b[0m, in \u001b[0;36mTransformerBasedClassification._train_loop_process_batches\u001b[0;34m(self, num_epoch, sub_train_, sub_valid_, weights, early_stopping, model_selection, optimizer, scheduler, tmp_dir, validate_every)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, masks, \u001b[38;5;28mcls\u001b[39m, weight, \u001b[38;5;241m*\u001b[39m_) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_iter):\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop:\n\u001b[0;32m--> 502\u001b[0m         loss, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    505\u001b[0m         train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:560\u001b[0m, in \u001b[0;36mTransformerBasedClassification._train_single_batch\u001b[0;34m(self, x, masks, cls, weight, optimizer)\u001b[0m\n\u001b[1;32m    557\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m weight\n\u001b[1;32m    558\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 560\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    564\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # ...where each iteration consists of labelling 20 samples\n",
    "    indices_queried = active_learner.query(num_samples=20)\n",
    "\n",
    "    # Simulate user interaction here. Replace this for real-world usage.\n",
    "    y = train.y[indices_queried]\n",
    "\n",
    "    # Return the labels for the current query to the active learner.\n",
    "    active_learner.update(y)\n",
    "\n",
    "    indices_labeled = np.concatenate([indices_queried, indices_labeled])\n",
    "    \n",
    "    print('---------------')\n",
    "    print(f'Iteration #{i} ({len(indices_labeled)} samples)')\n",
    "    # results.append(evaluate(active_learner, train[indices_labeled], test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
