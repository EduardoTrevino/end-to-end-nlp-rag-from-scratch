{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace 'your_file_path.pkl' with the path to your .pkl file\n",
    "file_path = 'splitDocuments.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    contents = pickle.load(file)\n",
    "\n",
    "# Extracting page_content from each document object\n",
    "docs_texts = []\n",
    "for sublist in contents:\n",
    "    for doc in sublist:  # Assuming each element in sublist is a document object\n",
    "        docs_texts.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevea/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:58<00:00, 39.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"Salesforce/SFR-Embedding-Mistral\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn import DataParallel\n",
    "import numpy as np\n",
    "\n",
    "class MistralEmbedder:\n",
    "    def __init__(self, model_name=\"Salesforce/SFR-Embedding-Mistral\", device='cuda'):\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of text documents using the Mistral model.\n",
    "        \n",
    "        Parameters:\n",
    "        - texts (List[str]): A list of texts to embed.\n",
    "        \n",
    "        Returns:\n",
    "        - List of embeddings as numpy arrays.\n",
    "        \"\"\"\n",
    "        # Ensure texts is a list for batch processing\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Generate embeddings and return them as numpy arrays\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings\n",
    "    def embed_query(self, query):\n",
    "        \"\"\"\n",
    "        Generate an embedding for a single query string using the Mistral model.\n",
    "        \n",
    "        Parameters:\n",
    "        - query (str): The query string to embed.\n",
    "        \n",
    "        Returns:\n",
    "        - A numpy array representing the embedding of the query.\n",
    "        \"\"\"\n",
    "        # Generate the embedding for the query\n",
    "        embedding = self.model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "        # Convert numpy array to list if necessary\n",
    "        embedding_list = embedding.tolist() if isinstance(embedding, np.ndarray) else embedding\n",
    "\n",
    "        # Return the embedding\n",
    "        return embedding_list\n",
    "\n",
    "# Initialize the embedding wrapper\n",
    "mistral_embedder = MistralEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.433129 , -0.4557455, -1.6711234, ...,  4.778883 , -2.628141 ,\n",
       "        2.1035042], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_embedder.embed_query(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MistralEmbedder' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming mistral_embedder is your instance of MistralEmbedder\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Delete the model attribute from your instance\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmistral_embedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Clear the CUDA cache\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MistralEmbedder' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the current device to CUDA device 1\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "# Assuming mistral_embedder is your instance of MistralEmbedder\n",
    "# Delete the model attribute from your instance\n",
    "del mistral_embedder.model\n",
    "\n",
    "# Clear the CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Optionally, check the memory status\n",
    "print(torch.cuda.memory_summary(device=1, abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5272.34 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = docs_texts.copy()\n",
    "\n",
    "# Build the vector store with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=mistral_embedder, persist_directory=\"MistralEmbed/\")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore._embedding_function = mistral_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is LTI\"\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Learn at LTI The LTI's degree programs draw from a common set of courses and core skills, but emphasize different types of expertise that prepare you for a wide range of career options. All of our programs provide the hands-on experience and rigorous curriculum that are the hallmark of computer science at Carnegie Mellon. Program Type: Ph.D. Programs Program Name: Ph.D. in Language and Information Technology Overview: The Ph.D. in LTI focuses on developing the next generation of scientific and\"), Document(page_content=\"Learn at LTI The LTI's degree programs draw from a common set of courses and core skills, but emphasize different types of expertise that prepare you for a wide range of career options. All of our programs provide the hands-on experience and rigorous curriculum that are the hallmark of computer science at Carnegie Mellon. Program Type: Ph.D. Programs Program Name: Dual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership) Overview: The LTI offers a dual-degree Ph.D. in\"), Document(page_content=\"Learn at LTI The LTI's degree programs draw from a common set of courses and core skills, but emphasize different types of expertise that prepare you for a wide range of career options. All of our programs provide the hands-on experience and rigorous curriculum that are the hallmark of computer science at Carnegie Mellon. Program Type: Master's Programs Program Name: Master of Language Technologies Overview: The MLT program prepares students for a research career in academia or industry. In this\"), Document(page_content='The LTI may grant transfer credit or issue an exemption  for equivalent graduate courses\\n\\npreviously completed at another institution. This decision rests with the chair of the particular program. If a student is exempt  from a required course due to prior courses or experience, the')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "# rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# rag_prompt.messages\n",
    "# prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use one sentence maximum and keep the answer CONCISE. Keep the answer CONCISE.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))\n",
    "# rag_prompt.messages = [prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions:   0%|          | 0/79 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      27.84 ms /    78 runs   (    0.36 ms per token,  2801.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2809.75 ms /   107 tokens (   26.26 ms per token,    38.08 tokens per second)\n",
      "llama_print_timings:        eval time =    5852.29 ms /    77 runs   (   76.00 ms per token,    13.16 tokens per second)\n",
      "llama_print_timings:       total time =    8933.81 ms /   184 tokens\n",
      "Answering questions:   1%|▏         | 1/79 [00:09<11:42,  9.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       9.60 ms /    26 runs   (    0.37 ms per token,  2707.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1671.30 ms /    62 tokens (   26.96 ms per token,    37.10 tokens per second)\n",
      "llama_print_timings:        eval time =    1697.83 ms /    25 runs   (   67.91 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    3454.54 ms /    87 tokens\n",
      "Answering questions:   3%|▎         | 2/79 [00:12<07:25,  5.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      28.47 ms /    79 runs   (    0.36 ms per token,  2774.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1371.15 ms /    52 tokens (   26.37 ms per token,    37.92 tokens per second)\n",
      "llama_print_timings:        eval time =    8872.36 ms /    78 runs   (  113.75 ms per token,     8.79 tokens per second)\n",
      "llama_print_timings:       total time =   10513.01 ms /   130 tokens\n",
      "Answering questions:   4%|▍         | 3/79 [00:23<10:06,  7.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      26.54 ms /    74 runs   (    0.36 ms per token,  2788.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8397.18 ms /   392 tokens (   21.42 ms per token,    46.68 tokens per second)\n",
      "llama_print_timings:        eval time =    5763.27 ms /    73 runs   (   78.95 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:       total time =   14424.42 ms /   465 tokens\n",
      "Answering questions:   5%|▌         | 4/79 [00:37<13:11, 10.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      42.14 ms /   120 runs   (    0.35 ms per token,  2847.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11177.54 ms /   529 tokens (   21.13 ms per token,    47.33 tokens per second)\n",
      "llama_print_timings:        eval time =   15785.96 ms /   119 runs   (  132.66 ms per token,     7.54 tokens per second)\n",
      "llama_print_timings:       total time =   27402.84 ms /   648 tokens\n",
      "Answering questions:   6%|▋         | 5/79 [01:05<20:32, 16.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       7.21 ms /    20 runs   (    0.36 ms per token,  2773.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6201.58 ms /   270 tokens (   22.97 ms per token,    43.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1409.19 ms /    19 runs   (   74.17 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:       total time =    7684.78 ms /   289 tokens\n",
      "Answering questions:   8%|▊         | 6/79 [01:12<16:35, 13.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      51.43 ms /   139 runs   (    0.37 ms per token,  2702.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12328.22 ms /   577 tokens (   21.37 ms per token,    46.80 tokens per second)\n",
      "llama_print_timings:        eval time =   10972.01 ms /   138 runs   (   79.51 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =   23831.06 ms /   715 tokens\n",
      "Answering questions:   9%|▉         | 7/79 [01:36<20:23, 16.99s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      27.29 ms /    76 runs   (    0.36 ms per token,  2785.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10358.36 ms /   465 tokens (   22.28 ms per token,    44.89 tokens per second)\n",
      "llama_print_timings:        eval time =    5777.36 ms /    75 runs   (   77.03 ms per token,    12.98 tokens per second)\n",
      "llama_print_timings:       total time =   16401.58 ms /   540 tokens\n",
      "Answering questions:  10%|█         | 8/79 [01:53<19:55, 16.83s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      25.30 ms /    70 runs   (    0.36 ms per token,  2766.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8144.60 ms /   400 tokens (   20.36 ms per token,    49.11 tokens per second)\n",
      "llama_print_timings:        eval time =    5134.61 ms /    69 runs   (   74.41 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:       total time =   13536.38 ms /   469 tokens\n",
      "Answering questions:  11%|█▏        | 9/79 [02:06<18:27, 15.83s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      12.85 ms /    36 runs   (    0.36 ms per token,  2802.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9509.48 ms /   443 tokens (   21.47 ms per token,    46.59 tokens per second)\n",
      "llama_print_timings:        eval time =    2706.86 ms /    35 runs   (   77.34 ms per token,    12.93 tokens per second)\n",
      "llama_print_timings:       total time =   12340.35 ms /   478 tokens\n",
      "Answering questions:  13%|█▎        | 10/79 [02:19<16:59, 14.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       7.20 ms /    20 runs   (    0.36 ms per token,  2779.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12197.67 ms /   560 tokens (   21.78 ms per token,    45.91 tokens per second)\n",
      "llama_print_timings:        eval time =    3086.81 ms /    19 runs   (  162.46 ms per token,     6.16 tokens per second)\n",
      "llama_print_timings:       total time =   15361.93 ms /   579 tokens\n",
      "Answering questions:  14%|█▍        | 11/79 [02:34<16:58, 14.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       7.15 ms /    20 runs   (    0.36 ms per token,  2799.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5891.35 ms /   232 tokens (   25.39 ms per token,    39.38 tokens per second)\n",
      "llama_print_timings:        eval time =    2400.70 ms /    19 runs   (  126.35 ms per token,     7.91 tokens per second)\n",
      "llama_print_timings:       total time =    8363.40 ms /   251 tokens\n",
      "Answering questions:  15%|█▌        | 12/79 [02:43<14:30, 12.99s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      42.87 ms /   117 runs   (    0.37 ms per token,  2729.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10032.57 ms /   445 tokens (   22.55 ms per token,    44.36 tokens per second)\n",
      "llama_print_timings:        eval time =   18721.48 ms /   116 runs   (  161.39 ms per token,     6.20 tokens per second)\n",
      "llama_print_timings:       total time =   29226.64 ms /   561 tokens\n",
      "Answering questions:  16%|█▋        | 13/79 [03:12<19:43, 17.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      22.31 ms /    62 runs   (    0.36 ms per token,  2778.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3785.38 ms /   143 tokens (   26.47 ms per token,    37.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4653.08 ms /    61 runs   (   76.28 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:       total time =    8667.13 ms /   204 tokens\n",
      "Answering questions:  18%|█▊        | 14/79 [03:21<16:25, 15.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      33.06 ms /    93 runs   (    0.36 ms per token,  2812.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10645.28 ms /   498 tokens (   21.38 ms per token,    46.78 tokens per second)\n",
      "llama_print_timings:        eval time =    7059.81 ms /    92 runs   (   76.74 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:       total time =   18071.28 ms /   590 tokens\n",
      "Answering questions:  19%|█▉        | 15/79 [03:39<17:07, 16.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      11.79 ms /    33 runs   (    0.36 ms per token,  2798.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9581.16 ms /   464 tokens (   20.65 ms per token,    48.43 tokens per second)\n",
      "llama_print_timings:        eval time =    2427.52 ms /    32 runs   (   75.86 ms per token,    13.18 tokens per second)\n",
      "llama_print_timings:       total time =   12138.22 ms /   496 tokens\n",
      "Answering questions:  20%|██        | 16/79 [03:51<15:38, 14.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       7.05 ms /    20 runs   (    0.35 ms per token,  2837.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3940.15 ms /   153 tokens (   25.75 ms per token,    38.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1382.58 ms /    19 runs   (   72.77 ms per token,    13.74 tokens per second)\n",
      "llama_print_timings:       total time =    5401.97 ms /   172 tokens\n",
      "Answering questions:  22%|██▏       | 17/79 [03:57<12:28, 12.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       3.17 ms /     9 runs   (    0.35 ms per token,  2840.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4152.96 ms /   159 tokens (   26.12 ms per token,    38.29 tokens per second)\n",
      "llama_print_timings:        eval time =     593.86 ms /     8 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =    4779.38 ms /   167 tokens\n",
      "Answering questions:  23%|██▎       | 18/79 [04:01<10:03,  9.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      19.96 ms /    58 runs   (    0.34 ms per token,  2906.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7766.93 ms /   321 tokens (   24.20 ms per token,    41.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4280.44 ms /    57 runs   (   75.10 ms per token,    13.32 tokens per second)\n",
      "llama_print_timings:       total time =   12267.35 ms /   378 tokens\n",
      "Answering questions:  24%|██▍       | 19/79 [04:14<10:38, 10.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      54.70 ms /   154 runs   (    0.36 ms per token,  2815.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9793.68 ms /   460 tokens (   21.29 ms per token,    46.97 tokens per second)\n",
      "llama_print_timings:        eval time =   11620.71 ms /   153 runs   (   75.95 ms per token,    13.17 tokens per second)\n",
      "llama_print_timings:       total time =   21990.85 ms /   613 tokens\n",
      "Answering questions:  25%|██▌       | 20/79 [04:36<13:49, 14.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       7.30 ms /    19 runs   (    0.38 ms per token,  2601.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13750.13 ms /   621 tokens (   22.14 ms per token,    45.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2984.72 ms /    18 runs   (  165.82 ms per token,     6.03 tokens per second)\n",
      "llama_print_timings:       total time =   16826.96 ms /   639 tokens\n",
      "Answering questions:  27%|██▋       | 21/79 [04:53<14:25, 14.92s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      21.86 ms /    55 runs   (    0.40 ms per token,  2516.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4374.14 ms /   161 tokens (   27.17 ms per token,    36.81 tokens per second)\n",
      "llama_print_timings:        eval time =    8507.42 ms /    54 runs   (  157.54 ms per token,     6.35 tokens per second)\n",
      "llama_print_timings:       total time =   13122.59 ms /   215 tokens\n",
      "Answering questions:  28%|██▊       | 22/79 [05:06<13:41, 14.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      29.00 ms /    80 runs   (    0.36 ms per token,  2758.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3911.81 ms /   148 tokens (   26.43 ms per token,    37.83 tokens per second)\n",
      "llama_print_timings:        eval time =    8942.59 ms /    79 runs   (  113.20 ms per token,     8.83 tokens per second)\n",
      "llama_print_timings:       total time =   13163.42 ms /   227 tokens\n",
      "Answering questions:  29%|██▉       | 23/79 [05:19<13:07, 14.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      28.18 ms /    79 runs   (    0.36 ms per token,  2803.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1542.67 ms /    57 tokens (   27.06 ms per token,    36.95 tokens per second)\n",
      "llama_print_timings:        eval time =    5473.42 ms /    78 runs   (   70.17 ms per token,    14.25 tokens per second)\n",
      "llama_print_timings:       total time =    7317.74 ms /   135 tokens\n",
      "Answering questions:  30%|███       | 24/79 [05:27<11:03, 12.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      39.02 ms /   108 runs   (    0.36 ms per token,  2767.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9854.20 ms /   441 tokens (   22.35 ms per token,    44.75 tokens per second)\n",
      "llama_print_timings:        eval time =    8005.39 ms /   107 runs   (   74.82 ms per token,    13.37 tokens per second)\n",
      "llama_print_timings:       total time =   18311.46 ms /   548 tokens\n",
      "Answering questions:  32%|███▏      | 25/79 [05:45<12:33, 13.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       8.22 ms /    23 runs   (    0.36 ms per token,  2796.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8623.10 ms /   388 tokens (   22.22 ms per token,    45.00 tokens per second)\n",
      "llama_print_timings:        eval time =    1675.17 ms /    22 runs   (   76.14 ms per token,    13.13 tokens per second)\n",
      "llama_print_timings:       total time =   10392.61 ms /   410 tokens\n",
      "Answering questions:  33%|███▎      | 26/79 [05:55<11:24, 12.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      14.96 ms /    42 runs   (    0.36 ms per token,  2807.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8366.58 ms /   384 tokens (   21.79 ms per token,    45.90 tokens per second)\n",
      "llama_print_timings:        eval time =    3090.68 ms /    41 runs   (   75.38 ms per token,    13.27 tokens per second)\n",
      "llama_print_timings:       total time =   11630.55 ms /   425 tokens\n",
      "Answering questions:  34%|███▍      | 27/79 [06:07<10:52, 12.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      60.99 ms /   174 runs   (    0.35 ms per token,  2853.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12526.05 ms /   571 tokens (   21.94 ms per token,    45.59 tokens per second)\n",
      "llama_print_timings:        eval time =   24385.32 ms /   173 runs   (  140.96 ms per token,     7.09 tokens per second)\n",
      "llama_print_timings:       total time =   37607.49 ms /   744 tokens\n",
      "Answering questions:  35%|███▌      | 28/79 [06:45<17:04, 20.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       2.88 ms /     7 runs   (    0.41 ms per token,  2428.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14549.72 ms /   664 tokens (   21.91 ms per token,    45.64 tokens per second)\n",
      "llama_print_timings:        eval time =     498.88 ms /     6 runs   (   83.15 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   15097.68 ms /   670 tokens\n",
      "Answering questions:  37%|███▋      | 29/79 [07:00<15:30, 18.62s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      10.29 ms /    28 runs   (    0.37 ms per token,  2720.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10259.34 ms /   445 tokens (   23.05 ms per token,    43.38 tokens per second)\n",
      "llama_print_timings:        eval time =    2109.70 ms /    27 runs   (   78.14 ms per token,    12.80 tokens per second)\n",
      "llama_print_timings:       total time =   12494.01 ms /   472 tokens\n",
      "Answering questions:  38%|███▊      | 30/79 [07:13<13:43, 16.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      20.88 ms /    55 runs   (    0.38 ms per token,  2634.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12507.98 ms /   578 tokens (   21.64 ms per token,    46.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4277.91 ms /    54 runs   (   79.22 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =   16998.74 ms /   632 tokens\n",
      "Answering questions:  39%|███▉      | 31/79 [07:30<13:30, 16.89s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      42.48 ms /   122 runs   (    0.35 ms per token,  2872.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10239.01 ms /   505 tokens (   20.28 ms per token,    49.32 tokens per second)\n",
      "llama_print_timings:        eval time =    9295.50 ms /   121 runs   (   76.82 ms per token,    13.02 tokens per second)\n",
      "llama_print_timings:       total time =   19997.18 ms /   626 tokens\n",
      "Answering questions:  41%|████      | 32/79 [07:50<13:58, 17.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      35.51 ms /    99 runs   (    0.36 ms per token,  2788.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9630.28 ms /   472 tokens (   20.40 ms per token,    49.01 tokens per second)\n",
      "llama_print_timings:        eval time =    7605.79 ms /    98 runs   (   77.61 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =   17607.69 ms /   570 tokens\n",
      "Answering questions:  42%|████▏     | 33/79 [08:07<13:38, 17.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      41.01 ms /   116 runs   (    0.35 ms per token,  2828.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7389.38 ms /   305 tokens (   24.23 ms per token,    41.28 tokens per second)\n",
      "llama_print_timings:        eval time =    8490.41 ms /   115 runs   (   73.83 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:       total time =   16343.85 ms /   420 tokens\n",
      "Answering questions:  43%|████▎     | 34/79 [08:24<13:02, 17.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      11.86 ms /    33 runs   (    0.36 ms per token,  2781.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7238.31 ms /   310 tokens (   23.35 ms per token,    42.83 tokens per second)\n",
      "llama_print_timings:        eval time =    2333.44 ms /    32 runs   (   72.92 ms per token,    13.71 tokens per second)\n",
      "llama_print_timings:       total time =    9705.96 ms /   342 tokens\n",
      "Answering questions:  44%|████▍     | 35/79 [08:34<11:04, 15.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       6.98 ms /    19 runs   (    0.37 ms per token,  2722.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8580.21 ms /   400 tokens (   21.45 ms per token,    46.62 tokens per second)\n",
      "llama_print_timings:        eval time =    1561.32 ms /    18 runs   (   86.74 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =   10215.70 ms /   418 tokens\n",
      "Answering questions:  46%|████▌     | 36/79 [08:44<09:47, 13.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      14.12 ms /    40 runs   (    0.35 ms per token,  2832.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11581.71 ms /   525 tokens (   22.06 ms per token,    45.33 tokens per second)\n",
      "llama_print_timings:        eval time =    3911.67 ms /    39 runs   (  100.30 ms per token,     9.97 tokens per second)\n",
      "llama_print_timings:       total time =   15653.57 ms /   564 tokens\n",
      "Answering questions:  47%|████▋     | 37/79 [09:00<09:59, 14.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      43.68 ms /   123 runs   (    0.36 ms per token,  2816.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3777.70 ms /   139 tokens (   27.18 ms per token,    36.79 tokens per second)\n",
      "llama_print_timings:        eval time =    8995.29 ms /   122 runs   (   73.73 ms per token,    13.56 tokens per second)\n",
      "llama_print_timings:       total time =   13226.73 ms /   261 tokens\n",
      "Answering questions:  48%|████▊     | 38/79 [09:13<09:33, 13.99s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      17.62 ms /    50 runs   (    0.35 ms per token,  2837.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3456.70 ms /   128 tokens (   27.01 ms per token,    37.03 tokens per second)\n",
      "llama_print_timings:        eval time =    3616.77 ms /    49 runs   (   73.81 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:       total time =    7254.02 ms /   177 tokens\n",
      "Answering questions:  49%|████▉     | 39/79 [09:20<07:59, 11.99s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      32.24 ms /    90 runs   (    0.36 ms per token,  2791.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11193.20 ms /   518 tokens (   21.61 ms per token,    46.28 tokens per second)\n",
      "llama_print_timings:        eval time =    6880.11 ms /    89 runs   (   77.30 ms per token,    12.94 tokens per second)\n",
      "llama_print_timings:       total time =   18417.81 ms /   607 tokens\n",
      "Answering questions:  51%|█████     | 40/79 [09:39<09:04, 13.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      37.24 ms /   106 runs   (    0.35 ms per token,  2846.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9584.64 ms /   454 tokens (   21.11 ms per token,    47.37 tokens per second)\n",
      "llama_print_timings:        eval time =    8071.55 ms /   105 runs   (   76.87 ms per token,    13.01 tokens per second)\n",
      "llama_print_timings:       total time =   18053.04 ms /   559 tokens\n",
      "Answering questions:  52%|█████▏    | 41/79 [09:57<09:38, 15.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      10.74 ms /    30 runs   (    0.36 ms per token,  2793.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7214.18 ms /   311 tokens (   23.20 ms per token,    43.11 tokens per second)\n",
      "llama_print_timings:        eval time =    2211.71 ms /    29 runs   (   76.27 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:       total time =    9535.08 ms /   340 tokens\n",
      "Answering questions:  53%|█████▎    | 42/79 [10:07<08:20, 13.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       2.15 ms /     6 runs   (    0.36 ms per token,  2785.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7692.64 ms /   351 tokens (   21.92 ms per token,    45.63 tokens per second)\n",
      "llama_print_timings:        eval time =     373.84 ms /     5 runs   (   74.77 ms per token,    13.37 tokens per second)\n",
      "llama_print_timings:       total time =    8089.79 ms /   356 tokens\n",
      "Answering questions:  54%|█████▍    | 43/79 [10:15<07:09, 11.92s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       9.41 ms /    27 runs   (    0.35 ms per token,  2869.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2130.59 ms /    82 tokens (   25.98 ms per token,    38.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1859.84 ms /    26 runs   (   71.53 ms per token,    13.98 tokens per second)\n",
      "llama_print_timings:       total time =    4088.91 ms /   108 tokens\n",
      "Answering questions:  56%|█████▌    | 44/79 [10:19<05:35,  9.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      46.73 ms /   132 runs   (    0.35 ms per token,  2824.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10630.33 ms /   501 tokens (   21.22 ms per token,    47.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9900.22 ms /   131 runs   (   75.57 ms per token,    13.23 tokens per second)\n",
      "llama_print_timings:       total time =   21026.85 ms /   632 tokens\n",
      "Answering questions:  57%|█████▋    | 45/79 [10:40<07:23, 13.05s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      16.16 ms /    45 runs   (    0.36 ms per token,  2783.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6049.96 ms /   246 tokens (   24.59 ms per token,    40.66 tokens per second)\n",
      "llama_print_timings:        eval time =    3302.44 ms /    44 runs   (   75.06 ms per token,    13.32 tokens per second)\n",
      "llama_print_timings:       total time =    9518.37 ms /   290 tokens\n",
      "Answering questions:  58%|█████▊    | 46/79 [10:50<06:36, 12.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      31.80 ms /    91 runs   (    0.35 ms per token,  2862.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8451.55 ms /   392 tokens (   21.56 ms per token,    46.38 tokens per second)\n",
      "llama_print_timings:        eval time =    6742.35 ms /    90 runs   (   74.92 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:       total time =   15536.68 ms /   482 tokens\n",
      "Answering questions:  59%|█████▉    | 47/79 [11:05<06:58, 13.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      45.59 ms /   130 runs   (    0.35 ms per token,  2851.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10966.10 ms /   515 tokens (   21.29 ms per token,    46.96 tokens per second)\n",
      "llama_print_timings:        eval time =    9902.80 ms /   129 runs   (   76.77 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:       total time =   21363.39 ms /   644 tokens\n",
      "Answering questions:  61%|██████    | 48/79 [11:27<08:03, 15.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      30.56 ms /    87 runs   (    0.35 ms per token,  2847.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9656.19 ms /   448 tokens (   21.55 ms per token,    46.40 tokens per second)\n",
      "llama_print_timings:        eval time =    6235.92 ms /    86 runs   (   72.51 ms per token,    13.79 tokens per second)\n",
      "llama_print_timings:       total time =   16251.77 ms /   534 tokens\n",
      "Answering questions:  62%|██████▏   | 49/79 [11:43<07:54, 15.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      40.45 ms /   112 runs   (    0.36 ms per token,  2768.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5391.57 ms /   208 tokens (   25.92 ms per token,    38.58 tokens per second)\n",
      "llama_print_timings:        eval time =    8026.33 ms /   111 runs   (   72.31 ms per token,    13.83 tokens per second)\n",
      "llama_print_timings:       total time =   13867.16 ms /   319 tokens\n",
      "Answering questions:  63%|██████▎   | 50/79 [11:57<07:22, 15.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      21.91 ms /    61 runs   (    0.36 ms per token,  2783.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9133.34 ms /   395 tokens (   23.12 ms per token,    43.25 tokens per second)\n",
      "llama_print_timings:        eval time =    4591.41 ms /    60 runs   (   76.52 ms per token,    13.07 tokens per second)\n",
      "llama_print_timings:       total time =   13961.31 ms /   455 tokens\n",
      "Answering questions:  65%|██████▍   | 51/79 [12:11<06:56, 14.89s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      21.79 ms /    62 runs   (    0.35 ms per token,  2845.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5135.11 ms /   206 tokens (   24.93 ms per token,    40.12 tokens per second)\n",
      "llama_print_timings:        eval time =    4850.26 ms /    61 runs   (   79.51 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =   10227.22 ms /   267 tokens\n",
      "Answering questions:  66%|██████▌   | 52/79 [12:21<06:04, 13.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       2.50 ms /     7 runs   (    0.36 ms per token,  2801.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5346.68 ms /   219 tokens (   24.41 ms per token,    40.96 tokens per second)\n",
      "llama_print_timings:        eval time =     934.13 ms /     6 runs   (  155.69 ms per token,     6.42 tokens per second)\n",
      "llama_print_timings:       total time =    6307.32 ms /   225 tokens\n",
      "Answering questions:  67%|██████▋   | 53/79 [12:28<04:55, 11.37s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      10.62 ms /    30 runs   (    0.35 ms per token,  2825.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6560.42 ms /   307 tokens (   21.37 ms per token,    46.80 tokens per second)\n",
      "llama_print_timings:        eval time =    2214.26 ms /    29 runs   (   76.35 ms per token,    13.10 tokens per second)\n",
      "llama_print_timings:       total time =    8885.65 ms /   336 tokens\n",
      "Answering questions:  68%|██████▊   | 54/79 [12:37<04:26, 10.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      22.18 ms /    61 runs   (    0.36 ms per token,  2750.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5772.96 ms /   224 tokens (   25.77 ms per token,    38.80 tokens per second)\n",
      "llama_print_timings:        eval time =    4409.63 ms /    60 runs   (   73.49 ms per token,    13.61 tokens per second)\n",
      "llama_print_timings:       total time =   10432.82 ms /   284 tokens\n",
      "Answering questions:  70%|██████▉   | 55/79 [12:47<04:14, 10.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      20.88 ms /    59 runs   (    0.35 ms per token,  2825.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4832.40 ms /   187 tokens (   25.84 ms per token,    38.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4168.72 ms /    58 runs   (   71.87 ms per token,    13.91 tokens per second)\n",
      "llama_print_timings:       total time =    9242.58 ms /   245 tokens\n",
      "Answering questions:  71%|███████   | 56/79 [12:56<03:55, 10.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      25.13 ms /    72 runs   (    0.35 ms per token,  2864.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8784.46 ms /   380 tokens (   23.12 ms per token,    43.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5482.86 ms /    71 runs   (   77.22 ms per token,    12.95 tokens per second)\n",
      "llama_print_timings:       total time =   14544.62 ms /   451 tokens\n",
      "Answering questions:  72%|███████▏  | 57/79 [13:11<04:13, 11.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      11.41 ms /    32 runs   (    0.36 ms per token,  2805.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10347.84 ms /   482 tokens (   21.47 ms per token,    46.58 tokens per second)\n",
      "llama_print_timings:        eval time =    2392.27 ms /    31 runs   (   77.17 ms per token,    12.96 tokens per second)\n",
      "llama_print_timings:       total time =   12864.80 ms /   513 tokens\n",
      "Answering questions:  73%|███████▎  | 58/79 [13:24<04:11, 11.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      29.61 ms /    83 runs   (    0.36 ms per token,  2803.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4803.63 ms /   184 tokens (   26.11 ms per token,    38.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6152.79 ms /    82 runs   (   75.03 ms per token,    13.33 tokens per second)\n",
      "llama_print_timings:       total time =   11271.13 ms /   266 tokens\n",
      "Answering questions:  75%|███████▍  | 59/79 [13:35<03:55, 11.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      32.37 ms /    91 runs   (    0.36 ms per token,  2811.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13665.75 ms /   632 tokens (   21.62 ms per token,    46.25 tokens per second)\n",
      "llama_print_timings:        eval time =    7255.38 ms /    90 runs   (   80.62 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:       total time =   21274.46 ms /   722 tokens\n",
      "Answering questions:  76%|███████▌  | 60/79 [13:57<04:38, 14.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      28.85 ms /    82 runs   (    0.35 ms per token,  2842.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9277.45 ms /   422 tokens (   21.98 ms per token,    45.49 tokens per second)\n",
      "llama_print_timings:        eval time =    6208.86 ms /    81 runs   (   76.65 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:       total time =   15802.51 ms /   503 tokens\n",
      "Answering questions:  77%|███████▋  | 61/79 [14:13<04:30, 15.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      26.06 ms /    73 runs   (    0.36 ms per token,  2801.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6674.40 ms /   266 tokens (   25.09 ms per token,    39.85 tokens per second)\n",
      "llama_print_timings:        eval time =    5546.56 ms /    72 runs   (   77.04 ms per token,    12.98 tokens per second)\n",
      "llama_print_timings:       total time =   12498.27 ms /   338 tokens\n",
      "Answering questions:  78%|███████▊  | 62/79 [14:25<04:02, 14.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       6.46 ms /    18 runs   (    0.36 ms per token,  2785.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8943.14 ms /   414 tokens (   21.60 ms per token,    46.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1330.71 ms /    17 runs   (   78.28 ms per token,    12.78 tokens per second)\n",
      "llama_print_timings:       total time =   10342.39 ms /   431 tokens\n",
      "Answering questions:  80%|███████▉  | 63/79 [14:36<03:29, 13.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      17.05 ms /    48 runs   (    0.36 ms per token,  2814.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8341.24 ms /   370 tokens (   22.54 ms per token,    44.36 tokens per second)\n",
      "llama_print_timings:        eval time =    3637.03 ms /    47 runs   (   77.38 ms per token,    12.92 tokens per second)\n",
      "llama_print_timings:       total time =   12157.97 ms /   417 tokens\n",
      "Answering questions:  81%|████████  | 64/79 [14:48<03:12, 12.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      20.93 ms /    54 runs   (    0.39 ms per token,  2579.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11159.95 ms /   511 tokens (   21.84 ms per token,    45.79 tokens per second)\n",
      "llama_print_timings:        eval time =    6054.86 ms /    53 runs   (  114.24 ms per token,     8.75 tokens per second)\n",
      "llama_print_timings:       total time =   17472.28 ms /   564 tokens\n",
      "Answering questions:  82%|████████▏ | 65/79 [15:05<03:19, 14.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      16.48 ms /    46 runs   (    0.36 ms per token,  2791.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12881.66 ms /   592 tokens (   21.76 ms per token,    45.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3542.69 ms /    45 runs   (   78.73 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:       total time =   16618.10 ms /   637 tokens\n",
      "Answering questions:  84%|████████▎ | 66/79 [15:22<03:15, 15.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      16.24 ms /    45 runs   (    0.36 ms per token,  2770.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10836.36 ms /   493 tokens (   21.98 ms per token,    45.49 tokens per second)\n",
      "llama_print_timings:        eval time =    3437.88 ms /    44 runs   (   78.13 ms per token,    12.80 tokens per second)\n",
      "llama_print_timings:       total time =   14469.85 ms /   537 tokens\n",
      "Answering questions:  85%|████████▍ | 67/79 [15:37<02:58, 14.87s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      10.79 ms /    30 runs   (    0.36 ms per token,  2779.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12380.65 ms /   569 tokens (   21.76 ms per token,    45.96 tokens per second)\n",
      "llama_print_timings:        eval time =    2189.72 ms /    29 runs   (   75.51 ms per token,    13.24 tokens per second)\n",
      "llama_print_timings:       total time =   14699.69 ms /   598 tokens\n",
      "Answering questions:  86%|████████▌ | 68/79 [15:51<02:43, 14.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      17.62 ms /    49 runs   (    0.36 ms per token,  2781.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11969.70 ms /   548 tokens (   21.84 ms per token,    45.78 tokens per second)\n",
      "llama_print_timings:        eval time =    3615.57 ms /    48 runs   (   75.32 ms per token,    13.28 tokens per second)\n",
      "llama_print_timings:       total time =   15790.52 ms /   596 tokens\n",
      "Answering questions:  87%|████████▋ | 69/79 [16:07<02:31, 15.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       9.75 ms /    22 runs   (    0.44 ms per token,  2257.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9140.99 ms /   437 tokens (   20.92 ms per token,    47.81 tokens per second)\n",
      "llama_print_timings:        eval time =    1649.38 ms /    21 runs   (   78.54 ms per token,    12.73 tokens per second)\n",
      "llama_print_timings:       total time =   10899.57 ms /   458 tokens\n",
      "Answering questions:  89%|████████▊ | 70/79 [16:18<02:05, 13.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      13.24 ms /    37 runs   (    0.36 ms per token,  2794.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3906.05 ms /   148 tokens (   26.39 ms per token,    37.89 tokens per second)\n",
      "llama_print_timings:        eval time =    2735.86 ms /    36 runs   (   76.00 ms per token,    13.16 tokens per second)\n",
      "llama_print_timings:       total time =    6793.12 ms /   184 tokens\n",
      "Answering questions:  90%|████████▉ | 71/79 [16:25<01:34, 11.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      12.41 ms /    31 runs   (    0.40 ms per token,  2498.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10699.95 ms /   468 tokens (   22.86 ms per token,    43.74 tokens per second)\n",
      "llama_print_timings:        eval time =    5090.72 ms /    30 runs   (  169.69 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =   15949.19 ms /   498 tokens\n",
      "Answering questions:  91%|█████████ | 72/79 [16:41<01:31, 13.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      31.18 ms /    86 runs   (    0.36 ms per token,  2758.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7687.04 ms /   355 tokens (   21.65 ms per token,    46.18 tokens per second)\n",
      "llama_print_timings:        eval time =    6447.38 ms /    85 runs   (   75.85 ms per token,    13.18 tokens per second)\n",
      "llama_print_timings:       total time =   14498.26 ms /   440 tokens\n",
      "Answering questions:  92%|█████████▏| 73/79 [16:56<01:21, 13.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      30.47 ms /    84 runs   (    0.36 ms per token,  2757.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7103.66 ms /   292 tokens (   24.33 ms per token,    41.11 tokens per second)\n",
      "llama_print_timings:        eval time =    6040.84 ms /    83 runs   (   72.78 ms per token,    13.74 tokens per second)\n",
      "llama_print_timings:       total time =   13492.95 ms /   375 tokens\n",
      "Answering questions:  94%|█████████▎| 74/79 [17:09<01:07, 13.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      10.77 ms /    30 runs   (    0.36 ms per token,  2784.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9352.41 ms /   445 tokens (   21.02 ms per token,    47.58 tokens per second)\n",
      "llama_print_timings:        eval time =    2573.31 ms /    29 runs   (   88.73 ms per token,    11.27 tokens per second)\n",
      "llama_print_timings:       total time =   12047.84 ms /   474 tokens\n",
      "Answering questions:  95%|█████████▍| 75/79 [17:21<00:52, 13.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =       6.24 ms /    17 runs   (    0.37 ms per token,  2723.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6146.46 ms /   260 tokens (   23.64 ms per token,    42.30 tokens per second)\n",
      "llama_print_timings:        eval time =    1798.23 ms /    16 runs   (  112.39 ms per token,     8.90 tokens per second)\n",
      "llama_print_timings:       total time =    8038.48 ms /   276 tokens\n",
      "Answering questions:  96%|█████████▌| 76/79 [17:30<00:34, 11.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      15.28 ms /    36 runs   (    0.42 ms per token,  2356.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9198.76 ms /   389 tokens (   23.65 ms per token,    42.29 tokens per second)\n",
      "llama_print_timings:        eval time =    4543.33 ms /    35 runs   (  129.81 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:       total time =   13918.78 ms /   424 tokens\n",
      "Answering questions:  97%|█████████▋| 77/79 [17:44<00:24, 12.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      14.02 ms /    38 runs   (    0.37 ms per token,  2710.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12059.10 ms /   575 tokens (   20.97 ms per token,    47.68 tokens per second)\n",
      "llama_print_timings:        eval time =    3021.70 ms /    37 runs   (   81.67 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:       total time =   15256.49 ms /   612 tokens\n",
      "Answering questions:  99%|█████████▊| 78/79 [17:59<00:13, 13.23s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2809.91 ms\n",
      "llama_print_timings:      sample time =      36.25 ms /   100 runs   (    0.36 ms per token,  2758.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9028.09 ms /   438 tokens (   20.61 ms per token,    48.52 tokens per second)\n",
      "llama_print_timings:        eval time =    7718.07 ms /    99 runs   (   77.96 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   17164.88 ms /   537 tokens\n",
      "Answering questions: 100%|██████████| 79/79 [18:16<00:00, 13.88s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load questions from questions.txt\n",
    "with open(\"SubmissionData/test/questions.txt\", \"r\") as f:\n",
    "    questions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Output directory for answers\n",
    "output_dir = \"SubmissionData/system_outputs/\"\n",
    "answer_file = os.path.join(output_dir, \"MistralEmbed.txt\")\n",
    "\n",
    "# Run the question-answering loop and save answers\n",
    "answers = []\n",
    "with tqdm(total=len(questions), desc=\"Answering questions\") as progress_bar:\n",
    "    with open(answer_file, \"w\") as f:\n",
    "        for question in questions:\n",
    "            response = qa_chain.invoke(question)\n",
    "            f.write(response.replace(\"\\n\",\"\"), \"\\n\")\n",
    "            answers.append(response)\n",
    "            progress_bar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-rag",
   "language": "python",
   "name": "nlp-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
