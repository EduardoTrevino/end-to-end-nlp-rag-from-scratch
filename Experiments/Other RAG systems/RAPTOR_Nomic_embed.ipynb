{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NOMIC_API_KEY=nk-4H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.014556885,\n",
       " 0.042907715,\n",
       " -0.19152832,\n",
       " -0.026611328,\n",
       " 0.04434204,\n",
       " -0.021774292,\n",
       " 0.04977417,\n",
       " 0.025512695,\n",
       " 0.041381836,\n",
       " 0.0018663406,\n",
       " 0.022338867,\n",
       " 0.040771484,\n",
       " 0.055603027,\n",
       " -0.015525818,\n",
       " -0.028305054,\n",
       " 0.0072402954,\n",
       " 0.028625488,\n",
       " -0.09265137,\n",
       " -0.005065918,\n",
       " 0.006893158,\n",
       " -0.044281006,\n",
       " -0.06561279,\n",
       " -0.03036499,\n",
       " 0.066711426,\n",
       " 0.040863037,\n",
       " -0.016738892,\n",
       " -0.03265381,\n",
       " 0.018630981,\n",
       " -0.057159424,\n",
       " 0.044006348,\n",
       " 0.026428223,\n",
       " 0.0009665489,\n",
       " 0.008293152,\n",
       " -0.02734375,\n",
       " 0.0045661926,\n",
       " -0.004306793,\n",
       " 0.019805908,\n",
       " 0.044067383,\n",
       " 0.008934021,\n",
       " 0.032958984,\n",
       " 0.03186035,\n",
       " 0.0025367737,\n",
       " -0.0054473877,\n",
       " -0.065979004,\n",
       " 0.032409668,\n",
       " -0.023498535,\n",
       " 0.05596924,\n",
       " 0.015716553,\n",
       " 0.072509766,\n",
       " -0.044281006,\n",
       " -0.023040771,\n",
       " -0.0048294067,\n",
       " 0.027633667,\n",
       " -0.06201172,\n",
       " 0.076538086,\n",
       " 0.015930176,\n",
       " 0.0390625,\n",
       " 0.008224487,\n",
       " 0.013786316,\n",
       " 0.02178955,\n",
       " 0.12438965,\n",
       " 0.001045227,\n",
       " 0.013343811,\n",
       " 0.06173706,\n",
       " 0.0236969,\n",
       " -0.056121826,\n",
       " -0.033050537,\n",
       " 0.047058105,\n",
       " -0.026657104,\n",
       " -0.040130615,\n",
       " -0.0039787292,\n",
       " 0.024215698,\n",
       " 0.0057525635,\n",
       " 0.01713562,\n",
       " -0.010902405,\n",
       " -0.00082063675,\n",
       " -0.06555176,\n",
       " -0.015716553,\n",
       " 0.034606934,\n",
       " 0.011703491,\n",
       " 0.026321411,\n",
       " 5.2273273e-05,\n",
       " 0.03527832,\n",
       " -0.04650879,\n",
       " 0.008308411,\n",
       " 0.01361084,\n",
       " 0.02494812,\n",
       " -0.005207062,\n",
       " -0.028213501,\n",
       " 0.012786865,\n",
       " 0.011360168,\n",
       " 0.01713562,\n",
       " 0.040771484,\n",
       " 0.033233643,\n",
       " -0.034179688,\n",
       " 0.025344849,\n",
       " 0.039215088,\n",
       " -0.0022583008,\n",
       " -0.021148682,\n",
       " -0.047973633,\n",
       " -0.014701843,\n",
       " 0.009391785,\n",
       " -0.0143585205,\n",
       " -0.008979797,\n",
       " 0.08251953,\n",
       " 0.017349243,\n",
       " -0.0035800934,\n",
       " 0.014320374,\n",
       " -0.034851074,\n",
       " -0.02279663,\n",
       " -0.05001831,\n",
       " 0.056549072,\n",
       " 0.019180298,\n",
       " -0.020431519,\n",
       " -0.0052871704,\n",
       " 0.023971558,\n",
       " 0.09161377,\n",
       " -0.03527832,\n",
       " 0.00894165,\n",
       " 0.04373169,\n",
       " 0.006099701,\n",
       " 0.016494751,\n",
       " 0.009109497,\n",
       " 0.08105469,\n",
       " -0.0045700073,\n",
       " -0.023284912,\n",
       " -0.03189087,\n",
       " 0.06427002,\n",
       " -0.0008196831,\n",
       " 0.004299164,\n",
       " 0.00819397,\n",
       " -0.03338623,\n",
       " -0.020401001,\n",
       " 0.01953125,\n",
       " -0.004852295,\n",
       " 0.010002136,\n",
       " -0.029754639,\n",
       " -0.04827881,\n",
       " 0.0038146973,\n",
       " 0.005882263,\n",
       " 0.023468018,\n",
       " -0.008102417,\n",
       " 0.0040779114,\n",
       " -0.031982422,\n",
       " 0.041809082,\n",
       " -0.06921387,\n",
       " 0.005393982,\n",
       " -0.046020508,\n",
       " -0.0051612854,\n",
       " -0.017028809,\n",
       " -0.055358887,\n",
       " -0.041656494,\n",
       " -0.013244629,\n",
       " -0.04763794,\n",
       " 0.049560547,\n",
       " -0.06524658,\n",
       " -0.03048706,\n",
       " 0.010398865,\n",
       " 0.027267456,\n",
       " 0.034362793,\n",
       " 0.037017822,\n",
       " -0.038146973,\n",
       " -0.03869629,\n",
       " 0.08288574,\n",
       " -0.015060425,\n",
       " -0.06756592,\n",
       " 0.010215759,\n",
       " 0.008766174,\n",
       " 0.0044403076,\n",
       " 0.043640137,\n",
       " 0.0010852814,\n",
       " -0.046936035,\n",
       " 0.056549072,\n",
       " -0.08392334,\n",
       " -0.008636475,\n",
       " -0.037353516,\n",
       " 0.008346558,\n",
       " -0.032714844,\n",
       " 0.05206299,\n",
       " -0.06921387,\n",
       " 0.08319092,\n",
       " -0.06842041,\n",
       " 0.07647705,\n",
       " 0.048217773,\n",
       " -0.036865234,\n",
       " -0.040283203,\n",
       " 0.014717102,\n",
       " 0.019805908,\n",
       " 0.018249512,\n",
       " -0.012237549,\n",
       " -0.004638672,\n",
       " 0.0140686035,\n",
       " -0.06719971,\n",
       " 0.0009775162,\n",
       " 0.016113281,\n",
       " -0.023345947,\n",
       " 0.008598328,\n",
       " -0.012283325,\n",
       " -0.04714966,\n",
       " -0.018463135,\n",
       " -0.031555176,\n",
       " -0.047180176,\n",
       " -0.023742676,\n",
       " 0.00907135,\n",
       " -0.06542969,\n",
       " 0.052215576,\n",
       " 0.03857422,\n",
       " 0.057647705,\n",
       " 0.0035247803,\n",
       " 0.00299263,\n",
       " 0.028274536,\n",
       " 0.023620605,\n",
       " 0.011169434,\n",
       " -0.030822754,\n",
       " 0.0096206665,\n",
       " 0.013221741,\n",
       " -0.009429932,\n",
       " -0.03692627,\n",
       " -0.035369873,\n",
       " 0.036224365,\n",
       " 0.04257202,\n",
       " -0.015342712,\n",
       " -0.012512207,\n",
       " -0.0060195923,\n",
       " -0.012039185,\n",
       " -0.0017910004,\n",
       " -0.013786316,\n",
       " -0.026016235,\n",
       " -0.00076913834,\n",
       " 0.022613525,\n",
       " -0.01121521,\n",
       " -0.08404541,\n",
       " 0.05810547,\n",
       " 0.006790161,\n",
       " 0.020568848,\n",
       " 0.031311035,\n",
       " -0.014183044,\n",
       " 0.072631836,\n",
       " 0.008613586,\n",
       " -0.002407074,\n",
       " 0.0033245087,\n",
       " 0.03869629,\n",
       " 0.0046577454,\n",
       " -0.018051147,\n",
       " -0.042419434,\n",
       " 0.012382507,\n",
       " 0.005252838,\n",
       " 0.034332275,\n",
       " 0.014007568,\n",
       " 0.03048706,\n",
       " 0.005039215,\n",
       " 0.0060806274,\n",
       " -0.035247803,\n",
       " 0.023468018,\n",
       " 0.01007843,\n",
       " -0.037475586,\n",
       " -0.03652954,\n",
       " 0.058380127,\n",
       " 0.049957275,\n",
       " 3.170967e-05,\n",
       " 0.0096588135,\n",
       " -0.04840088,\n",
       " 0.0014486313,\n",
       " -0.047302246,\n",
       " -0.041931152,\n",
       " -2.4437904e-05,\n",
       " -0.070007324,\n",
       " -0.047454834,\n",
       " -0.020492554,\n",
       " -0.022567749,\n",
       " 0.0069084167,\n",
       " 0.054595947,\n",
       " 0.034423828,\n",
       " 0.011764526,\n",
       " -0.02180481,\n",
       " 0.028579712,\n",
       " -0.012283325,\n",
       " 0.019119263,\n",
       " 0.005001068,\n",
       " 0.057800293,\n",
       " -0.03024292,\n",
       " -0.036254883,\n",
       " -0.017456055,\n",
       " 0.023864746,\n",
       " -0.009864807,\n",
       " 0.03527832,\n",
       " 0.0012998581,\n",
       " 0.019836426,\n",
       " 0.055664062,\n",
       " -0.0010633469,\n",
       " 0.010215759,\n",
       " 0.03189087,\n",
       " -0.040405273,\n",
       " 0.056884766,\n",
       " 0.026321411,\n",
       " 0.018432617,\n",
       " 0.054351807,\n",
       " 0.00031471252,\n",
       " 0.018798828,\n",
       " 0.028533936,\n",
       " 0.021575928,\n",
       " 0.04940796,\n",
       " 0.022445679,\n",
       " 0.02684021,\n",
       " -0.01852417,\n",
       " -0.0637207,\n",
       " 0.04876709,\n",
       " -0.0029563904,\n",
       " 0.03942871,\n",
       " 0.020889282,\n",
       " -0.029541016,\n",
       " -0.055419922,\n",
       " -0.014556885,\n",
       " 0.001411438,\n",
       " -0.02684021,\n",
       " 0.026733398,\n",
       " 0.034942627,\n",
       " 0.051635742,\n",
       " 0.012161255,\n",
       " 0.027648926,\n",
       " -0.0085372925,\n",
       " -0.049468994,\n",
       " -0.040893555,\n",
       " -0.08306885,\n",
       " -0.017532349,\n",
       " 0.0013837814,\n",
       " -0.0018444061,\n",
       " 0.053466797,\n",
       " -0.03640747,\n",
       " -0.043304443,\n",
       " -0.019241333,\n",
       " 0.04675293,\n",
       " -0.022918701,\n",
       " -0.05050659,\n",
       " -0.0016479492,\n",
       " 0.058685303,\n",
       " 0.016082764,\n",
       " -0.011047363,\n",
       " -0.019119263,\n",
       " 0.02230835,\n",
       " 0.022262573,\n",
       " 0.009521484,\n",
       " -0.029388428,\n",
       " -0.033691406,\n",
       " -0.0051078796,\n",
       " -0.009902954,\n",
       " -0.04257202,\n",
       " 0.020339966,\n",
       " 0.02293396,\n",
       " 0.01751709,\n",
       " -0.054260254,\n",
       " 0.017745972,\n",
       " 0.00680542,\n",
       " 0.0021629333,\n",
       " 0.022628784,\n",
       " 0.008262634,\n",
       " 0.05102539,\n",
       " 0.0054740906,\n",
       " 0.027389526,\n",
       " 0.026687622,\n",
       " 0.028411865,\n",
       " 0.030654907,\n",
       " 0.012931824,\n",
       " -0.049560547,\n",
       " 0.0030403137,\n",
       " 0.04031372,\n",
       " -0.015052795,\n",
       " -0.013267517,\n",
       " 0.08728027,\n",
       " 0.021255493,\n",
       " -0.046325684,\n",
       " -0.012023926,\n",
       " 0.025817871,\n",
       " 0.05770874,\n",
       " -0.0236969,\n",
       " -0.013076782,\n",
       " -0.084228516,\n",
       " -0.047912598,\n",
       " 0.014083862,\n",
       " -0.012321472,\n",
       " -0.0035972595,\n",
       " -0.022537231,\n",
       " 0.01953125,\n",
       " 0.04953003,\n",
       " 0.0206604,\n",
       " 0.01612854,\n",
       " -0.010665894,\n",
       " -0.03302002,\n",
       " -0.023239136,\n",
       " 0.027862549,\n",
       " -0.0038967133,\n",
       " 0.012557983,\n",
       " -0.05206299,\n",
       " 0.011932373,\n",
       " -0.039764404,\n",
       " -0.03201294,\n",
       " 0.023071289,\n",
       " -0.003376007,\n",
       " 0.009338379,\n",
       " -0.00067329407,\n",
       " -0.016281128,\n",
       " -0.04925537,\n",
       " 0.0062446594,\n",
       " 0.0062446594,\n",
       " -0.054992676,\n",
       " 0.034729004,\n",
       " -0.027557373,\n",
       " -0.029663086,\n",
       " 0.060577393,\n",
       " -0.017425537,\n",
       " 0.014732361,\n",
       " 0.0209198,\n",
       " 0.022537231,\n",
       " 0.0079422,\n",
       " -0.006477356,\n",
       " 0.04498291,\n",
       " 0.01399231,\n",
       " -0.028060913,\n",
       " -0.020263672,\n",
       " -0.023742676,\n",
       " 0.057891846,\n",
       " 0.026031494,\n",
       " 0.0099105835,\n",
       " -0.014289856,\n",
       " 0.013748169,\n",
       " 0.004020691,\n",
       " -0.0060272217,\n",
       " 0.038116455,\n",
       " 0.031402588,\n",
       " -0.06585693,\n",
       " -0.008369446,\n",
       " 0.058441162,\n",
       " 0.016204834,\n",
       " 0.00970459,\n",
       " -0.029067993,\n",
       " 0.0463562,\n",
       " 0.05114746,\n",
       " -0.011932373,\n",
       " 0.024215698,\n",
       " 0.025665283,\n",
       " 0.010192871,\n",
       " -0.03302002,\n",
       " -0.05050659,\n",
       " -0.0005002022,\n",
       " -0.0043563843,\n",
       " 0.08117676,\n",
       " 0.013458252,\n",
       " 0.021469116,\n",
       " -0.006214142,\n",
       " -0.008262634,\n",
       " -0.05218506,\n",
       " 0.0055351257,\n",
       " 0.04525757,\n",
       " -0.0009775162,\n",
       " 0.066711426,\n",
       " -0.0637207,\n",
       " -0.006793976,\n",
       " -0.026672363,\n",
       " 0.048736572,\n",
       " 0.054748535,\n",
       " 0.03881836,\n",
       " 0.005970001,\n",
       " -0.010192871,\n",
       " 0.040008545,\n",
       " 0.024978638,\n",
       " -0.038970947,\n",
       " 0.017303467,\n",
       " -0.04144287,\n",
       " 0.019836426,\n",
       " 0.06021118,\n",
       " -0.026275635,\n",
       " 0.0032081604,\n",
       " 0.056884766,\n",
       " -0.062805176,\n",
       " 0.021987915,\n",
       " -0.0028953552,\n",
       " 0.032836914,\n",
       " -0.023483276,\n",
       " -0.04046631,\n",
       " 0.026245117,\n",
       " -0.03994751,\n",
       " 0.07287598,\n",
       " -0.04989624,\n",
       " -0.04559326,\n",
       " 0.015655518,\n",
       " -0.003944397,\n",
       " 0.07220459,\n",
       " -0.00440979,\n",
       " 0.017486572,\n",
       " 0.034698486,\n",
       " 0.05999756,\n",
       " 0.014564514,\n",
       " 0.030899048,\n",
       " -0.06768799,\n",
       " -0.032989502,\n",
       " -0.0077171326,\n",
       " -0.015464783,\n",
       " 0.04864502,\n",
       " 0.01889038,\n",
       " -0.03778076,\n",
       " 0.021530151,\n",
       " 0.0051612854,\n",
       " -0.028167725,\n",
       " 0.034942627,\n",
       " 0.0031280518,\n",
       " -0.013061523,\n",
       " 0.0028839111,\n",
       " -0.05319214,\n",
       " -0.030197144,\n",
       " -0.028396606,\n",
       " 0.008483887,\n",
       " 0.01373291,\n",
       " 0.036468506,\n",
       " 0.034301758,\n",
       " 0.013870239,\n",
       " -0.018508911,\n",
       " 0.0680542,\n",
       " 0.057159424,\n",
       " -0.04699707,\n",
       " 0.02444458,\n",
       " -0.0050811768,\n",
       " -0.04559326,\n",
       " -0.043395996,\n",
       " -0.0095825195,\n",
       " -0.032226562,\n",
       " 0.03869629,\n",
       " -0.0011196136,\n",
       " -0.0107421875,\n",
       " 0.095336914,\n",
       " -0.05267334,\n",
       " 0.01626587,\n",
       " 0.011779785,\n",
       " -0.03552246,\n",
       " -0.013435364,\n",
       " 0.003490448,\n",
       " 0.037109375,\n",
       " -0.010627747,\n",
       " 0.011627197,\n",
       " 0.0065841675,\n",
       " 0.007461548,\n",
       " -0.05831909,\n",
       " 0.0011425018,\n",
       " 0.00472641,\n",
       " -0.031982422,\n",
       " 0.057800293,\n",
       " -0.009422302,\n",
       " -0.02180481,\n",
       " -0.052856445,\n",
       " -0.007320404,\n",
       " -0.0007710457,\n",
       " 0.04373169,\n",
       " -0.1071167,\n",
       " -0.014503479,\n",
       " -0.019256592,\n",
       " 0.00016665459,\n",
       " -0.08068848,\n",
       " -0.011940002,\n",
       " 0.0053977966,\n",
       " -0.054260254,\n",
       " -0.03552246,\n",
       " 0.04083252,\n",
       " -0.013015747,\n",
       " 0.06329346,\n",
       " 0.0068473816,\n",
       " 0.041534424,\n",
       " -0.02407837,\n",
       " -0.025436401,\n",
       " 0.05819702,\n",
       " -0.011482239,\n",
       " -0.050201416,\n",
       " -0.03552246,\n",
       " -0.03338623,\n",
       " 0.023010254,\n",
       " -0.020690918,\n",
       " -0.032409668,\n",
       " 0.042053223,\n",
       " -0.0029335022,\n",
       " -0.008148193,\n",
       " -0.023712158,\n",
       " -0.050842285,\n",
       " 0.032714844,\n",
       " -0.045532227,\n",
       " 0.012191772,\n",
       " 0.05596924,\n",
       " 0.036865234,\n",
       " -0.010772705,\n",
       " -0.011497498,\n",
       " -0.03265381,\n",
       " 0.036010742,\n",
       " -0.05206299,\n",
       " -0.0680542,\n",
       " 0.014442444,\n",
       " 0.04849243,\n",
       " -0.00919342,\n",
       " 0.0018348694,\n",
       " 0.022033691,\n",
       " -0.04421997,\n",
       " -0.033843994,\n",
       " -0.023971558,\n",
       " -0.038391113,\n",
       " -0.010871887,\n",
       " -0.013648987,\n",
       " 0.038085938,\n",
       " -0.02331543,\n",
       " 0.007835388,\n",
       " 0.0413208,\n",
       " -0.025177002,\n",
       " 0.027145386,\n",
       " -0.028579712,\n",
       " -0.032714844,\n",
       " -0.00041651726,\n",
       " -0.052825928,\n",
       " -0.034179688,\n",
       " 0.004940033,\n",
       " 0.010894775,\n",
       " -0.061645508,\n",
       " 0.018325806,\n",
       " -0.01210022,\n",
       " -0.029846191,\n",
       " -0.028396606,\n",
       " 0.0020923615,\n",
       " -0.015220642,\n",
       " 0.051574707,\n",
       " -0.01826477,\n",
       " 0.07128906,\n",
       " -0.00970459,\n",
       " -0.052947998,\n",
       " -0.036010742,\n",
       " 0.0013198853,\n",
       " 0.033294678,\n",
       " -0.019866943,\n",
       " -0.034729004,\n",
       " -0.0871582,\n",
       " -0.017303467,\n",
       " 0.0029754639,\n",
       " -0.010925293,\n",
       " 0.044067383,\n",
       " -0.009124756,\n",
       " -0.038604736,\n",
       " 0.04336548,\n",
       " 0.020111084,\n",
       " -0.015182495,\n",
       " -0.043304443,\n",
       " -0.022445679,\n",
       " 0.08666992,\n",
       " -0.027633667,\n",
       " 0.019927979,\n",
       " -0.011253357,\n",
       " 0.059753418,\n",
       " -0.03265381,\n",
       " 0.09631348,\n",
       " 0.10394287,\n",
       " 0.028137207,\n",
       " 0.0053901672,\n",
       " 0.007881165,\n",
       " -0.0011453629,\n",
       " 0.003730774,\n",
       " -0.05822754,\n",
       " -0.027862549,\n",
       " -0.017974854,\n",
       " 0.016601562,\n",
       " -0.003320694,\n",
       " -0.033599854,\n",
       " -0.0231781,\n",
       " 0.016571045,\n",
       " -0.0021076202,\n",
       " -0.05267334,\n",
       " -0.0022068024,\n",
       " -0.03805542,\n",
       " -0.013465881,\n",
       " 0.0070152283,\n",
       " -0.039642334,\n",
       " -0.03366089,\n",
       " -0.03768921,\n",
       " 0.045928955,\n",
       " 0.02394104,\n",
       " -0.010604858,\n",
       " 0.046966553,\n",
       " -0.0070381165,\n",
       " 0.0023918152,\n",
       " -0.014289856,\n",
       " -0.0040130615,\n",
       " 0.0045433044,\n",
       " -0.0126953125,\n",
       " -0.033935547,\n",
       " -0.0016021729,\n",
       " -0.0061531067,\n",
       " -0.012023926,\n",
       " -0.004814148,\n",
       " -0.01197052,\n",
       " -0.072021484,\n",
       " -0.0848999,\n",
       " 0.025283813,\n",
       " -0.052215576,\n",
       " -0.016342163,\n",
       " -0.038635254,\n",
       " 7.915497e-05,\n",
       " -0.040618896,\n",
       " -0.059051514,\n",
       " 0.02432251,\n",
       " 0.02758789,\n",
       " 0.03692627,\n",
       " 0.0002465248,\n",
       " 0.01763916,\n",
       " -0.052520752,\n",
       " -0.0770874,\n",
       " 0.054016113,\n",
       " 0.023727417,\n",
       " -0.0062828064,\n",
       " 0.0033130646,\n",
       " 0.013549805,\n",
       " 0.025878906,\n",
       " 0.0028247833,\n",
       " 0.04724121,\n",
       " 0.054016113,\n",
       " 0.01889038,\n",
       " -0.04827881,\n",
       " -0.016326904,\n",
       " -0.021850586,\n",
       " 0.035308838,\n",
       " 0.030334473,\n",
       " -0.037994385,\n",
       " -0.04333496,\n",
       " -0.036468506,\n",
       " -0.033447266,\n",
       " -0.014755249,\n",
       " 0.018630981,\n",
       " 0.013046265,\n",
       " 0.010528564,\n",
       " 0.026031494,\n",
       " 0.024398804,\n",
       " -0.0046691895,\n",
       " -0.04660034,\n",
       " 0.045074463,\n",
       " 0.0066223145,\n",
       " -0.018554688,\n",
       " -0.06719971,\n",
       " -0.013809204,\n",
       " -0.044647217,\n",
       " -0.044158936,\n",
       " -0.060668945,\n",
       " -0.021987915,\n",
       " -0.004169464,\n",
       " 0.0076293945,\n",
       " -0.0014104843,\n",
       " -0.020843506,\n",
       " 0.041137695,\n",
       " -0.00069379807,\n",
       " 0.052215576,\n",
       " -0.050567627,\n",
       " 0.043823242,\n",
       " -0.00025105476,\n",
       " 0.005924225,\n",
       " 0.0044441223,\n",
       " 0.045837402,\n",
       " 0.06628418,\n",
       " 0.002576828,\n",
       " 0.046020508,\n",
       " 0.063964844,\n",
       " 0.031280518,\n",
       " 0.035980225,\n",
       " -0.023239136,\n",
       " -0.03427124,\n",
       " 0.028640747,\n",
       " 0.02168274,\n",
       " -0.059173584,\n",
       " -0.040771484,\n",
       " 0.00623703]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.embed_query(\"My query to look up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List with elements\n",
      "Index: 0, Value type: <class 'list'>, Value preview: [Document(page_content='3 2 0 2\\n\\nl u J\\n\\n0 1\\n\\\n",
      "Index: 1, Value type: <class 'list'>, Value preview: [Document(page_content=\"About Scotty\\n\\nThe Scotti\n",
      "Index: 2, Value type: <class 'list'>, Value preview: [Document(page_content='Sweepstakes Slang\\n\\nBuggy\n",
      "Index: 3, Value type: <class 'list'>, Value preview: [Document(page_content='2024-2025 OFFICIAL Academi\n",
      "Index: 4, Value type: <class 'list'>, Value preview: [Document(page_content='Andrew Carnegie\\n\\nA self-\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace 'your_file_path.pkl' with the path to your .pkl file\n",
    "file_path = 'splitDocuments.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    contents = pickle.load(file)\n",
    "\n",
    "def print_limited_structure(data, limit=5):\n",
    "    \"\"\"Print a limited view of the data structure.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        print(\"Dictionary with keys: values\")\n",
    "        for i, (key, value) in enumerate(data.items()):\n",
    "            print(f\"Key: {key}, Value type: {type(value)}, Value preview: {str(value)[:50]}\")\n",
    "            if i >= limit - 1:\n",
    "                break\n",
    "    elif isinstance(data, list):\n",
    "        print(\"List with elements\")\n",
    "        for i in range(min(limit, len(data))):\n",
    "            print(f\"Index: {i}, Value type: {type(data[i])}, Value preview: {str(data[i])[:50]}\")\n",
    "    elif isinstance(data, (set, tuple)):\n",
    "        print(f\"{type(data).__name__} with elements\")\n",
    "        for i, item in enumerate(data):\n",
    "            print(f\"Element: {i}, Value type: {type(item)}, Value preview: {str(item)[:50]}\")\n",
    "            if i >= limit - 1:\n",
    "                break\n",
    "    else:\n",
    "        print(\"Data type:\", type(data))\n",
    "        print(f\"Preview: {str(data)[:200]}\")  # Print the first 200 characters\n",
    "\n",
    "print_limited_structure(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "\n",
    "# Assuming contents is your loaded .pkl data and is a list of lists\n",
    "# where each sublist contains document objects with a page_content attribute\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Disable the check for all special tokens\n",
    "    num_tokens = len(encoding.encode(string, disallowed_special=()))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# Extracting page_content from each document object\n",
    "docs_texts = []\n",
    "for sublist in contents:\n",
    "    for doc in sublist:  # Assuming each element in sublist is a document object\n",
    "        docs_texts.append(doc.page_content)\n",
    "\n",
    "# Calculate the number of tokens for each document's page_content\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 2 0 2\\n\\nl u J\\n\\n0 1\\n\\n] L C . s c [\\n\\n2 v 5 8 1 8 1 . 5 0 3 2 : v i X r a\\n\\nSyntax and Semantics Meet in the Middle: Probing the Syntax-Semantics Interface of LMs Through Agentivity\\n\\nLindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig Language Technologies Institute Carnegie Mellon University {ltjuatja, mengyan3, lsl, gneubig}@cs.cmu.edu\\n\\n1\\n\\nAbstract', 'Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models han- dle the interactions in meaning across words and larger syntactic formsi.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitiz- ing the unique linguistic properties', 'of a sub- set of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level pri- ors given a specific syntactic context. Over- all, GPT-3 text-davinci-003 performs extremely well across all experiments, outper- forming all other models tested by far. In fact, the results are even better correlated with hu- man judgements than both', 'syntactic and seman- tic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and dis- covery than select corpora for certain tasks.1', 'Introduction\\n\\nan agent in the writing event. On the other hand, the subject of (1b), this passage, doesnt do any writingit is what is created in the event of writ- ing. In contrast to this author, this passage is a patient. The agent and patient roles are not dis- crete categories, but rather prototypes on opposite ends of a continuum. These protoroles have a number of contributing properties such as causing an event for agents and undergoing change of state for patients (Dowty, 1991).', 'The contrast between the minimal pair in (1) suggests that there are lexical semantic properties of the subjects that give rise to these two distinct readings: one that describes how the subject gen- erally does an action as in (1a), and another that describes how an event generally unfolds when the subject undergoes an action as in (1b). Intuitively, a speaker may know from the meaning of author that authors are animate, have some degree of voli- tion, and typically write things, whereas', 'passages (of text) are inanimate, have no volition, and are typically written. The knowledge of these aspects of meaning must somehow interact with the syn- tactic form of the sentences in (1) to disambiguate between the two possible readings, and an agent or patient role for the subject follows from the mean- ing of the statement as a whole.', 'Consider the English sentences in (1) below:\\n\\nNow consider the (somewhat unusual) sentences\\n\\n(1)\\n\\na. This author writes easily.\\n\\nin (2) which use the transitive form of write:\\n\\nb. This passage writes easily.\\n\\n(2)\\n\\na. Something writes this author easily.', 'These sentences display an interesting property of certain optionally transitive verbs in English. Al- though they share an identical surface syntactic structurea noun phrase in subject position fol- lowed by the intransitive form of the verb and an adverb phrase modifying the verbthey entail very different things about the roles of their subjects.\\n\\nThe subject of (1a) is someone that does the action of writing; in other words, this author is\\n\\nb. This passage writes something easily.', 'At first glance, the above sentences (with the same sense of write as in 1) are infelicitous unless we imagine some obscure context where this author is something like a character in a text and this pas- sage is somehow anthropomorphized and capable of writing; these contexts go against our natural intuitions of the semantics of passage and au- thor.2 Unlike the syntactic form of the sentences\\n\\n1Code\\n\\nis\\n\\navailable\\n\\nlindiatjuatja/lm_sem\\n\\nat https://github.com/']\n"
     ]
    }
   ],
   "source": [
    "print(docs_texts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevea/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|| 3/3 [00:01<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"Salesforce/SFR-Embedding-Mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevea/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|| 3/3 [00:01<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"Salesforce/SFR-Embedding-Mistral\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.7783623 -0.5919665  4.3199406 ...  7.873832  -7.350942   2.5289717]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming docs_texts \n",
    "embeddings = np.array([model.encode(doc) for doc in docs_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Modules\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "print(\"Imported Modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "# Previously, embed_cluster_texts was designed to take texts, embed them, and then cluster.\n",
    "# Now, you have pre-computed embeddings, so you modify your approach to directly use those embeddings.\n",
    "\n",
    "def cluster_texts_with_precomputed_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Clusters texts using precomputed embeddings and returns a DataFrame with cluster labels.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: numpy.ndarray, the precomputed embeddings for the texts.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    # Assuming perform_clustering is a function you've defined that expects embeddings\n",
    "    # and returns cluster labels (modify this part according to your actual clustering implementation)\n",
    "    cluster_labels = perform_clustering(embeddings, dim=10, threshold=0.1)  # Example parameters\n",
    "\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = docs_texts  # Store original texts\n",
    "    df[\"embd\"] = list(embeddings)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = model.encode(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of information about language Technology Institute Faculty at Carnegie Mellon University (CMU).  \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "    #   Split formatted_txt if it's too long\n",
    "        if len(formatted_txt) > 2000:  # Adjust the number based on your template length\n",
    "            text_chunks = split_text(formatted_txt, 1500)  # Example chunk size, adjust as needed\n",
    "            summary_parts = []\n",
    "            for chunk in text_chunks:\n",
    "                # Update the context in your template accordingly\n",
    "                temp_prompt = template.format(context=chunk)\n",
    "                # Invoke the chain with the updated prompt for each chunk\n",
    "                summary_part = chain.invoke({\"context\": temp_prompt})\n",
    "                summary_parts.append(summary_part)\n",
    "            # Combine the individual summaries into one\n",
    "            combined_summary = ' '.join(summary_parts)\n",
    "            summaries.append(combined_summary)\n",
    "        else:\n",
    "            # Proceed as before if the text is within the limit\n",
    "            summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "# Function to split text into smaller parts\n",
    "def split_text(text, chunk_size):\n",
    "    # Split the text into chunks of approximately 'chunk_size' characters\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered_texts = cluster_texts_with_precomputed_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  3 2 0 2\\n\\nl u J\\n\\n0 1\\n\\n] L C . s c [\\n\\n2 ...   \n",
      "1  Recent advances in large language models have ...   \n",
      "2  of a sub- set of optionally transitive English...   \n",
      "3  syntactic and seman- tic corpus statistics. Th...   \n",
      "4  Introduction\\n\\nan agent in the writing event....   \n",
      "5  The contrast between the minimal pair in (1) s...   \n",
      "6  passages (of text) are inanimate, have no voli...   \n",
      "7  Consider the English sentences in (1) below:\\n...   \n",
      "\n",
      "                                                embd  cluster  \n",
      "0  [5.068998, -2.2683353, 2.1725945, -1.4779862, ...  [214.0]  \n",
      "1  [5.7855673, -1.0487652, 0.7265795, -1.2334801,...  [214.0]  \n",
      "2  [3.226253, -1.0835481, 1.3811967, 0.23921128, ...  [564.0]  \n",
      "3  [6.0345435, -3.3031888, 0.27811188, -1.2451488...  [214.0]  \n",
      "4  [5.4009376, -0.12218815, 2.6706438, -1.7392813...  [913.0]  \n",
      "5  [2.4489007, -1.1088563, -1.977748, -3.9637551,...  [913.0]  \n",
      "6  [4.161246, -0.48568708, 1.7780865, -3.9184294,...  [913.0]  \n",
      "7  [3.8849313, 0.29882342, -1.5985764, -0.2426184...  [913.0]  \n"
     ]
    }
   ],
   "source": [
    "print(df_clustered_texts[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "file_path = 'df_clustered_texts.csv'\n",
    "df_clustered_texts.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1097 clusters--\n"
     ]
    }
   ],
   "source": [
    "df_clusters = df_clustered_texts.copy()\n",
    "# Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "expanded_list = []\n",
    "\n",
    "# Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "for index, row in df_clusters.iterrows():\n",
    "    for cluster in row[\"cluster\"]:\n",
    "        expanded_list.append(\n",
    "            {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "        )\n",
    "\n",
    "# Create a new DataFrame from the expanded list\n",
    "expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "# Retrieve unique cluster identifiers for processing\n",
    "all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "print(f\"--Generated {len(all_clusters)} clusters--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5272.34 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      23.30 ms /   143 runs   (    0.16 ms per token,  6138.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8401.76 ms /   491 tokens (   17.11 ms per token,    58.44 tokens per second)\n",
      "llama_print_timings:        eval time =   24004.57 ms /   142 runs   (  169.05 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:       total time =   32790.43 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      39.86 ms /   256 runs   (    0.16 ms per token,  6422.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6294.80 ms /   374 tokens (   16.83 ms per token,    59.41 tokens per second)\n",
      "llama_print_timings:        eval time =   43439.81 ms /   255 runs   (  170.35 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =   50459.72 ms /   629 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      36.91 ms /   237 runs   (    0.16 ms per token,  6420.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5767.89 ms /   376 tokens (   15.34 ms per token,    65.19 tokens per second)\n",
      "llama_print_timings:        eval time =   40179.32 ms /   236 runs   (  170.25 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =   46616.70 ms /   612 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      31.80 ms /   206 runs   (    0.15 ms per token,  6478.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6653.19 ms /   428 tokens (   15.54 ms per token,    64.33 tokens per second)\n",
      "llama_print_timings:        eval time =   35446.93 ms /   205 runs   (  172.91 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   42693.83 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      46.62 ms /   256 runs   (    0.18 ms per token,  5491.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6004.14 ms /   367 tokens (   16.36 ms per token,    61.12 tokens per second)\n",
      "llama_print_timings:        eval time =   44077.77 ms /   255 runs   (  172.85 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   50945.40 ms /   622 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      44.51 ms /   256 runs   (    0.17 ms per token,  5751.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6060.30 ms /   387 tokens (   15.66 ms per token,    63.86 tokens per second)\n",
      "llama_print_timings:        eval time =   44080.38 ms /   255 runs   (  172.86 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   50952.67 ms /   642 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      43.96 ms /   256 runs   (    0.17 ms per token,  5823.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8006.21 ms /   481 tokens (   16.64 ms per token,    60.08 tokens per second)\n",
      "llama_print_timings:        eval time =   43794.28 ms /   255 runs   (  171.74 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   52608.85 ms /   736 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      41.86 ms /   256 runs   (    0.16 ms per token,  6115.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7971.71 ms /   512 tokens (   15.57 ms per token,    64.23 tokens per second)\n",
      "llama_print_timings:        eval time =   44906.57 ms /   256 runs   (  175.42 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   53711.75 ms /   768 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      44.01 ms /   256 runs   (    0.17 ms per token,  5817.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8245.60 ms /   535 tokens (   15.41 ms per token,    64.88 tokens per second)\n",
      "llama_print_timings:        eval time =   45239.19 ms /   255 runs   (  177.41 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   54332.80 ms /   790 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      45.74 ms /   256 runs   (    0.18 ms per token,  5596.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6838.90 ms /   458 tokens (   14.93 ms per token,    66.97 tokens per second)\n",
      "llama_print_timings:        eval time =   45304.20 ms /   255 runs   (  177.66 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   53033.80 ms /   713 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      48.89 ms /   256 runs   (    0.19 ms per token,  5236.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6521.96 ms /   428 tokens (   15.24 ms per token,    65.62 tokens per second)\n",
      "llama_print_timings:        eval time =   45349.30 ms /   255 runs   (  177.84 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   52754.51 ms /   683 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      43.47 ms /   256 runs   (    0.17 ms per token,  5888.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6651.23 ms /   433 tokens (   15.36 ms per token,    65.10 tokens per second)\n",
      "llama_print_timings:        eval time =   44249.15 ms /   255 runs   (  173.53 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   51787.70 ms /   688 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      39.41 ms /   256 runs   (    0.15 ms per token,  6496.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6984.51 ms /   449 tokens (   15.56 ms per token,    64.29 tokens per second)\n",
      "llama_print_timings:        eval time =   43953.15 ms /   255 runs   (  172.37 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:       total time =   51760.66 ms /   704 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      40.20 ms /   256 runs   (    0.16 ms per token,  6367.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7373.33 ms /   476 tokens (   15.49 ms per token,    64.56 tokens per second)\n",
      "llama_print_timings:        eval time =   44072.58 ms /   255 runs   (  172.83 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   52292.18 ms /   731 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     temp_prompt \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mformat(context\u001b[38;5;241m=\u001b[39mchunk)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Invoke the chain with the updated prompt for each chunk\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     summary_part \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     summary_parts\u001b[38;5;241m.\u001b[39mappend(summary_part)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Combine the individual summaries into one\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/runnables/base.py:2089\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2088\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2089\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2092\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:246\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    244\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 246\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    257\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:541\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    535\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    540\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:714\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    699\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m         )\n\u001b[1;32m    701\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    702\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    703\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m         )\n\u001b[1;32m    713\u001b[0m     ]\n\u001b[0;32m--> 714\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:578\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    577\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    579\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:565\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    557\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    562\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 565\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    569\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    573\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    574\u001b[0m         )\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1153\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1152\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1156\u001b[0m     )\n\u001b[1;32m   1157\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_community/llms/llamacpp.py:288\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombined_text_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_community/llms/llamacpp.py:341\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    340\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 341\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpart\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/llama_cpp/llama.py:1000\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m    998\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1000\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_eos\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/llama_cpp/llama.py:682\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    684\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    685\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    686\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    700\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    701\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/llama_cpp/llama.py:522\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    518\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    520\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    521\u001b[0m )\n\u001b[0;32m--> 522\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/llama_cpp/_internals.py:311\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to split text into smaller parts\n",
    "def split_text(text, chunk_size):\n",
    "    # Split the text into chunks of approximately 'chunk_size' characters\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "level = 1\n",
    "# Summarization\n",
    "template = \"\"\"Here is a sub-set of information about language Technology Institute Faculty at Carnegie Mellon University (CMU).  \n",
    "Give a detailed summary of the documentation provided.\n",
    "Documentation:\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# Format text within each cluster for summarization\n",
    "summaries = []\n",
    "\n",
    "for i in all_clusters:\n",
    "    df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "    formatted_txt = fmt_txt(df_cluster)\n",
    "#   Split formatted_txt if it's too long\n",
    "    if len(formatted_txt) > 2000:  # Adjust the number based on your template length\n",
    "        text_chunks = split_text(formatted_txt, 1500)  # Example chunk size, adjust as needed\n",
    "        summary_parts = []\n",
    "        for chunk in text_chunks:\n",
    "            # Update the context in your template accordingly\n",
    "            temp_prompt = template.format(context=chunk)\n",
    "            # Invoke the chain with the updated prompt for each chunk\n",
    "            summary_part = chain.invoke({\"context\": temp_prompt})\n",
    "            summary_parts.append(summary_part)\n",
    "        # Combine the individual summaries into one\n",
    "        combined_summary = ' '.join(summary_parts)\n",
    "        summaries.append(combined_summary)\n",
    "    else:\n",
    "        # Proceed as before if the text is within the limit\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "        \n",
    "# Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "df_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"summaries\": summaries,\n",
    "        \"level\": [level] * len(summaries),\n",
    "        \"cluster\": list(all_clusters),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming llm and other necessary imports and variables (like template, chain) are already defined\n",
    "\n",
    "def recursive_summarization(texts: List[str], level: int, n_levels: int) -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    if level > n_levels:\n",
    "        return {}  # Base case: reached the maximum level of recursion\n",
    "\n",
    "    # Your summarization logic here (similar to what you've already defined)\n",
    "    # This includes: expanding df, creating summaries, etc.\n",
    "    # Note: You might need to adjust it to accept a list of texts as input\n",
    "\n",
    "    # For demonstration, let's use placeholders for df_clusters and df_summary\n",
    "    df_clusters = None  # Placeholder for the DataFrame that includes clusters\n",
    "    df_summary = None  # Placeholder for the DataFrame that includes summaries\n",
    "    \n",
    "    # Here, you would perform the summarization logic that you've described\n",
    "    # This includes expanding the DataFrame, summarizing each cluster, etc.\n",
    "    # Since the detailed implementation depends on the specifics of your setup, we'll focus on the recursion logic\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results = {level: (df_clusters, df_summary)}\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique() if df_summary is not None else 0\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist() if df_summary is not None else []\n",
    "        next_level_results = recursive_summarization(new_texts, level + 1, n_levels)\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize your recursion\n",
    "n_levels = 3  # Desired number of recursion levels\n",
    "initial_texts = df_clustered_texts[\"text\"].tolist()  # Assuming df_clustered_texts is your initial DataFrame\n",
    "results = recursive_summarization(initial_texts, level=1, n_levels=n_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37655\n"
     ]
    }
   ],
   "source": [
    "print(len(docs_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_texts = docs_texts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37655\n"
     ]
    }
   ],
   "source": [
    "print(len(leaf_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5272.34 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1107 clusters--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.06 ms /   256 runs   (    0.15 ms per token,  6554.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9472.40 ms /   497 tokens (   19.06 ms per token,    52.47 tokens per second)\n",
      "llama_print_timings:        eval time =   44967.04 ms /   255 runs   (  176.34 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   55162.24 ms /   752 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.55 ms /   256 runs   (    0.15 ms per token,  6640.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5780.48 ms /   378 tokens (   15.29 ms per token,    65.39 tokens per second)\n",
      "llama_print_timings:        eval time =   45689.22 ms /   255 runs   (  179.17 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   52202.48 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.04 ms /   256 runs   (    0.15 ms per token,  6557.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5781.30 ms /   378 tokens (   15.29 ms per token,    65.38 tokens per second)\n",
      "llama_print_timings:        eval time =   46444.82 ms /   255 runs   (  182.14 ms per token,     5.49 tokens per second)\n",
      "llama_print_timings:       total time =   53000.49 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      26.86 ms /   177 runs   (    0.15 ms per token,  6590.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6441.66 ms /   430 tokens (   14.98 ms per token,    66.75 tokens per second)\n",
      "llama_print_timings:        eval time =   31007.87 ms /   176 runs   (  176.18 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   37967.51 ms /   606 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      22.51 ms /   148 runs   (    0.15 ms per token,  6574.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6078.90 ms /   369 tokens (   16.47 ms per token,    60.70 tokens per second)\n",
      "llama_print_timings:        eval time =   24186.68 ms /   147 runs   (  164.54 ms per token,     6.08 tokens per second)\n",
      "llama_print_timings:       total time =   30700.61 ms /   516 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.04 ms /   256 runs   (    0.16 ms per token,  6394.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6624.57 ms /   431 tokens (   15.37 ms per token,    65.06 tokens per second)\n",
      "llama_print_timings:        eval time =   44333.32 ms /   255 runs   (  173.86 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   51770.48 ms /   686 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.03 ms /   256 runs   (    0.15 ms per token,  6559.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8179.13 ms /   503 tokens (   16.26 ms per token,    61.50 tokens per second)\n",
      "llama_print_timings:        eval time =   46102.83 ms /   255 runs   (  180.80 ms per token,     5.53 tokens per second)\n",
      "llama_print_timings:       total time =   55084.59 ms /   758 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.52 ms /   256 runs   (    0.16 ms per token,  6317.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7023.56 ms /   477 tokens (   14.72 ms per token,    67.91 tokens per second)\n",
      "llama_print_timings:        eval time =   47056.34 ms /   255 runs   (  184.53 ms per token,     5.42 tokens per second)\n",
      "llama_print_timings:       total time =   54903.60 ms /   732 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      21.64 ms /   137 runs   (    0.16 ms per token,  6329.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8341.65 ms /   522 tokens (   15.98 ms per token,    62.58 tokens per second)\n",
      "llama_print_timings:        eval time =   24937.07 ms /   136 runs   (  183.36 ms per token,     5.45 tokens per second)\n",
      "llama_print_timings:       total time =   33694.35 ms /   658 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.98 ms /   256 runs   (    0.15 ms per token,  6568.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6696.91 ms /   444 tokens (   15.08 ms per token,    66.30 tokens per second)\n",
      "llama_print_timings:        eval time =   44959.14 ms /   255 runs   (  176.31 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   52438.27 ms /   699 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.33 ms /   256 runs   (    0.15 ms per token,  6508.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6817.73 ms /   449 tokens (   15.18 ms per token,    65.86 tokens per second)\n",
      "llama_print_timings:        eval time =   44995.99 ms /   255 runs   (  176.45 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   52598.27 ms /   704 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      23.46 ms /   151 runs   (    0.16 ms per token,  6436.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7966.93 ms /   477 tokens (   16.70 ms per token,    59.87 tokens per second)\n",
      "llama_print_timings:        eval time =   26839.39 ms /   150 runs   (  178.93 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   35274.26 ms /   627 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      28.82 ms /   183 runs   (    0.16 ms per token,  6349.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7036.66 ms /   439 tokens (   16.03 ms per token,    62.39 tokens per second)\n",
      "llama_print_timings:        eval time =   32762.78 ms /   182 runs   (  180.02 ms per token,     5.56 tokens per second)\n",
      "llama_print_timings:       total time =   40377.00 ms /   621 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.28 ms /   256 runs   (    0.15 ms per token,  6516.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7576.78 ms /   478 tokens (   15.85 ms per token,    63.09 tokens per second)\n",
      "llama_print_timings:        eval time =   44642.29 ms /   255 runs   (  175.07 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   53045.52 ms /   733 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.01 ms /   256 runs   (    0.15 ms per token,  6562.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7730.03 ms /   512 tokens (   15.10 ms per token,    66.24 tokens per second)\n",
      "llama_print_timings:        eval time =   45273.47 ms /   256 runs   (  176.85 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   53823.36 ms /   768 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.04 ms /   256 runs   (    0.15 ms per token,  6558.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3024.52 ms /   154 tokens (   19.64 ms per token,    50.92 tokens per second)\n",
      "llama_print_timings:        eval time =   44352.24 ms /   255 runs   (  173.93 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   48164.32 ms /   409 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.82 ms /   256 runs   (    0.16 ms per token,  6428.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6557.92 ms /   403 tokens (   16.27 ms per token,    61.45 tokens per second)\n",
      "llama_print_timings:        eval time =   44878.47 ms /   255 runs   (  175.99 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   52248.59 ms /   658 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.93 ms /   256 runs   (    0.16 ms per token,  6254.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9428.15 ms /   569 tokens (   16.57 ms per token,    60.35 tokens per second)\n",
      "llama_print_timings:        eval time =   45566.62 ms /   255 runs   (  178.69 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   55831.88 ms /   824 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.36 ms /   256 runs   (    0.15 ms per token,  6503.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7522.80 ms /   491 tokens (   15.32 ms per token,    65.27 tokens per second)\n",
      "llama_print_timings:        eval time =   45716.19 ms /   255 runs   (  179.28 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   54050.18 ms /   746 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.62 ms /   256 runs   (    0.15 ms per token,  6628.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6175.64 ms /   414 tokens (   14.92 ms per token,    67.04 tokens per second)\n",
      "llama_print_timings:        eval time =   44098.99 ms /   255 runs   (  172.94 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   51076.11 ms /   669 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.80 ms /   256 runs   (    0.15 ms per token,  6597.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8876.91 ms /   526 tokens (   16.88 ms per token,    59.25 tokens per second)\n",
      "llama_print_timings:        eval time =   44378.29 ms /   255 runs   (  174.03 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   54085.49 ms /   781 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.59 ms /   256 runs   (    0.15 ms per token,  6466.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7069.27 ms /   448 tokens (   15.78 ms per token,    63.37 tokens per second)\n",
      "llama_print_timings:        eval time =   44097.03 ms /   255 runs   (  172.93 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   51997.07 ms /   703 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      37.90 ms /   256 runs   (    0.15 ms per token,  6754.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7193.22 ms /   428 tokens (   16.81 ms per token,    59.50 tokens per second)\n",
      "llama_print_timings:        eval time =   45444.04 ms /   255 runs   (  178.21 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   53436.88 ms /   683 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.17 ms /   256 runs   (    0.15 ms per token,  6535.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6735.97 ms /   447 tokens (   15.07 ms per token,    66.36 tokens per second)\n",
      "llama_print_timings:        eval time =   45216.37 ms /   255 runs   (  177.32 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   52780.08 ms /   702 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.52 ms /   256 runs   (    0.15 ms per token,  6477.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6561.97 ms /   406 tokens (   16.16 ms per token,    61.87 tokens per second)\n",
      "llama_print_timings:        eval time =   44895.25 ms /   255 runs   (  176.06 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   52291.19 ms /   661 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.34 ms /   256 runs   (    0.15 ms per token,  6676.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6358.90 ms /   415 tokens (   15.32 ms per token,    65.26 tokens per second)\n",
      "llama_print_timings:        eval time =   46823.80 ms /   255 runs   (  183.62 ms per token,     5.45 tokens per second)\n",
      "llama_print_timings:       total time =   53977.88 ms /   670 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.37 ms /   256 runs   (    0.15 ms per token,  6672.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8679.17 ms /   551 tokens (   15.75 ms per token,    63.49 tokens per second)\n",
      "llama_print_timings:        eval time =   46954.99 ms /   255 runs   (  184.14 ms per token,     5.43 tokens per second)\n",
      "llama_print_timings:       total time =   56409.40 ms /   806 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.13 ms /   256 runs   (    0.15 ms per token,  6714.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7287.70 ms /   458 tokens (   15.91 ms per token,    62.85 tokens per second)\n",
      "llama_print_timings:        eval time =   45473.79 ms /   255 runs   (  178.33 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   53582.75 ms /   713 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =       9.77 ms /    65 runs   (    0.15 ms per token,  6655.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6596.79 ms /   425 tokens (   15.52 ms per token,    64.43 tokens per second)\n",
      "llama_print_timings:        eval time =   11376.03 ms /    64 runs   (  177.75 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   18166.41 ms /   489 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.29 ms /   256 runs   (    0.15 ms per token,  6515.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8904.89 ms /   561 tokens (   15.87 ms per token,    63.00 tokens per second)\n",
      "llama_print_timings:        eval time =   44053.97 ms /   255 runs   (  172.76 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   53807.43 ms /   816 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.09 ms /   256 runs   (    0.16 ms per token,  6385.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10068.55 ms /   641 tokens (   15.71 ms per token,    63.66 tokens per second)\n",
      "llama_print_timings:        eval time =   44161.69 ms /   255 runs   (  173.18 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   55081.19 ms /   896 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      37.75 ms /   245 runs   (    0.15 ms per token,  6490.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5201.31 ms /   293 tokens (   17.75 ms per token,    56.33 tokens per second)\n",
      "llama_print_timings:        eval time =   40920.18 ms /   244 runs   (  167.71 ms per token,     5.96 tokens per second)\n",
      "llama_print_timings:       total time =   46908.73 ms /   537 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.45 ms /   256 runs   (    0.15 ms per token,  6490.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5447.49 ms /   362 tokens (   15.05 ms per token,    66.45 tokens per second)\n",
      "llama_print_timings:        eval time =   43749.39 ms /   255 runs   (  171.57 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   49995.30 ms /   617 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.71 ms /   256 runs   (    0.16 ms per token,  6446.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5708.28 ms /   371 tokens (   15.39 ms per token,    64.99 tokens per second)\n",
      "llama_print_timings:        eval time =   45295.24 ms /   255 runs   (  177.63 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   51819.64 ms /   626 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.71 ms /   164 runs   (    0.15 ms per token,  6636.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5661.53 ms /   362 tokens (   15.64 ms per token,    63.94 tokens per second)\n",
      "llama_print_timings:        eval time =   27381.48 ms /   163 runs   (  167.98 ms per token,     5.95 tokens per second)\n",
      "llama_print_timings:       total time =   33557.99 ms /   525 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.17 ms /   256 runs   (    0.15 ms per token,  6536.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6473.48 ms /   404 tokens (   16.02 ms per token,    62.41 tokens per second)\n",
      "llama_print_timings:        eval time =   44371.27 ms /   255 runs   (  174.00 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   51681.42 ms /   659 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      27.82 ms /   180 runs   (    0.15 ms per token,  6470.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6167.03 ms /   380 tokens (   16.23 ms per token,    61.62 tokens per second)\n",
      "llama_print_timings:        eval time =   33434.01 ms /   179 runs   (  186.78 ms per token,     5.35 tokens per second)\n",
      "llama_print_timings:       total time =   40165.35 ms /   559 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.13 ms /   256 runs   (    0.15 ms per token,  6542.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6253.57 ms /   405 tokens (   15.44 ms per token,    64.76 tokens per second)\n",
      "llama_print_timings:        eval time =   45655.54 ms /   255 runs   (  179.04 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   52728.88 ms /   660 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.69 ms /   256 runs   (    0.16 ms per token,  6449.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6374.42 ms /   396 tokens (   16.10 ms per token,    62.12 tokens per second)\n",
      "llama_print_timings:        eval time =   44603.01 ms /   255 runs   (  174.91 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   51823.87 ms /   651 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.84 ms /   256 runs   (    0.15 ms per token,  6590.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6282.76 ms /   415 tokens (   15.14 ms per token,    66.05 tokens per second)\n",
      "llama_print_timings:        eval time =   43699.27 ms /   255 runs   (  171.37 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   50806.41 ms /   670 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.99 ms /   256 runs   (    0.15 ms per token,  6564.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5979.09 ms /   388 tokens (   15.41 ms per token,    64.89 tokens per second)\n",
      "llama_print_timings:        eval time =   44600.21 ms /   255 runs   (  174.90 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   51402.82 ms /   643 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.91 ms /   256 runs   (    0.15 ms per token,  6579.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8466.55 ms /   548 tokens (   15.45 ms per token,    64.73 tokens per second)\n",
      "llama_print_timings:        eval time =   45023.34 ms /   255 runs   (  176.56 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   54331.82 ms /   803 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.84 ms /   256 runs   (    0.15 ms per token,  6590.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6363.76 ms /   428 tokens (   14.87 ms per token,    67.26 tokens per second)\n",
      "llama_print_timings:        eval time =   45322.39 ms /   255 runs   (  177.73 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   52505.19 ms /   683 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      36.09 ms /   234 runs   (    0.15 ms per token,  6483.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6234.23 ms /   401 tokens (   15.55 ms per token,    64.32 tokens per second)\n",
      "llama_print_timings:        eval time =   40555.49 ms /   233 runs   (  174.06 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   47543.17 ms /   634 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.02 ms /   256 runs   (    0.15 ms per token,  6560.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3432.06 ms /   211 tokens (   16.27 ms per token,    61.48 tokens per second)\n",
      "llama_print_timings:        eval time =   45217.98 ms /   255 runs   (  177.33 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   49466.07 ms /   466 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      44.05 ms /   256 runs   (    0.17 ms per token,  5811.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11254.14 ms /   706 tokens (   15.94 ms per token,    62.73 tokens per second)\n",
      "llama_print_timings:        eval time =   45402.20 ms /   255 runs   (  178.05 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   57572.45 ms /   961 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.45 ms /   256 runs   (    0.15 ms per token,  6488.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7783.22 ms /   498 tokens (   15.63 ms per token,    63.98 tokens per second)\n",
      "llama_print_timings:        eval time =   43989.76 ms /   255 runs   (  172.51 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:       total time =   52642.76 ms /   753 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.29 ms /   256 runs   (    0.15 ms per token,  6515.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3382.88 ms /   198 tokens (   17.09 ms per token,    58.53 tokens per second)\n",
      "llama_print_timings:        eval time =   42455.08 ms /   255 runs   (  166.49 ms per token,     6.01 tokens per second)\n",
      "llama_print_timings:       total time =   46664.37 ms /   453 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      36.74 ms /   221 runs   (    0.17 ms per token,  6015.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7507.62 ms /   476 tokens (   15.77 ms per token,    63.40 tokens per second)\n",
      "llama_print_timings:        eval time =   39047.65 ms /   220 runs   (  177.49 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   47277.06 ms /   696 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.36 ms /   139 runs   (    0.18 ms per token,  5706.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5959.64 ms /   393 tokens (   15.16 ms per token,    65.94 tokens per second)\n",
      "llama_print_timings:        eval time =   24344.57 ms /   138 runs   (  176.41 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   30768.91 ms /   531 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.31 ms /   256 runs   (    0.15 ms per token,  6511.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7208.76 ms /   456 tokens (   15.81 ms per token,    63.26 tokens per second)\n",
      "llama_print_timings:        eval time =   44393.13 ms /   255 runs   (  174.09 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   52419.06 ms /   711 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      18.73 ms /   124 runs   (    0.15 ms per token,  6622.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9809.23 ms /   635 tokens (   15.45 ms per token,    64.73 tokens per second)\n",
      "llama_print_timings:        eval time =   22214.70 ms /   123 runs   (  180.61 ms per token,     5.54 tokens per second)\n",
      "llama_print_timings:       total time =   32401.99 ms /   758 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      20.74 ms /   136 runs   (    0.15 ms per token,  6558.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3518.06 ms /   218 tokens (   16.14 ms per token,    61.97 tokens per second)\n",
      "llama_print_timings:        eval time =   23221.29 ms /   135 runs   (  172.01 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   27157.08 ms /   353 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.77 ms /   160 runs   (    0.15 ms per token,  6459.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6402.60 ms /   400 tokens (   16.01 ms per token,    62.47 tokens per second)\n",
      "llama_print_timings:        eval time =   27685.30 ms /   159 runs   (  174.12 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   34609.21 ms /   559 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.99 ms /   256 runs   (    0.16 ms per token,  6401.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6569.56 ms /   395 tokens (   16.63 ms per token,    60.13 tokens per second)\n",
      "llama_print_timings:        eval time =   44912.76 ms /   255 runs   (  176.13 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   52316.59 ms /   650 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.23 ms /   256 runs   (    0.16 ms per token,  6363.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6563.85 ms /   400 tokens (   16.41 ms per token,    60.94 tokens per second)\n",
      "llama_print_timings:        eval time =   44931.65 ms /   255 runs   (  176.20 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   52342.57 ms /   655 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.24 ms /   256 runs   (    0.15 ms per token,  6524.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6420.99 ms /   402 tokens (   15.97 ms per token,    62.61 tokens per second)\n",
      "llama_print_timings:        eval time =   45664.50 ms /   255 runs   (  179.08 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   52907.97 ms /   657 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.77 ms /   256 runs   (    0.16 ms per token,  6437.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9591.18 ms /   614 tokens (   15.62 ms per token,    64.02 tokens per second)\n",
      "llama_print_timings:        eval time =   46754.17 ms /   255 runs   (  183.35 ms per token,     5.45 tokens per second)\n",
      "llama_print_timings:       total time =   57193.73 ms /   869 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.53 ms /   256 runs   (    0.15 ms per token,  6476.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11438.66 ms /   684 tokens (   16.72 ms per token,    59.80 tokens per second)\n",
      "llama_print_timings:        eval time =   45923.81 ms /   255 runs   (  180.09 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:       total time =   58209.52 ms /   939 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.37 ms /   256 runs   (    0.16 ms per token,  6341.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6558.70 ms /   393 tokens (   16.69 ms per token,    59.92 tokens per second)\n",
      "llama_print_timings:        eval time =   45051.88 ms /   255 runs   (  176.67 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   52441.97 ms /   648 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.88 ms /   256 runs   (    0.15 ms per token,  6584.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5039.34 ms /   317 tokens (   15.90 ms per token,    62.91 tokens per second)\n",
      "llama_print_timings:        eval time =   44010.29 ms /   255 runs   (  172.59 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   49886.99 ms /   572 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.48 ms /   256 runs   (    0.16 ms per token,  6324.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15093.73 ms /   974 tokens (   15.50 ms per token,    64.53 tokens per second)\n",
      "llama_print_timings:        eval time =   44908.67 ms /   255 runs   (  176.11 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   60873.61 ms /  1229 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      41.28 ms /   256 runs   (    0.16 ms per token,  6202.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13200.68 ms /   831 tokens (   15.89 ms per token,    62.95 tokens per second)\n",
      "llama_print_timings:        eval time =   45373.86 ms /   255 runs   (  177.94 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   59443.66 ms /  1086 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.14 ms /   256 runs   (    0.15 ms per token,  6539.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14427.41 ms /   922 tokens (   15.65 ms per token,    63.91 tokens per second)\n",
      "llama_print_timings:        eval time =   46813.21 ms /   255 runs   (  183.58 ms per token,     5.45 tokens per second)\n",
      "llama_print_timings:       total time =   62101.44 ms /  1177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      41.12 ms /   256 runs   (    0.16 ms per token,  6225.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10942.06 ms /   653 tokens (   16.76 ms per token,    59.68 tokens per second)\n",
      "llama_print_timings:        eval time =   45230.83 ms /   255 runs   (  177.38 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   57023.83 ms /   908 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.93 ms /   256 runs   (    0.16 ms per token,  6411.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14162.21 ms /   888 tokens (   15.95 ms per token,    62.70 tokens per second)\n",
      "llama_print_timings:        eval time =   45587.54 ms /   255 runs   (  178.77 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   60594.36 ms /  1143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      12.78 ms /    80 runs   (    0.16 ms per token,  6261.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13552.18 ms /   816 tokens (   16.61 ms per token,    60.21 tokens per second)\n",
      "llama_print_timings:        eval time =   14404.64 ms /    79 runs   (  182.34 ms per token,     5.48 tokens per second)\n",
      "llama_print_timings:       total time =   28213.60 ms /   895 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      10.17 ms /    67 runs   (    0.15 ms per token,  6588.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6906.36 ms /   442 tokens (   15.63 ms per token,    64.00 tokens per second)\n",
      "llama_print_timings:        eval time =   10810.09 ms /    66 runs   (  163.79 ms per token,     6.11 tokens per second)\n",
      "llama_print_timings:       total time =   17939.43 ms /   508 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.06 ms /   256 runs   (    0.15 ms per token,  6554.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10413.90 ms /   623 tokens (   16.72 ms per token,    59.82 tokens per second)\n",
      "llama_print_timings:        eval time =   44323.23 ms /   255 runs   (  173.82 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   55575.26 ms /   878 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.07 ms /   256 runs   (    0.15 ms per token,  6551.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10258.53 ms /   660 tokens (   15.54 ms per token,    64.34 tokens per second)\n",
      "llama_print_timings:        eval time =   44360.37 ms /   255 runs   (  173.96 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   55445.56 ms /   915 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.03 ms /   256 runs   (    0.15 ms per token,  6731.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2998.29 ms /   181 tokens (   16.57 ms per token,    60.37 tokens per second)\n",
      "llama_print_timings:        eval time =   45142.05 ms /   255 runs   (  177.03 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   48932.09 ms /   436 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      35.69 ms /   233 runs   (    0.15 ms per token,  6528.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5991.86 ms /   381 tokens (   15.73 ms per token,    63.59 tokens per second)\n",
      "llama_print_timings:        eval time =   39811.53 ms /   232 runs   (  171.60 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   46559.48 ms /   613 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.91 ms /   159 runs   (    0.16 ms per token,  6382.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5980.86 ms /   354 tokens (   16.90 ms per token,    59.19 tokens per second)\n",
      "llama_print_timings:        eval time =   27599.26 ms /   158 runs   (  174.68 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   34105.47 ms /   512 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      23.86 ms /   158 runs   (    0.15 ms per token,  6621.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5377.87 ms /   352 tokens (   15.28 ms per token,    65.45 tokens per second)\n",
      "llama_print_timings:        eval time =   28477.93 ms /   157 runs   (  181.39 ms per token,     5.51 tokens per second)\n",
      "llama_print_timings:       total time =   34348.17 ms /   509 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.24 ms /   256 runs   (    0.15 ms per token,  6523.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5391.52 ms /   355 tokens (   15.19 ms per token,    65.84 tokens per second)\n",
      "llama_print_timings:        eval time =   45542.78 ms /   255 runs   (  178.60 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   51760.10 ms /   610 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.31 ms /   256 runs   (    0.15 ms per token,  6512.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2086.49 ms /   115 tokens (   18.14 ms per token,    55.12 tokens per second)\n",
      "llama_print_timings:        eval time =   43791.88 ms /   255 runs   (  171.73 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   46673.20 ms /   370 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.64 ms /   256 runs   (    0.15 ms per token,  6624.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6982.39 ms /   471 tokens (   14.82 ms per token,    67.46 tokens per second)\n",
      "llama_print_timings:        eval time =   45472.36 ms /   255 runs   (  178.32 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   53244.59 ms /   726 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.40 ms /   256 runs   (    0.15 ms per token,  6497.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5500.72 ms /   356 tokens (   15.45 ms per token,    64.72 tokens per second)\n",
      "llama_print_timings:        eval time =   44922.85 ms /   255 runs   (  176.17 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   51244.39 ms /   611 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.07 ms /   256 runs   (    0.15 ms per token,  6551.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6971.60 ms /   470 tokens (   14.83 ms per token,    67.42 tokens per second)\n",
      "llama_print_timings:        eval time =   44253.86 ms /   255 runs   (  173.54 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   52034.30 ms /   725 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.56 ms /   256 runs   (    0.15 ms per token,  6471.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8640.78 ms /   561 tokens (   15.40 ms per token,    64.92 tokens per second)\n",
      "llama_print_timings:        eval time =   45179.69 ms /   255 runs   (  177.18 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   54637.06 ms /   816 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      23.84 ms /   157 runs   (    0.15 ms per token,  6585.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7108.75 ms /   446 tokens (   15.94 ms per token,    62.74 tokens per second)\n",
      "llama_print_timings:        eval time =   27116.87 ms /   156 runs   (  173.83 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   34708.04 ms /   602 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.83 ms /   256 runs   (    0.15 ms per token,  6592.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6720.90 ms /   411 tokens (   16.35 ms per token,    61.15 tokens per second)\n",
      "llama_print_timings:        eval time =   44486.78 ms /   255 runs   (  174.46 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   52002.81 ms /   666 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.40 ms /   256 runs   (    0.15 ms per token,  6497.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7188.34 ms /   473 tokens (   15.20 ms per token,    65.80 tokens per second)\n",
      "llama_print_timings:        eval time =   44432.84 ms /   255 runs   (  174.25 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   52429.09 ms /   728 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.02 ms /   256 runs   (    0.16 ms per token,  6396.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6933.52 ms /   444 tokens (   15.62 ms per token,    64.04 tokens per second)\n",
      "llama_print_timings:        eval time =   45559.80 ms /   255 runs   (  178.67 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   53319.03 ms /   699 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.10 ms /   256 runs   (    0.15 ms per token,  6547.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6875.58 ms /   449 tokens (   15.31 ms per token,    65.30 tokens per second)\n",
      "llama_print_timings:        eval time =   44700.72 ms /   255 runs   (  175.30 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   52379.35 ms /   704 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.33 ms /   256 runs   (    0.15 ms per token,  6679.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5671.37 ms /   331 tokens (   17.13 ms per token,    58.36 tokens per second)\n",
      "llama_print_timings:        eval time =   45207.14 ms /   255 runs   (  177.28 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   51656.22 ms /   586 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      32.17 ms /   204 runs   (    0.16 ms per token,  6341.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7390.68 ms /   479 tokens (   15.43 ms per token,    64.81 tokens per second)\n",
      "llama_print_timings:        eval time =   35946.42 ms /   203 runs   (  177.08 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   43980.71 ms /   682 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.97 ms /   256 runs   (    0.15 ms per token,  6569.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5729.70 ms /   360 tokens (   15.92 ms per token,    62.83 tokens per second)\n",
      "llama_print_timings:        eval time =   44549.05 ms /   255 runs   (  174.70 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   51069.56 ms /   615 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      36.30 ms /   232 runs   (    0.16 ms per token,  6392.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7251.73 ms /   473 tokens (   15.33 ms per token,    65.23 tokens per second)\n",
      "llama_print_timings:        eval time =   39278.17 ms /   231 runs   (  170.04 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   47282.54 ms /   704 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.25 ms /   256 runs   (    0.15 ms per token,  6522.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10365.60 ms /   642 tokens (   16.15 ms per token,    61.94 tokens per second)\n",
      "llama_print_timings:        eval time =   44924.30 ms /   255 runs   (  176.17 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   56091.47 ms /   897 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.65 ms /   256 runs   (    0.15 ms per token,  6456.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7059.77 ms /   468 tokens (   15.08 ms per token,    66.29 tokens per second)\n",
      "llama_print_timings:        eval time =   43261.18 ms /   255 runs   (  169.65 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =   51131.56 ms /   723 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      22.18 ms /   145 runs   (    0.15 ms per token,  6536.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8609.11 ms /   554 tokens (   15.54 ms per token,    64.35 tokens per second)\n",
      "llama_print_timings:        eval time =   25578.85 ms /   144 runs   (  177.63 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   34634.63 ms /   698 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      28.97 ms /   185 runs   (    0.16 ms per token,  6385.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10463.57 ms /   662 tokens (   15.81 ms per token,    63.27 tokens per second)\n",
      "llama_print_timings:        eval time =   31803.48 ms /   184 runs   (  172.84 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   42844.19 ms /   846 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.59 ms /   256 runs   (    0.15 ms per token,  6466.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9251.42 ms /   554 tokens (   16.70 ms per token,    59.88 tokens per second)\n",
      "llama_print_timings:        eval time =   43692.82 ms /   255 runs   (  171.34 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   53787.77 ms /   809 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      23.21 ms /   153 runs   (    0.15 ms per token,  6591.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9259.22 ms /   559 tokens (   16.56 ms per token,    60.37 tokens per second)\n",
      "llama_print_timings:        eval time =   27170.07 ms /   152 runs   (  178.75 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   36911.78 ms /   711 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.92 ms /   256 runs   (    0.15 ms per token,  6577.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9136.04 ms /   593 tokens (   15.41 ms per token,    64.91 tokens per second)\n",
      "llama_print_timings:        eval time =   43689.94 ms /   255 runs   (  171.33 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   53661.69 ms /   848 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.30 ms /   256 runs   (    0.15 ms per token,  6514.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8652.29 ms /   544 tokens (   15.90 ms per token,    62.87 tokens per second)\n",
      "llama_print_timings:        eval time =   44167.18 ms /   255 runs   (  173.20 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   53635.94 ms /   799 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.06 ms /   256 runs   (    0.16 ms per token,  6390.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7169.74 ms /   449 tokens (   15.97 ms per token,    62.62 tokens per second)\n",
      "llama_print_timings:        eval time =   43533.16 ms /   255 runs   (  170.72 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   51543.94 ms /   704 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      17.22 ms /   115 runs   (    0.15 ms per token,  6677.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7544.10 ms /   490 tokens (   15.40 ms per token,    64.95 tokens per second)\n",
      "llama_print_timings:        eval time =   20460.04 ms /   114 runs   (  179.47 ms per token,     5.57 tokens per second)\n",
      "llama_print_timings:       total time =   28348.62 ms /   604 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.43 ms /   256 runs   (    0.15 ms per token,  6662.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7901.78 ms /   515 tokens (   15.34 ms per token,    65.18 tokens per second)\n",
      "llama_print_timings:        eval time =   45246.38 ms /   255 runs   (  177.44 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   53961.37 ms /   770 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.96 ms /   256 runs   (    0.15 ms per token,  6571.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7461.99 ms /   499 tokens (   14.95 ms per token,    66.87 tokens per second)\n",
      "llama_print_timings:        eval time =   44330.08 ms /   255 runs   (  173.84 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   52598.28 ms /   754 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      30.30 ms /   201 runs   (    0.15 ms per token,  6633.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6455.11 ms /   432 tokens (   14.94 ms per token,    66.92 tokens per second)\n",
      "llama_print_timings:        eval time =   33802.65 ms /   200 runs   (  169.01 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:       total time =   40877.85 ms /   632 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.95 ms /   256 runs   (    0.16 ms per token,  6408.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8717.28 ms /   560 tokens (   15.57 ms per token,    64.24 tokens per second)\n",
      "llama_print_timings:        eval time =   42183.94 ms /   255 runs   (  165.43 ms per token,     6.04 tokens per second)\n",
      "llama_print_timings:       total time =   51725.30 ms /   815 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.23 ms /   251 runs   (    0.15 ms per token,  6566.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9038.65 ms /   559 tokens (   16.17 ms per token,    61.85 tokens per second)\n",
      "llama_print_timings:        eval time =   44056.17 ms /   250 runs   (  176.22 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   53897.68 ms /   809 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.58 ms /   256 runs   (    0.15 ms per token,  6468.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8263.60 ms /   519 tokens (   15.92 ms per token,    62.81 tokens per second)\n",
      "llama_print_timings:        eval time =   44845.24 ms /   255 runs   (  175.86 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   53936.12 ms /   774 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      36.82 ms /   238 runs   (    0.15 ms per token,  6464.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5218.18 ms /   322 tokens (   16.21 ms per token,    61.71 tokens per second)\n",
      "llama_print_timings:        eval time =   41183.38 ms /   237 runs   (  173.77 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   47154.28 ms /   559 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.88 ms /   256 runs   (    0.16 ms per token,  6419.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6697.76 ms /   440 tokens (   15.22 ms per token,    65.69 tokens per second)\n",
      "llama_print_timings:        eval time =   43589.02 ms /   255 runs   (  170.94 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   51132.09 ms /   695 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      35.08 ms /   229 runs   (    0.15 ms per token,  6528.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5887.28 ms /   377 tokens (   15.62 ms per token,    64.04 tokens per second)\n",
      "llama_print_timings:        eval time =   40622.00 ms /   228 runs   (  178.17 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   47239.13 ms /   605 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.60 ms /   256 runs   (    0.15 ms per token,  6632.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5132.03 ms /   335 tokens (   15.32 ms per token,    65.28 tokens per second)\n",
      "llama_print_timings:        eval time =   43865.16 ms /   255 runs   (  172.02 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   49804.97 ms /   590 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      37.55 ms /   237 runs   (    0.16 ms per token,  6312.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5600.75 ms /   331 tokens (   16.92 ms per token,    59.10 tokens per second)\n",
      "llama_print_timings:        eval time =   41005.39 ms /   236 runs   (  173.75 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   47380.55 ms /   567 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.58 ms /   256 runs   (    0.15 ms per token,  6635.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5904.23 ms /   357 tokens (   16.54 ms per token,    60.47 tokens per second)\n",
      "llama_print_timings:        eval time =   43376.31 ms /   255 runs   (  170.10 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   50078.21 ms /   612 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      27.55 ms /   178 runs   (    0.15 ms per token,  6461.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5213.28 ms /   340 tokens (   15.33 ms per token,    65.22 tokens per second)\n",
      "llama_print_timings:        eval time =   31094.51 ms /   177 runs   (  175.68 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   36884.95 ms /   517 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      32.30 ms /   211 runs   (    0.15 ms per token,  6533.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5165.36 ms /   337 tokens (   15.33 ms per token,    65.24 tokens per second)\n",
      "llama_print_timings:        eval time =   35610.64 ms /   210 runs   (  169.57 ms per token,     5.90 tokens per second)\n",
      "llama_print_timings:       total time =   41448.95 ms /   547 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      34.15 ms /   227 runs   (    0.15 ms per token,  6647.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5424.82 ms /   347 tokens (   15.63 ms per token,    63.97 tokens per second)\n",
      "llama_print_timings:        eval time =   38776.92 ms /   226 runs   (  171.58 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   44936.17 ms /   573 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      26.14 ms /   171 runs   (    0.15 ms per token,  6542.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6194.51 ms /   382 tokens (   16.22 ms per token,    61.67 tokens per second)\n",
      "llama_print_timings:        eval time =   29570.33 ms /   170 runs   (  173.94 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   36305.59 ms /   552 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.01 ms /   256 runs   (    0.15 ms per token,  6562.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6011.69 ms /   396 tokens (   15.18 ms per token,    65.87 tokens per second)\n",
      "llama_print_timings:        eval time =   43702.15 ms /   255 runs   (  171.38 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   50549.49 ms /   651 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.85 ms /   256 runs   (    0.16 ms per token,  6424.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4619.32 ms /   293 tokens (   15.77 ms per token,    63.43 tokens per second)\n",
      "llama_print_timings:        eval time =   44901.63 ms /   255 runs   (  176.08 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   50363.18 ms /   548 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      31.31 ms /   199 runs   (    0.16 ms per token,  6355.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4915.38 ms /   313 tokens (   15.70 ms per token,    63.68 tokens per second)\n",
      "llama_print_timings:        eval time =   32949.47 ms /   198 runs   (  166.41 ms per token,     6.01 tokens per second)\n",
      "llama_print_timings:       total time =   38517.20 ms /   511 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.25 ms /   256 runs   (    0.15 ms per token,  6522.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4124.12 ms /   260 tokens (   15.86 ms per token,    63.04 tokens per second)\n",
      "llama_print_timings:        eval time =   44517.69 ms /   255 runs   (  174.58 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   49451.03 ms /   515 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.43 ms /   256 runs   (    0.15 ms per token,  6492.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11032.41 ms /   685 tokens (   16.11 ms per token,    62.09 tokens per second)\n",
      "llama_print_timings:        eval time =   44526.76 ms /   255 runs   (  174.61 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   56411.97 ms /   940 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.86 ms /   256 runs   (    0.15 ms per token,  6587.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14829.66 ms /   956 tokens (   15.51 ms per token,    64.47 tokens per second)\n",
      "llama_print_timings:        eval time =   45603.31 ms /   255 runs   (  178.84 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   61267.63 ms /  1211 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      16.31 ms /   105 runs   (    0.16 ms per token,  6438.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10854.82 ms /   693 tokens (   15.66 ms per token,    63.84 tokens per second)\n",
      "llama_print_timings:        eval time =   18755.10 ms /   104 runs   (  180.34 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:       total time =   29948.75 ms /   797 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.29 ms /   256 runs   (    0.15 ms per token,  6686.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9030.77 ms /   565 tokens (   15.98 ms per token,    62.56 tokens per second)\n",
      "llama_print_timings:        eval time =   46122.19 ms /   255 runs   (  180.87 ms per token,     5.53 tokens per second)\n",
      "llama_print_timings:       total time =   55970.72 ms /   820 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.44 ms /   256 runs   (    0.15 ms per token,  6490.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6561.85 ms /   425 tokens (   15.44 ms per token,    64.77 tokens per second)\n",
      "llama_print_timings:        eval time =   44842.05 ms /   255 runs   (  175.85 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   52258.46 ms /   680 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.56 ms /   256 runs   (    0.15 ms per token,  6471.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7721.75 ms /   495 tokens (   15.60 ms per token,    64.10 tokens per second)\n",
      "llama_print_timings:        eval time =   44332.81 ms /   255 runs   (  173.85 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   52922.23 ms /   750 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.63 ms /   256 runs   (    0.15 ms per token,  6460.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6317.94 ms /   419 tokens (   15.08 ms per token,    66.32 tokens per second)\n",
      "llama_print_timings:        eval time =   44387.31 ms /   255 runs   (  174.07 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   51583.73 ms /   674 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      35.45 ms /   232 runs   (    0.15 ms per token,  6544.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5426.42 ms /   352 tokens (   15.42 ms per token,    64.87 tokens per second)\n",
      "llama_print_timings:        eval time =   40141.60 ms /   231 runs   (  173.77 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   46324.19 ms /   583 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.62 ms /   256 runs   (    0.15 ms per token,  6628.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5024.48 ms /   321 tokens (   15.65 ms per token,    63.89 tokens per second)\n",
      "llama_print_timings:        eval time =   44770.79 ms /   255 runs   (  175.57 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   50627.30 ms /   576 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.69 ms /   256 runs   (    0.15 ms per token,  6616.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5679.48 ms /   372 tokens (   15.27 ms per token,    65.50 tokens per second)\n",
      "llama_print_timings:        eval time =   44686.13 ms /   255 runs   (  175.24 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   51189.31 ms /   627 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.21 ms /   256 runs   (    0.15 ms per token,  6529.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6844.62 ms /   400 tokens (   17.11 ms per token,    58.44 tokens per second)\n",
      "llama_print_timings:        eval time =   44628.01 ms /   255 runs   (  175.01 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   52313.99 ms /   655 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.77 ms /   256 runs   (    0.15 ms per token,  6602.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5216.86 ms /   342 tokens (   15.25 ms per token,    65.56 tokens per second)\n",
      "llama_print_timings:        eval time =   45123.22 ms /   255 runs   (  176.95 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   51146.01 ms /   597 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      18.85 ms /   125 runs   (    0.15 ms per token,  6630.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5690.22 ms /   365 tokens (   15.59 ms per token,    64.15 tokens per second)\n",
      "llama_print_timings:        eval time =   21402.56 ms /   124 runs   (  172.60 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   27476.44 ms /   489 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.95 ms /   256 runs   (    0.16 ms per token,  6408.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6486.66 ms /   379 tokens (   17.12 ms per token,    58.43 tokens per second)\n",
      "llama_print_timings:        eval time =   43652.06 ms /   255 runs   (  171.18 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   50970.43 ms /   634 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      27.75 ms /   182 runs   (    0.15 ms per token,  6558.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5376.01 ms /   348 tokens (   15.45 ms per token,    64.73 tokens per second)\n",
      "llama_print_timings:        eval time =   31154.15 ms /   181 runs   (  172.12 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   37092.53 ms /   529 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.57 ms /   256 runs   (    0.15 ms per token,  6469.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5509.76 ms /   351 tokens (   15.70 ms per token,    63.71 tokens per second)\n",
      "llama_print_timings:        eval time =   43783.47 ms /   255 runs   (  171.70 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   50137.98 ms /   606 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.84 ms /   256 runs   (    0.15 ms per token,  6590.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5669.16 ms /   374 tokens (   15.16 ms per token,    65.97 tokens per second)\n",
      "llama_print_timings:        eval time =   44102.38 ms /   255 runs   (  172.95 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   50601.69 ms /   629 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.13 ms /   256 runs   (    0.15 ms per token,  6714.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6052.51 ms /   404 tokens (   14.98 ms per token,    66.75 tokens per second)\n",
      "llama_print_timings:        eval time =   45294.34 ms /   255 runs   (  177.62 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   52171.41 ms /   659 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.69 ms /   256 runs   (    0.16 ms per token,  6449.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6174.21 ms /   380 tokens (   16.25 ms per token,    61.55 tokens per second)\n",
      "llama_print_timings:        eval time =   44429.00 ms /   255 runs   (  174.23 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   51445.08 ms /   635 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.84 ms /   256 runs   (    0.15 ms per token,  6591.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5691.16 ms /   381 tokens (   14.94 ms per token,    66.95 tokens per second)\n",
      "llama_print_timings:        eval time =   45454.31 ms /   255 runs   (  178.25 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   51967.39 ms /   636 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.18 ms /   256 runs   (    0.15 ms per token,  6533.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3909.58 ms /   240 tokens (   16.29 ms per token,    61.39 tokens per second)\n",
      "llama_print_timings:        eval time =   43837.29 ms /   255 runs   (  171.91 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   48589.67 ms /   495 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.30 ms /   256 runs   (    0.15 ms per token,  6514.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6859.03 ms /   442 tokens (   15.52 ms per token,    64.44 tokens per second)\n",
      "llama_print_timings:        eval time =   44847.67 ms /   255 runs   (  175.87 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   52575.97 ms /   697 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.20 ms /   256 runs   (    0.15 ms per token,  6530.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6346.75 ms /   407 tokens (   15.59 ms per token,    64.13 tokens per second)\n",
      "llama_print_timings:        eval time =   45403.58 ms /   255 runs   (  178.05 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   52577.06 ms /   662 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      31.99 ms /   206 runs   (    0.16 ms per token,  6439.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7018.16 ms /   454 tokens (   15.46 ms per token,    64.69 tokens per second)\n",
      "llama_print_timings:        eval time =   35801.80 ms /   205 runs   (  174.64 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   43517.43 ms /   659 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.68 ms /   256 runs   (    0.15 ms per token,  6452.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12643.93 ms /   782 tokens (   16.17 ms per token,    61.85 tokens per second)\n",
      "llama_print_timings:        eval time =   45441.18 ms /   255 runs   (  178.20 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   58975.21 ms /  1037 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.79 ms /   256 runs   (    0.15 ms per token,  6599.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9041.31 ms /   566 tokens (   15.97 ms per token,    62.60 tokens per second)\n",
      "llama_print_timings:        eval time =   44373.14 ms /   255 runs   (  174.01 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   54276.70 ms /   821 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.94 ms /   256 runs   (    0.16 ms per token,  6409.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10940.41 ms /   670 tokens (   16.33 ms per token,    61.24 tokens per second)\n",
      "llama_print_timings:        eval time =   44283.31 ms /   255 runs   (  173.66 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   56096.74 ms /   925 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.01 ms /   256 runs   (    0.15 ms per token,  6562.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6860.58 ms /   452 tokens (   15.18 ms per token,    65.88 tokens per second)\n",
      "llama_print_timings:        eval time =   44261.89 ms /   255 runs   (  173.58 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   52007.00 ms /   707 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.81 ms /   256 runs   (    0.15 ms per token,  6595.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5721.51 ms /   361 tokens (   15.85 ms per token,    63.10 tokens per second)\n",
      "llama_print_timings:        eval time =   43380.63 ms /   255 runs   (  170.12 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   49972.49 ms /   616 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.26 ms /   256 runs   (    0.15 ms per token,  6690.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12270.46 ms /   772 tokens (   15.89 ms per token,    62.92 tokens per second)\n",
      "llama_print_timings:        eval time =   46026.41 ms /   255 runs   (  180.50 ms per token,     5.54 tokens per second)\n",
      "llama_print_timings:       total time =   59127.25 ms /  1027 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.96 ms /   256 runs   (    0.16 ms per token,  6405.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13062.45 ms /   844 tokens (   15.48 ms per token,    64.61 tokens per second)\n",
      "llama_print_timings:        eval time =   46136.42 ms /   255 runs   (  180.93 ms per token,     5.53 tokens per second)\n",
      "llama_print_timings:       total time =   60053.39 ms /  1099 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.95 ms /   256 runs   (    0.16 ms per token,  6407.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6064.02 ms /   364 tokens (   16.66 ms per token,    60.03 tokens per second)\n",
      "llama_print_timings:        eval time =   43608.68 ms /   255 runs   (  171.01 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   50523.64 ms /   619 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.95 ms /   256 runs   (    0.15 ms per token,  6573.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7572.12 ms /   482 tokens (   15.71 ms per token,    63.65 tokens per second)\n",
      "llama_print_timings:        eval time =   43696.12 ms /   255 runs   (  171.36 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   52098.19 ms /   737 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.74 ms /   256 runs   (    0.15 ms per token,  6608.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10509.44 ms /   677 tokens (   15.52 ms per token,    64.42 tokens per second)\n",
      "llama_print_timings:        eval time =   45057.59 ms /   255 runs   (  176.70 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   56407.12 ms /   932 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.68 ms /   256 runs   (    0.15 ms per token,  6619.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14197.96 ms /   894 tokens (   15.88 ms per token,    62.97 tokens per second)\n",
      "llama_print_timings:        eval time =   46583.09 ms /   255 runs   (  182.68 ms per token,     5.47 tokens per second)\n",
      "llama_print_timings:       total time =   61636.49 ms /  1149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.48 ms /   256 runs   (    0.15 ms per token,  6652.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11733.23 ms /   760 tokens (   15.44 ms per token,    64.77 tokens per second)\n",
      "llama_print_timings:        eval time =   46441.33 ms /   255 runs   (  182.12 ms per token,     5.49 tokens per second)\n",
      "llama_print_timings:       total time =   59018.60 ms /  1015 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.08 ms /   256 runs   (    0.15 ms per token,  6550.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6609.13 ms /   436 tokens (   15.16 ms per token,    65.97 tokens per second)\n",
      "llama_print_timings:        eval time =   43335.75 ms /   255 runs   (  169.94 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   50798.75 ms /   691 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.69 ms /   256 runs   (    0.16 ms per token,  6450.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6020.24 ms /   384 tokens (   15.68 ms per token,    63.78 tokens per second)\n",
      "llama_print_timings:        eval time =   44748.21 ms /   255 runs   (  175.48 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   51599.08 ms /   639 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.52 ms /   256 runs   (    0.15 ms per token,  6646.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12805.45 ms /   823 tokens (   15.56 ms per token,    64.27 tokens per second)\n",
      "llama_print_timings:        eval time =   45441.40 ms /   255 runs   (  178.20 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   59130.70 ms /  1078 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.44 ms /   256 runs   (    0.15 ms per token,  6490.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6739.75 ms /   407 tokens (   16.56 ms per token,    60.39 tokens per second)\n",
      "llama_print_timings:        eval time =   44380.40 ms /   255 runs   (  174.04 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   51978.09 ms /   662 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      21.05 ms /   134 runs   (    0.16 ms per token,  6366.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6337.21 ms /   420 tokens (   15.09 ms per token,    66.28 tokens per second)\n",
      "llama_print_timings:        eval time =   23224.29 ms /   133 runs   (  174.62 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   29995.34 ms /   553 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.92 ms /   256 runs   (    0.15 ms per token,  6576.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6210.40 ms /   392 tokens (   15.84 ms per token,    63.12 tokens per second)\n",
      "llama_print_timings:        eval time =   45509.16 ms /   255 runs   (  178.47 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   52544.66 ms /   647 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.76 ms /   256 runs   (    0.15 ms per token,  6605.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8806.32 ms /   542 tokens (   16.25 ms per token,    61.55 tokens per second)\n",
      "llama_print_timings:        eval time =   43891.39 ms /   255 runs   (  172.12 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   53514.61 ms /   797 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.95 ms /   256 runs   (    0.15 ms per token,  6572.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6417.12 ms /   410 tokens (   15.65 ms per token,    63.89 tokens per second)\n",
      "llama_print_timings:        eval time =   44259.90 ms /   255 runs   (  173.57 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   51504.82 ms /   665 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.10 ms /   256 runs   (    0.15 ms per token,  6547.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12075.86 ms /   799 tokens (   15.11 ms per token,    66.17 tokens per second)\n",
      "llama_print_timings:        eval time =   47537.14 ms /   255 runs   (  186.42 ms per token,     5.36 tokens per second)\n",
      "llama_print_timings:       total time =   60498.38 ms /  1054 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.57 ms /   256 runs   (    0.15 ms per token,  6469.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5914.09 ms /   380 tokens (   15.56 ms per token,    64.25 tokens per second)\n",
      "llama_print_timings:        eval time =   43690.22 ms /   255 runs   (  171.33 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   50480.49 ms /   635 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.89 ms /   256 runs   (    0.16 ms per token,  6417.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7295.29 ms /   479 tokens (   15.23 ms per token,    65.66 tokens per second)\n",
      "llama_print_timings:        eval time =   43747.58 ms /   255 runs   (  171.56 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   51892.62 ms /   734 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      35.83 ms /   236 runs   (    0.15 ms per token,  6586.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8557.19 ms /   544 tokens (   15.73 ms per token,    63.57 tokens per second)\n",
      "llama_print_timings:        eval time =   41445.22 ms /   235 runs   (  176.36 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   50766.34 ms /   779 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.73 ms /   256 runs   (    0.15 ms per token,  6610.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5297.93 ms /   323 tokens (   16.40 ms per token,    60.97 tokens per second)\n",
      "llama_print_timings:        eval time =   44029.68 ms /   255 runs   (  172.67 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   50139.49 ms /   578 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.85 ms /   256 runs   (    0.16 ms per token,  6424.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5897.76 ms /   346 tokens (   17.05 ms per token,    58.67 tokens per second)\n",
      "llama_print_timings:        eval time =   45043.93 ms /   255 runs   (  176.64 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   51796.77 ms /   601 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.81 ms /   256 runs   (    0.16 ms per token,  6430.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6091.85 ms /   389 tokens (   15.66 ms per token,    63.86 tokens per second)\n",
      "llama_print_timings:        eval time =   44542.03 ms /   255 runs   (  174.67 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   51482.27 ms /   644 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.21 ms /   256 runs   (    0.15 ms per token,  6528.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5924.24 ms /   360 tokens (   16.46 ms per token,    60.77 tokens per second)\n",
      "llama_print_timings:        eval time =   43545.86 ms /   255 runs   (  170.77 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   50314.46 ms /   615 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      28.12 ms /   183 runs   (    0.15 ms per token,  6507.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6288.03 ms /   378 tokens (   16.63 ms per token,    60.11 tokens per second)\n",
      "llama_print_timings:        eval time =   31866.91 ms /   182 runs   (  175.09 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   38760.04 ms /   560 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.24 ms /   256 runs   (    0.15 ms per token,  6523.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5992.97 ms /   399 tokens (   15.02 ms per token,    66.58 tokens per second)\n",
      "llama_print_timings:        eval time =   45089.13 ms /   255 runs   (  176.82 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   51945.87 ms /   654 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      29.60 ms /   197 runs   (    0.15 ms per token,  6654.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5824.09 ms /   383 tokens (   15.21 ms per token,    65.76 tokens per second)\n",
      "llama_print_timings:        eval time =   34307.18 ms /   196 runs   (  175.04 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   40755.16 ms /   579 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.33 ms /   256 runs   (    0.15 ms per token,  6678.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6747.32 ms /   432 tokens (   15.62 ms per token,    64.03 tokens per second)\n",
      "llama_print_timings:        eval time =   45089.15 ms /   255 runs   (  176.82 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   52678.53 ms /   687 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.71 ms /   256 runs   (    0.15 ms per token,  6613.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6555.29 ms /   437 tokens (   15.00 ms per token,    66.66 tokens per second)\n",
      "llama_print_timings:        eval time =   45395.57 ms /   255 runs   (  178.02 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   52770.99 ms /   692 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      34.55 ms /   231 runs   (    0.15 ms per token,  6685.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5763.07 ms /   379 tokens (   15.21 ms per token,    65.76 tokens per second)\n",
      "llama_print_timings:        eval time =   40372.67 ms /   230 runs   (  175.53 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   46860.76 ms /   609 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.58 ms /   256 runs   (    0.16 ms per token,  6308.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6815.50 ms /   419 tokens (   16.27 ms per token,    61.48 tokens per second)\n",
      "llama_print_timings:        eval time =   43706.08 ms /   255 runs   (  171.40 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   51355.32 ms /   674 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.18 ms /   256 runs   (    0.15 ms per token,  6533.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7157.33 ms /   439 tokens (   16.30 ms per token,    61.34 tokens per second)\n",
      "llama_print_timings:        eval time =   45524.64 ms /   255 runs   (  178.53 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   53546.63 ms /   694 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      26.82 ms /   175 runs   (    0.15 ms per token,  6524.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10168.10 ms /   596 tokens (   17.06 ms per token,    58.61 tokens per second)\n",
      "llama_print_timings:        eval time =   30121.43 ms /   174 runs   (  173.11 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   40861.83 ms /   770 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.90 ms /   256 runs   (    0.16 ms per token,  6416.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8152.80 ms /   512 tokens (   15.92 ms per token,    62.80 tokens per second)\n",
      "llama_print_timings:        eval time =   44970.75 ms /   255 runs   (  176.36 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   53990.50 ms /   767 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.92 ms /   256 runs   (    0.15 ms per token,  6578.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5996.80 ms /   392 tokens (   15.30 ms per token,    65.37 tokens per second)\n",
      "llama_print_timings:        eval time =   45210.68 ms /   255 runs   (  177.30 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   52041.97 ms /   647 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.77 ms /   256 runs   (    0.16 ms per token,  6437.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6266.54 ms /   416 tokens (   15.06 ms per token,    66.38 tokens per second)\n",
      "llama_print_timings:        eval time =   44294.51 ms /   255 runs   (  173.70 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   51420.55 ms /   671 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      17.84 ms /   115 runs   (    0.16 ms per token,  6446.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5580.51 ms /   343 tokens (   16.27 ms per token,    61.46 tokens per second)\n",
      "llama_print_timings:        eval time =   19976.51 ms /   114 runs   (  175.23 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   25931.75 ms /   457 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      23.62 ms /   156 runs   (    0.15 ms per token,  6604.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5616.57 ms /   363 tokens (   15.47 ms per token,    64.63 tokens per second)\n",
      "llama_print_timings:        eval time =   26409.56 ms /   155 runs   (  170.38 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =   32529.58 ms /   518 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      17.86 ms /   118 runs   (    0.15 ms per token,  6606.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7500.32 ms /   517 tokens (   14.51 ms per token,    68.93 tokens per second)\n",
      "llama_print_timings:        eval time =   20125.00 ms /   117 runs   (  172.01 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   28015.11 ms /   634 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.98 ms /   256 runs   (    0.15 ms per token,  6567.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7624.22 ms /   496 tokens (   15.37 ms per token,    65.06 tokens per second)\n",
      "llama_print_timings:        eval time =   45280.58 ms /   255 runs   (  177.57 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   53758.76 ms /   751 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.12 ms /   256 runs   (    0.15 ms per token,  6543.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8031.37 ms /   521 tokens (   15.42 ms per token,    64.87 tokens per second)\n",
      "llama_print_timings:        eval time =   44504.81 ms /   255 runs   (  174.53 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   53390.33 ms /   776 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      13.89 ms /    91 runs   (    0.15 ms per token,  6551.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6859.71 ms /   456 tokens (   15.04 ms per token,    66.48 tokens per second)\n",
      "llama_print_timings:        eval time =   16103.65 ms /    90 runs   (  178.93 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   23269.67 ms /   546 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.50 ms /   256 runs   (    0.15 ms per token,  6481.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7123.54 ms /   477 tokens (   14.93 ms per token,    66.96 tokens per second)\n",
      "llama_print_timings:        eval time =   44088.57 ms /   255 runs   (  172.90 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   52079.74 ms /   732 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.81 ms /   256 runs   (    0.16 ms per token,  6430.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8711.09 ms /   533 tokens (   16.34 ms per token,    61.19 tokens per second)\n",
      "llama_print_timings:        eval time =   44091.96 ms /   255 runs   (  172.91 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   53662.58 ms /   788 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      14.95 ms /    95 runs   (    0.16 ms per token,  6355.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9117.52 ms /   539 tokens (   16.92 ms per token,    59.12 tokens per second)\n",
      "llama_print_timings:        eval time =   17228.14 ms /    94 runs   (  183.28 ms per token,     5.46 tokens per second)\n",
      "llama_print_timings:       total time =   26664.09 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.61 ms /   256 runs   (    0.15 ms per token,  6631.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10277.64 ms /   624 tokens (   16.47 ms per token,    60.71 tokens per second)\n",
      "llama_print_timings:        eval time =   45261.88 ms /   255 runs   (  177.50 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   56391.89 ms /   879 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.69 ms /   256 runs   (    0.15 ms per token,  6617.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9208.56 ms /   589 tokens (   15.63 ms per token,    63.96 tokens per second)\n",
      "llama_print_timings:        eval time =   44825.42 ms /   255 runs   (  175.79 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   54896.91 ms /   844 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.10 ms /   256 runs   (    0.15 ms per token,  6547.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11280.97 ms /   695 tokens (   16.23 ms per token,    61.61 tokens per second)\n",
      "llama_print_timings:        eval time =   45816.08 ms /   255 runs   (  179.67 ms per token,     5.57 tokens per second)\n",
      "llama_print_timings:       total time =   57965.23 ms /   950 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.39 ms /   256 runs   (    0.15 ms per token,  6668.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11183.50 ms /   700 tokens (   15.98 ms per token,    62.59 tokens per second)\n",
      "llama_print_timings:        eval time =   46299.57 ms /   255 runs   (  181.57 ms per token,     5.51 tokens per second)\n",
      "llama_print_timings:       total time =   58327.54 ms /   955 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      21.21 ms /   142 runs   (    0.15 ms per token,  6694.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5677.65 ms /   380 tokens (   14.94 ms per token,    66.93 tokens per second)\n",
      "llama_print_timings:        eval time =   24319.62 ms /   141 runs   (  172.48 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:       total time =   30442.51 ms /   521 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      42.52 ms /   256 runs   (    0.17 ms per token,  6020.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7804.06 ms /   498 tokens (   15.67 ms per token,    63.81 tokens per second)\n",
      "llama_print_timings:        eval time =   44083.92 ms /   255 runs   (  172.88 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   52745.52 ms /   753 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.12 ms /   252 runs   (    0.15 ms per token,  6610.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6991.20 ms /   409 tokens (   17.09 ms per token,    58.50 tokens per second)\n",
      "llama_print_timings:        eval time =   42434.47 ms /   251 runs   (  169.06 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:       total time =   50244.82 ms /   660 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      19.32 ms /   122 runs   (    0.16 ms per token,  6313.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5660.11 ms /   363 tokens (   15.59 ms per token,    64.13 tokens per second)\n",
      "llama_print_timings:        eval time =   20913.71 ms /   121 runs   (  172.84 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   26988.55 ms /   484 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.52 ms /   256 runs   (    0.15 ms per token,  6477.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6065.12 ms /   389 tokens (   15.59 ms per token,    64.14 tokens per second)\n",
      "llama_print_timings:        eval time =   43624.20 ms /   255 runs   (  171.08 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   50529.11 ms /   644 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      22.96 ms /   151 runs   (    0.15 ms per token,  6576.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5499.91 ms /   341 tokens (   16.13 ms per token,    62.00 tokens per second)\n",
      "llama_print_timings:        eval time =   25571.62 ms /   150 runs   (  170.48 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =   31583.89 ms /   491 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.98 ms /   256 runs   (    0.15 ms per token,  6568.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6379.32 ms /   378 tokens (   16.88 ms per token,    59.25 tokens per second)\n",
      "llama_print_timings:        eval time =   43290.71 ms /   255 runs   (  169.77 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =   50527.97 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.80 ms /   256 runs   (    0.16 ms per token,  6432.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7216.18 ms /   465 tokens (   15.52 ms per token,    64.44 tokens per second)\n",
      "llama_print_timings:        eval time =   43654.24 ms /   255 runs   (  171.19 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   51726.39 ms /   720 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.68 ms /   256 runs   (    0.16 ms per token,  6450.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5920.00 ms /   390 tokens (   15.18 ms per token,    65.88 tokens per second)\n",
      "llama_print_timings:        eval time =   43810.07 ms /   255 runs   (  171.80 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   50609.12 ms /   645 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.36 ms /   256 runs   (    0.15 ms per token,  6504.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6489.95 ms /   401 tokens (   16.18 ms per token,    61.79 tokens per second)\n",
      "llama_print_timings:        eval time =   44802.01 ms /   255 runs   (  175.69 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   52156.91 ms /   656 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.76 ms /   256 runs   (    0.15 ms per token,  6604.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5555.69 ms /   365 tokens (   15.22 ms per token,    65.70 tokens per second)\n",
      "llama_print_timings:        eval time =   45501.47 ms /   255 runs   (  178.44 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   51876.20 ms /   620 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      37.30 ms /   244 runs   (    0.15 ms per token,  6541.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6296.70 ms /   381 tokens (   16.53 ms per token,    60.51 tokens per second)\n",
      "llama_print_timings:        eval time =   43883.46 ms /   243 runs   (  180.59 ms per token,     5.54 tokens per second)\n",
      "llama_print_timings:       total time =   50998.15 ms /   624 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      19.02 ms /   124 runs   (    0.15 ms per token,  6520.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6216.94 ms /   410 tokens (   15.16 ms per token,    65.95 tokens per second)\n",
      "llama_print_timings:        eval time =   21992.35 ms /   123 runs   (  178.80 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   28622.41 ms /   533 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.23 ms /   256 runs   (    0.15 ms per token,  6696.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5950.13 ms /   381 tokens (   15.62 ms per token,    64.03 tokens per second)\n",
      "llama_print_timings:        eval time =   45539.87 ms /   255 runs   (  178.59 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   52311.46 ms /   636 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.09 ms /   256 runs   (    0.15 ms per token,  6721.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5883.85 ms /   356 tokens (   16.53 ms per token,    60.50 tokens per second)\n",
      "llama_print_timings:        eval time =   45851.37 ms /   255 runs   (  179.81 ms per token,     5.56 tokens per second)\n",
      "llama_print_timings:       total time =   52546.45 ms /   611 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      20.57 ms /   133 runs   (    0.15 ms per token,  6465.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5611.82 ms /   345 tokens (   16.27 ms per token,    61.48 tokens per second)\n",
      "llama_print_timings:        eval time =   22217.76 ms /   132 runs   (  168.32 ms per token,     5.94 tokens per second)\n",
      "llama_print_timings:       total time =   28276.61 ms /   477 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.68 ms /   256 runs   (    0.15 ms per token,  6618.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6191.14 ms /   396 tokens (   15.63 ms per token,    63.96 tokens per second)\n",
      "llama_print_timings:        eval time =   44520.48 ms /   255 runs   (  174.59 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   51574.45 ms /   651 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.88 ms /   256 runs   (    0.15 ms per token,  6584.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7102.79 ms /   434 tokens (   16.37 ms per token,    61.10 tokens per second)\n",
      "llama_print_timings:        eval time =   44721.01 ms /   255 runs   (  175.38 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   52686.51 ms /   689 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.55 ms /   256 runs   (    0.15 ms per token,  6472.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6028.71 ms /   388 tokens (   15.54 ms per token,    64.36 tokens per second)\n",
      "llama_print_timings:        eval time =   44306.77 ms /   255 runs   (  173.75 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   51200.51 ms /   643 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /    92 runs   (    0.15 ms per token,  6694.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5665.88 ms /   368 tokens (   15.40 ms per token,    64.95 tokens per second)\n",
      "llama_print_timings:        eval time =   16189.67 ms /    91 runs   (  177.91 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   22137.29 ms /   459 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.53 ms /   256 runs   (    0.15 ms per token,  6644.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5885.51 ms /   371 tokens (   15.86 ms per token,    63.04 tokens per second)\n",
      "llama_print_timings:        eval time =   45519.32 ms /   255 runs   (  178.51 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   52239.28 ms /   626 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      36.50 ms /   236 runs   (    0.15 ms per token,  6466.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5350.37 ms /   351 tokens (   15.24 ms per token,    65.60 tokens per second)\n",
      "llama_print_timings:        eval time =   40468.59 ms /   235 runs   (  172.21 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   46613.36 ms /   586 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      29.44 ms /   187 runs   (    0.16 ms per token,  6351.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5739.67 ms /   364 tokens (   15.77 ms per token,    63.42 tokens per second)\n",
      "llama_print_timings:        eval time =   32774.78 ms /   186 runs   (  176.21 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   39144.78 ms /   550 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      22.59 ms /   145 runs   (    0.16 ms per token,  6418.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5166.14 ms /   335 tokens (   15.42 ms per token,    64.85 tokens per second)\n",
      "llama_print_timings:        eval time =   24806.28 ms /   144 runs   (  172.27 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:       total time =   30455.05 ms /   479 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.55 ms /   256 runs   (    0.15 ms per token,  6640.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5427.75 ms /   361 tokens (   15.04 ms per token,    66.51 tokens per second)\n",
      "llama_print_timings:        eval time =   45646.22 ms /   255 runs   (  179.00 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   51896.26 ms /   616 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.39 ms /   256 runs   (    0.15 ms per token,  6668.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5735.82 ms /   371 tokens (   15.46 ms per token,    64.68 tokens per second)\n",
      "llama_print_timings:        eval time =   44732.59 ms /   255 runs   (  175.42 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   51315.85 ms /   626 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.41 ms /   254 runs   (    0.15 ms per token,  6613.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5793.33 ms /   365 tokens (   15.87 ms per token,    63.00 tokens per second)\n",
      "llama_print_timings:        eval time =   44922.28 ms /   253 runs   (  177.56 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   51550.18 ms /   618 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      30.82 ms /   198 runs   (    0.16 ms per token,  6425.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7423.47 ms /   495 tokens (   15.00 ms per token,    66.68 tokens per second)\n",
      "llama_print_timings:        eval time =   35147.53 ms /   197 runs   (  178.41 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   43248.61 ms /   692 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.48 ms /   256 runs   (    0.15 ms per token,  6653.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7287.58 ms /   442 tokens (   16.49 ms per token,    60.65 tokens per second)\n",
      "llama_print_timings:        eval time =   44933.18 ms /   255 runs   (  176.21 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   53054.01 ms /   697 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.18 ms /   256 runs   (    0.15 ms per token,  6533.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6634.66 ms /   437 tokens (   15.18 ms per token,    65.87 tokens per second)\n",
      "llama_print_timings:        eval time =   43950.03 ms /   255 runs   (  172.35 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:       total time =   51463.53 ms /   692 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.61 ms /   256 runs   (    0.15 ms per token,  6463.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5673.93 ms /   373 tokens (   15.21 ms per token,    65.74 tokens per second)\n",
      "llama_print_timings:        eval time =   44192.51 ms /   255 runs   (  173.30 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   50750.98 ms /   628 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.78 ms /   256 runs   (    0.15 ms per token,  6600.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7656.01 ms /   494 tokens (   15.50 ms per token,    64.52 tokens per second)\n",
      "llama_print_timings:        eval time =   43310.97 ms /   255 runs   (  169.85 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =   51835.56 ms /   749 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.40 ms /   256 runs   (    0.15 ms per token,  6496.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6756.42 ms /   448 tokens (   15.08 ms per token,    66.31 tokens per second)\n",
      "llama_print_timings:        eval time =   43597.69 ms /   255 runs   (  170.97 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   51229.38 ms /   703 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.32 ms /   256 runs   (    0.15 ms per token,  6511.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5703.54 ms /   377 tokens (   15.13 ms per token,    66.10 tokens per second)\n",
      "llama_print_timings:        eval time =   43076.18 ms /   255 runs   (  168.93 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:       total time =   49654.04 ms /   632 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.51 ms /   250 runs   (    0.15 ms per token,  6492.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6555.74 ms /   428 tokens (   15.32 ms per token,    65.29 tokens per second)\n",
      "llama_print_timings:        eval time =   43802.22 ms /   249 runs   (  175.91 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   51221.88 ms /   677 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.97 ms /   256 runs   (    0.16 ms per token,  6405.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5752.26 ms /   351 tokens (   16.39 ms per token,    61.02 tokens per second)\n",
      "llama_print_timings:        eval time =   44070.68 ms /   255 runs   (  172.83 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   50709.57 ms /   606 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.01 ms /   256 runs   (    0.15 ms per token,  6561.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7490.90 ms /   488 tokens (   15.35 ms per token,    65.15 tokens per second)\n",
      "llama_print_timings:        eval time =   43897.77 ms /   255 runs   (  172.15 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   52250.83 ms /   743 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      33.75 ms /   220 runs   (    0.15 ms per token,  6518.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6385.85 ms /   401 tokens (   15.92 ms per token,    62.80 tokens per second)\n",
      "llama_print_timings:        eval time =   38064.49 ms /   219 runs   (  173.81 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   45187.52 ms /   620 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.46 ms /   256 runs   (    0.15 ms per token,  6487.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2193.16 ms /   108 tokens (   20.31 ms per token,    49.24 tokens per second)\n",
      "llama_print_timings:        eval time =   43134.58 ms /   255 runs   (  169.16 ms per token,     5.91 tokens per second)\n",
      "llama_print_timings:       total time =   46185.11 ms /   363 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      18.22 ms /   119 runs   (    0.15 ms per token,  6529.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5542.05 ms /   368 tokens (   15.06 ms per token,    66.40 tokens per second)\n",
      "llama_print_timings:        eval time =   20904.83 ms /   118 runs   (  177.16 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   26827.37 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.48 ms /   256 runs   (    0.15 ms per token,  6483.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5529.01 ms /   353 tokens (   15.66 ms per token,    63.85 tokens per second)\n",
      "llama_print_timings:        eval time =   45405.49 ms /   255 runs   (  178.06 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   51802.47 ms /   608 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.51 ms /   256 runs   (    0.15 ms per token,  6647.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6240.66 ms /   384 tokens (   16.25 ms per token,    61.53 tokens per second)\n",
      "llama_print_timings:        eval time =   43960.34 ms /   255 runs   (  172.39 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:       total time =   51032.95 ms /   639 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.51 ms /   256 runs   (    0.15 ms per token,  6479.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6381.72 ms /   369 tokens (   17.29 ms per token,    57.82 tokens per second)\n",
      "llama_print_timings:        eval time =   43878.22 ms /   255 runs   (  172.07 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   51107.03 ms /   624 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      19.07 ms /   123 runs   (    0.16 ms per token,  6448.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5367.89 ms /   350 tokens (   15.34 ms per token,    65.20 tokens per second)\n",
      "llama_print_timings:        eval time =   20973.06 ms /   122 runs   (  171.91 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   26743.73 ms /   472 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      37.40 ms /   241 runs   (    0.16 ms per token,  6443.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5118.61 ms /   336 tokens (   15.23 ms per token,    65.64 tokens per second)\n",
      "llama_print_timings:        eval time =   41612.61 ms /   240 runs   (  173.39 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   47536.27 ms /   576 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.10 ms /   256 runs   (    0.15 ms per token,  6547.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7790.99 ms /   494 tokens (   15.77 ms per token,    63.41 tokens per second)\n",
      "llama_print_timings:        eval time =   45335.92 ms /   255 runs   (  177.79 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   54004.23 ms /   749 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      23.10 ms /   150 runs   (    0.15 ms per token,  6494.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6474.93 ms /   410 tokens (   15.79 ms per token,    63.32 tokens per second)\n",
      "llama_print_timings:        eval time =   25394.93 ms /   149 runs   (  170.44 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =   32372.43 ms /   559 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      37.97 ms /   250 runs   (    0.15 ms per token,  6584.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5380.93 ms /   354 tokens (   15.20 ms per token,    65.79 tokens per second)\n",
      "llama_print_timings:        eval time =   42362.98 ms /   249 runs   (  170.13 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   48606.85 ms /   603 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.13 ms /   256 runs   (    0.15 ms per token,  6542.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6219.61 ms /   403 tokens (   15.43 ms per token,    64.80 tokens per second)\n",
      "llama_print_timings:        eval time =   45419.52 ms /   255 runs   (  178.12 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   52496.23 ms /   658 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      33.04 ms /   214 runs   (    0.15 ms per token,  6476.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6915.57 ms /   414 tokens (   16.70 ms per token,    59.86 tokens per second)\n",
      "llama_print_timings:        eval time =   36857.73 ms /   213 runs   (  173.04 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   44506.47 ms /   627 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =       1.49 ms /    10 runs   (    0.15 ms per token,  6715.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6898.96 ms /   407 tokens (   16.95 ms per token,    58.99 tokens per second)\n",
      "llama_print_timings:        eval time =    1707.23 ms /     9 runs   (  189.69 ms per token,     5.27 tokens per second)\n",
      "llama_print_timings:       total time =    8636.73 ms /   416 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.06 ms /   255 runs   (    0.15 ms per token,  6529.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6129.72 ms /   393 tokens (   15.60 ms per token,    64.11 tokens per second)\n",
      "llama_print_timings:        eval time =   44041.60 ms /   254 runs   (  173.39 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   51023.51 ms /   647 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =       9.45 ms /    62 runs   (    0.15 ms per token,  6561.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6423.64 ms /   417 tokens (   15.40 ms per token,    64.92 tokens per second)\n",
      "llama_print_timings:        eval time =   10927.94 ms /    61 runs   (  179.15 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   17564.58 ms /   478 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      25.28 ms /   167 runs   (    0.15 ms per token,  6605.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7171.94 ms /   459 tokens (   15.63 ms per token,    64.00 tokens per second)\n",
      "llama_print_timings:        eval time =   28906.58 ms /   166 runs   (  174.14 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   36631.44 ms /   625 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      30.73 ms /   197 runs   (    0.16 ms per token,  6409.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6907.29 ms /   447 tokens (   15.45 ms per token,    64.71 tokens per second)\n",
      "llama_print_timings:        eval time =   34325.64 ms /   196 runs   (  175.13 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   41901.12 ms /   643 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.13 ms /   256 runs   (    0.16 ms per token,  6379.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7711.35 ms /   483 tokens (   15.97 ms per token,    62.63 tokens per second)\n",
      "llama_print_timings:        eval time =   44964.56 ms /   255 runs   (  176.33 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   53559.82 ms /   738 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      28.36 ms /   184 runs   (    0.15 ms per token,  6488.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4372.96 ms /   265 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_print_timings:        eval time =   30678.04 ms /   183 runs   (  167.64 ms per token,     5.97 tokens per second)\n",
      "llama_print_timings:       total time =   35684.73 ms /   448 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      28.27 ms /   186 runs   (    0.15 ms per token,  6580.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6976.84 ms /   442 tokens (   15.78 ms per token,    63.35 tokens per second)\n",
      "llama_print_timings:        eval time =   32723.63 ms /   185 runs   (  176.88 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   40329.44 ms /   627 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.52 ms /   256 runs   (    0.15 ms per token,  6645.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6981.57 ms /   457 tokens (   15.28 ms per token,    65.46 tokens per second)\n",
      "llama_print_timings:        eval time =   44928.40 ms /   255 runs   (  176.19 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   52770.33 ms /   712 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.98 ms /   256 runs   (    0.15 ms per token,  6567.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6704.64 ms /   436 tokens (   15.38 ms per token,    65.03 tokens per second)\n",
      "llama_print_timings:        eval time =   45746.49 ms /   255 runs   (  179.40 ms per token,     5.57 tokens per second)\n",
      "llama_print_timings:       total time =   53306.60 ms /   691 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      35.26 ms /   236 runs   (    0.15 ms per token,  6692.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5283.68 ms /   351 tokens (   15.05 ms per token,    66.43 tokens per second)\n",
      "llama_print_timings:        eval time =   41179.30 ms /   235 runs   (  175.23 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   47239.46 ms /   586 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.18 ms /   256 runs   (    0.15 ms per token,  6534.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6913.60 ms /   420 tokens (   16.46 ms per token,    60.75 tokens per second)\n",
      "llama_print_timings:        eval time =   48031.87 ms /   255 runs   (  188.36 ms per token,     5.31 tokens per second)\n",
      "llama_print_timings:       total time =   55807.01 ms /   675 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.74 ms /   256 runs   (    0.16 ms per token,  6441.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7157.38 ms /   461 tokens (   15.53 ms per token,    64.41 tokens per second)\n",
      "llama_print_timings:        eval time =   44812.42 ms /   255 runs   (  175.73 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   52835.49 ms /   716 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      31.67 ms /   208 runs   (    0.15 ms per token,  6567.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1470.86 ms /    62 tokens (   23.72 ms per token,    42.15 tokens per second)\n",
      "llama_print_timings:        eval time =   35434.54 ms /   207 runs   (  171.18 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:       total time =   37571.60 ms /   269 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.94 ms /   256 runs   (    0.16 ms per token,  6408.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9296.15 ms /   543 tokens (   17.12 ms per token,    58.41 tokens per second)\n",
      "llama_print_timings:        eval time =   45447.85 ms /   255 runs   (  178.23 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   55630.50 ms /   798 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.66 ms /   256 runs   (    0.15 ms per token,  6455.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8785.03 ms /   545 tokens (   16.12 ms per token,    62.04 tokens per second)\n",
      "llama_print_timings:        eval time =   42665.42 ms /   255 runs   (  167.32 ms per token,     5.98 tokens per second)\n",
      "llama_print_timings:       total time =   52349.48 ms /   800 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.42 ms /   256 runs   (    0.15 ms per token,  6663.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10484.55 ms /   600 tokens (   17.47 ms per token,    57.23 tokens per second)\n",
      "llama_print_timings:        eval time =   44595.34 ms /   255 runs   (  174.88 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   55927.98 ms /   855 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.12 ms /   256 runs   (    0.15 ms per token,  6544.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7231.94 ms /   460 tokens (   15.72 ms per token,    63.61 tokens per second)\n",
      "llama_print_timings:        eval time =   44537.72 ms /   255 runs   (  174.66 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   52631.51 ms /   715 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.40 ms /   256 runs   (    0.15 ms per token,  6497.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9706.54 ms /   577 tokens (   16.82 ms per token,    59.44 tokens per second)\n",
      "llama_print_timings:        eval time =   45621.95 ms /   255 runs   (  178.91 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   56190.18 ms /   832 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.68 ms /   256 runs   (    0.16 ms per token,  6451.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10052.50 ms /   641 tokens (   15.68 ms per token,    63.77 tokens per second)\n",
      "llama_print_timings:        eval time =   43729.15 ms /   255 runs   (  171.49 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   54678.08 ms /   896 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.35 ms /   256 runs   (    0.15 ms per token,  6506.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8023.56 ms /   522 tokens (   15.37 ms per token,    65.06 tokens per second)\n",
      "llama_print_timings:        eval time =   43731.33 ms /   255 runs   (  171.50 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   52630.25 ms /   777 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      29.68 ms /   192 runs   (    0.15 ms per token,  6469.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9258.03 ms /   581 tokens (   15.93 ms per token,    62.76 tokens per second)\n",
      "llama_print_timings:        eval time =   33646.49 ms /   191 runs   (  176.16 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   43555.73 ms /   772 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.02 ms /   256 runs   (    0.15 ms per token,  6560.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9083.00 ms /   576 tokens (   15.77 ms per token,    63.42 tokens per second)\n",
      "llama_print_timings:        eval time =   44305.37 ms /   255 runs   (  173.75 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   54265.58 ms /   831 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      29.88 ms /   195 runs   (    0.15 ms per token,  6526.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9746.20 ms /   591 tokens (   16.49 ms per token,    60.64 tokens per second)\n",
      "llama_print_timings:        eval time =   34306.64 ms /   194 runs   (  176.84 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   44716.91 ms /   785 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.36 ms /   256 runs   (    0.15 ms per token,  6503.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9228.72 ms /   576 tokens (   16.02 ms per token,    62.41 tokens per second)\n",
      "llama_print_timings:        eval time =   45310.14 ms /   255 runs   (  177.69 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   55425.95 ms /   831 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.15 ms /   256 runs   (    0.15 ms per token,  6539.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10357.83 ms /   646 tokens (   16.03 ms per token,    62.37 tokens per second)\n",
      "llama_print_timings:        eval time =   44251.96 ms /   255 runs   (  173.54 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   55493.00 ms /   901 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.31 ms /   256 runs   (    0.16 ms per token,  6351.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9552.19 ms /   605 tokens (   15.79 ms per token,    63.34 tokens per second)\n",
      "llama_print_timings:        eval time =   43333.20 ms /   255 runs   (  169.93 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   53780.66 ms /   860 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.94 ms /   256 runs   (    0.16 ms per token,  6409.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9021.67 ms /   562 tokens (   16.05 ms per token,    62.29 tokens per second)\n",
      "llama_print_timings:        eval time =   43616.86 ms /   255 runs   (  171.05 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   53527.45 ms /   817 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.29 ms /   256 runs   (    0.15 ms per token,  6516.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9654.90 ms /   618 tokens (   15.62 ms per token,    64.01 tokens per second)\n",
      "llama_print_timings:        eval time =   45016.73 ms /   255 runs   (  176.54 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   55541.55 ms /   873 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.22 ms /   256 runs   (    0.15 ms per token,  6527.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8238.01 ms /   517 tokens (   15.93 ms per token,    62.76 tokens per second)\n",
      "llama_print_timings:        eval time =   42357.72 ms /   255 runs   (  166.11 ms per token,     6.02 tokens per second)\n",
      "llama_print_timings:       total time =   51477.17 ms /   772 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.42 ms /   256 runs   (    0.16 ms per token,  6333.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9017.78 ms /   543 tokens (   16.61 ms per token,    60.21 tokens per second)\n",
      "llama_print_timings:        eval time =   44854.21 ms /   255 runs   (  175.90 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   54758.69 ms /   798 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.26 ms /   256 runs   (    0.15 ms per token,  6519.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9130.51 ms /   573 tokens (   15.93 ms per token,    62.76 tokens per second)\n",
      "llama_print_timings:        eval time =   44813.30 ms /   255 runs   (  175.74 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   54809.58 ms /   828 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.64 ms /   256 runs   (    0.15 ms per token,  6458.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9774.89 ms /   606 tokens (   16.13 ms per token,    62.00 tokens per second)\n",
      "llama_print_timings:        eval time =   43617.61 ms /   255 runs   (  171.05 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   54262.01 ms /   861 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.93 ms /   163 runs   (    0.15 ms per token,  6537.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7335.10 ms /   466 tokens (   15.74 ms per token,    63.53 tokens per second)\n",
      "llama_print_timings:        eval time =   27852.46 ms /   162 runs   (  171.93 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   35720.23 ms /   628 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.94 ms /   256 runs   (    0.16 ms per token,  6410.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7212.92 ms /   477 tokens (   15.12 ms per token,    66.13 tokens per second)\n",
      "llama_print_timings:        eval time =   43801.72 ms /   255 runs   (  171.77 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   51869.58 ms /   732 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.73 ms /   159 runs   (    0.16 ms per token,  6429.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9261.44 ms /   571 tokens (   16.22 ms per token,    61.65 tokens per second)\n",
      "llama_print_timings:        eval time =   27951.05 ms /   158 runs   (  176.91 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   37734.99 ms /   729 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      29.77 ms /   193 runs   (    0.15 ms per token,  6483.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7904.04 ms /   512 tokens (   15.44 ms per token,    64.78 tokens per second)\n",
      "llama_print_timings:        eval time =   34283.94 ms /   193 runs   (  177.64 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   42831.53 ms /   705 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.76 ms /   256 runs   (    0.16 ms per token,  6439.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6987.21 ms /   431 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =   44508.12 ms /   255 runs   (  174.54 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   52341.39 ms /   686 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.22 ms /   256 runs   (    0.15 ms per token,  6527.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9275.77 ms /   586 tokens (   15.83 ms per token,    63.18 tokens per second)\n",
      "llama_print_timings:        eval time =   44012.94 ms /   255 runs   (  172.60 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   54155.94 ms /   841 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.07 ms /   256 runs   (    0.16 ms per token,  6389.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8840.55 ms /   539 tokens (   16.40 ms per token,    60.97 tokens per second)\n",
      "llama_print_timings:        eval time =   43901.02 ms /   255 runs   (  172.16 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   53620.60 ms /   794 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.74 ms /   256 runs   (    0.15 ms per token,  6608.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8840.47 ms /   562 tokens (   15.73 ms per token,    63.57 tokens per second)\n",
      "llama_print_timings:        eval time =   44445.62 ms /   255 runs   (  174.30 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   54138.08 ms /   817 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.09 ms /   256 runs   (    0.15 ms per token,  6549.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7552.95 ms /   500 tokens (   15.11 ms per token,    66.20 tokens per second)\n",
      "llama_print_timings:        eval time =   45246.96 ms /   255 runs   (  177.44 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   53665.79 ms /   755 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      37.91 ms /   250 runs   (    0.15 ms per token,  6594.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9381.28 ms /   579 tokens (   16.20 ms per token,    61.72 tokens per second)\n",
      "llama_print_timings:        eval time =   43095.20 ms /   249 runs   (  173.07 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   53299.55 ms /   828 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.22 ms /   256 runs   (    0.15 ms per token,  6527.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7313.91 ms /   483 tokens (   15.14 ms per token,    66.04 tokens per second)\n",
      "llama_print_timings:        eval time =   44008.63 ms /   255 runs   (  172.58 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   52178.85 ms /   738 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.78 ms /   256 runs   (    0.15 ms per token,  6601.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8134.84 ms /   528 tokens (   15.41 ms per token,    64.91 tokens per second)\n",
      "llama_print_timings:        eval time =   44687.93 ms /   255 runs   (  175.25 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   53683.41 ms /   783 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.43 ms /   256 runs   (    0.15 ms per token,  6661.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8112.22 ms /   530 tokens (   15.31 ms per token,    65.33 tokens per second)\n",
      "llama_print_timings:        eval time =   44592.02 ms /   255 runs   (  174.87 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   53529.10 ms /   785 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.88 ms /   256 runs   (    0.15 ms per token,  6583.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7835.80 ms /   511 tokens (   15.33 ms per token,    65.21 tokens per second)\n",
      "llama_print_timings:        eval time =   44731.38 ms /   255 runs   (  175.42 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   53413.05 ms /   766 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      25.92 ms /   170 runs   (    0.15 ms per token,  6558.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7950.10 ms /   479 tokens (   16.60 ms per token,    60.25 tokens per second)\n",
      "llama_print_timings:        eval time =   28762.72 ms /   169 runs   (  170.19 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   37267.80 ms /   648 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.23 ms /   256 runs   (    0.15 ms per token,  6524.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7999.36 ms /   478 tokens (   16.74 ms per token,    59.75 tokens per second)\n",
      "llama_print_timings:        eval time =   45053.74 ms /   255 runs   (  176.68 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   53902.20 ms /   733 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      18.04 ms /   117 runs   (    0.15 ms per token,  6485.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8019.14 ms /   497 tokens (   16.14 ms per token,    61.98 tokens per second)\n",
      "llama_print_timings:        eval time =   19900.33 ms /   116 runs   (  171.55 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   28312.74 ms /   613 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.66 ms /   256 runs   (    0.15 ms per token,  6455.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7889.46 ms /   504 tokens (   15.65 ms per token,    63.88 tokens per second)\n",
      "llama_print_timings:        eval time =   46450.51 ms /   255 runs   (  182.16 ms per token,     5.49 tokens per second)\n",
      "llama_print_timings:       total time =   55216.76 ms /   759 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      18.93 ms /   126 runs   (    0.15 ms per token,  6657.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8281.33 ms /   521 tokens (   15.90 ms per token,    62.91 tokens per second)\n",
      "llama_print_timings:        eval time =   22719.04 ms /   125 runs   (  181.75 ms per token,     5.50 tokens per second)\n",
      "llama_print_timings:       total time =   31407.08 ms /   646 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.01 ms /   256 runs   (    0.15 ms per token,  6562.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8162.15 ms /   501 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =   44493.36 ms /   255 runs   (  174.48 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   53544.81 ms /   756 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.91 ms /   256 runs   (    0.16 ms per token,  6414.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9025.27 ms /   573 tokens (   15.75 ms per token,    63.49 tokens per second)\n",
      "llama_print_timings:        eval time =   44854.50 ms /   255 runs   (  175.90 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   54763.79 ms /   828 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.62 ms /   157 runs   (    0.16 ms per token,  6376.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6819.53 ms /   449 tokens (   15.19 ms per token,    65.84 tokens per second)\n",
      "llama_print_timings:        eval time =   26158.62 ms /   156 runs   (  167.68 ms per token,     5.96 tokens per second)\n",
      "llama_print_timings:       total time =   33504.81 ms /   605 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.71 ms /   256 runs   (    0.15 ms per token,  6612.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6712.68 ms /   447 tokens (   15.02 ms per token,    66.59 tokens per second)\n",
      "llama_print_timings:        eval time =   44175.94 ms /   255 runs   (  173.24 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   51753.32 ms /   702 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.36 ms /   256 runs   (    0.15 ms per token,  6503.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7008.29 ms /   465 tokens (   15.07 ms per token,    66.35 tokens per second)\n",
      "llama_print_timings:        eval time =   43762.66 ms /   255 runs   (  171.62 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   51637.94 ms /   720 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      25.87 ms /   169 runs   (    0.15 ms per token,  6533.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7039.05 ms /   445 tokens (   15.82 ms per token,    63.22 tokens per second)\n",
      "llama_print_timings:        eval time =   28685.09 ms /   168 runs   (  170.74 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   36291.60 ms /   613 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      27.24 ms /   181 runs   (    0.15 ms per token,  6645.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2201.95 ms /   102 tokens (   21.59 ms per token,    46.32 tokens per second)\n",
      "llama_print_timings:        eval time =   28463.51 ms /   180 runs   (  158.13 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:       total time =   31247.25 ms /   282 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.81 ms /   256 runs   (    0.15 ms per token,  6595.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7545.02 ms /   495 tokens (   15.24 ms per token,    65.61 tokens per second)\n",
      "llama_print_timings:        eval time =   43488.82 ms /   255 runs   (  170.54 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   51904.97 ms /   750 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      29.30 ms /   187 runs   (    0.16 ms per token,  6383.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5286.47 ms /   351 tokens (   15.06 ms per token,    66.40 tokens per second)\n",
      "llama_print_timings:        eval time =   31607.75 ms /   186 runs   (  169.93 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   37533.44 ms /   537 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      32.56 ms /   216 runs   (    0.15 ms per token,  6633.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5579.46 ms /   361 tokens (   15.46 ms per token,    64.70 tokens per second)\n",
      "llama_print_timings:        eval time =   37209.70 ms /   215 runs   (  173.07 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   43509.86 ms /   576 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.05 ms /   256 runs   (    0.15 ms per token,  6555.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6229.03 ms /   409 tokens (   15.23 ms per token,    65.66 tokens per second)\n",
      "llama_print_timings:        eval time =   43152.80 ms /   255 runs   (  169.23 ms per token,     5.91 tokens per second)\n",
      "llama_print_timings:       total time =   50259.88 ms /   664 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.98 ms /   256 runs   (    0.16 ms per token,  6403.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5905.84 ms /   354 tokens (   16.68 ms per token,    59.94 tokens per second)\n",
      "llama_print_timings:        eval time =   45424.38 ms /   255 runs   (  178.13 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   52189.74 ms /   609 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.64 ms /   157 runs   (    0.16 ms per token,  6372.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5776.77 ms /   370 tokens (   15.61 ms per token,    64.05 tokens per second)\n",
      "llama_print_timings:        eval time =   26648.09 ms /   156 runs   (  170.82 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   32968.46 ms /   526 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.40 ms /   256 runs   (    0.15 ms per token,  6496.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6018.73 ms /   352 tokens (   17.10 ms per token,    58.48 tokens per second)\n",
      "llama_print_timings:        eval time =   41046.66 ms /   255 runs   (  160.97 ms per token,     6.21 tokens per second)\n",
      "llama_print_timings:       total time =   47939.31 ms /   607 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.78 ms /   256 runs   (    0.16 ms per token,  6435.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9015.69 ms /   554 tokens (   16.27 ms per token,    61.45 tokens per second)\n",
      "llama_print_timings:        eval time =   44566.21 ms /   255 runs   (  174.77 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   54466.98 ms /   809 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      31.77 ms /   202 runs   (    0.16 ms per token,  6359.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7172.62 ms /   455 tokens (   15.76 ms per token,    63.44 tokens per second)\n",
      "llama_print_timings:        eval time =   35776.28 ms /   201 runs   (  177.99 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   43642.58 ms /   656 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.60 ms /   256 runs   (    0.15 ms per token,  6464.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6210.79 ms /   410 tokens (   15.15 ms per token,    66.01 tokens per second)\n",
      "llama_print_timings:        eval time =   45142.81 ms /   255 runs   (  177.03 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   52223.84 ms /   665 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.41 ms /   256 runs   (    0.15 ms per token,  6496.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8333.94 ms /   540 tokens (   15.43 ms per token,    64.80 tokens per second)\n",
      "llama_print_timings:        eval time =   45585.01 ms /   255 runs   (  178.76 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   54803.83 ms /   795 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      19.16 ms /   124 runs   (    0.15 ms per token,  6473.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9182.72 ms /   572 tokens (   16.05 ms per token,    62.29 tokens per second)\n",
      "llama_print_timings:        eval time =   22376.15 ms /   123 runs   (  181.92 ms per token,     5.50 tokens per second)\n",
      "llama_print_timings:       total time =   31978.08 ms /   695 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.85 ms /   256 runs   (    0.16 ms per token,  6424.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8699.43 ms /   560 tokens (   15.53 ms per token,    64.37 tokens per second)\n",
      "llama_print_timings:        eval time =   46124.58 ms /   255 runs   (  180.88 ms per token,     5.53 tokens per second)\n",
      "llama_print_timings:       total time =   55722.93 ms /   815 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.67 ms /   256 runs   (    0.15 ms per token,  6619.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8704.06 ms /   551 tokens (   15.80 ms per token,    63.30 tokens per second)\n",
      "llama_print_timings:        eval time =   44752.92 ms /   255 runs   (  175.50 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   54321.27 ms /   806 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.01 ms /   256 runs   (    0.15 ms per token,  6562.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9928.30 ms /   608 tokens (   16.33 ms per token,    61.24 tokens per second)\n",
      "llama_print_timings:        eval time =   44479.27 ms /   255 runs   (  174.43 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:       total time =   55274.48 ms /   863 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.47 ms /   256 runs   (    0.15 ms per token,  6653.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3060.89 ms /   167 tokens (   18.33 ms per token,    54.56 tokens per second)\n",
      "llama_print_timings:        eval time =   41856.10 ms /   255 runs   (  164.14 ms per token,     6.09 tokens per second)\n",
      "llama_print_timings:       total time =   45768.92 ms /   422 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.49 ms /   256 runs   (    0.15 ms per token,  6650.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7479.01 ms /   501 tokens (   14.93 ms per token,    66.99 tokens per second)\n",
      "llama_print_timings:        eval time =   45654.33 ms /   255 runs   (  179.04 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   53966.51 ms /   756 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.92 ms /   256 runs   (    0.15 ms per token,  6577.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6966.77 ms /   448 tokens (   15.55 ms per token,    64.31 tokens per second)\n",
      "llama_print_timings:        eval time =   45053.11 ms /   255 runs   (  176.68 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   52859.50 ms /   703 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      15.17 ms /    99 runs   (    0.15 ms per token,  6524.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8142.59 ms /   531 tokens (   15.33 ms per token,    65.21 tokens per second)\n",
      "llama_print_timings:        eval time =   16852.41 ms /    98 runs   (  171.96 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   25308.95 ms /   629 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      28.30 ms /   187 runs   (    0.15 ms per token,  6607.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6525.81 ms /   397 tokens (   16.44 ms per token,    60.84 tokens per second)\n",
      "llama_print_timings:        eval time =   32719.39 ms /   186 runs   (  175.91 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   39848.36 ms /   583 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      33.37 ms /   219 runs   (    0.15 ms per token,  6563.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7431.33 ms /   489 tokens (   15.20 ms per token,    65.80 tokens per second)\n",
      "llama_print_timings:        eval time =   38225.68 ms /   218 runs   (  175.35 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   46383.51 ms /   707 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.02 ms /   256 runs   (    0.15 ms per token,  6560.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8735.77 ms /   555 tokens (   15.74 ms per token,    63.53 tokens per second)\n",
      "llama_print_timings:        eval time =   44365.50 ms /   255 runs   (  173.98 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   53983.58 ms /   810 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.68 ms /   256 runs   (    0.16 ms per token,  6451.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8960.02 ms /   574 tokens (   15.61 ms per token,    64.06 tokens per second)\n",
      "llama_print_timings:        eval time =   44302.88 ms /   255 runs   (  173.74 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   54157.57 ms /   829 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      31.97 ms /   210 runs   (    0.15 ms per token,  6569.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9455.93 ms /   581 tokens (   16.28 ms per token,    61.44 tokens per second)\n",
      "llama_print_timings:        eval time =   36238.49 ms /   209 runs   (  173.39 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   46426.23 ms /   790 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.49 ms /   256 runs   (    0.15 ms per token,  6651.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8508.05 ms /   535 tokens (   15.90 ms per token,    62.88 tokens per second)\n",
      "llama_print_timings:        eval time =   45716.50 ms /   255 runs   (  179.28 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   55101.10 ms /   790 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.77 ms /   256 runs   (    0.15 ms per token,  6603.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8633.00 ms /   537 tokens (   16.08 ms per token,    62.20 tokens per second)\n",
      "llama_print_timings:        eval time =   45382.78 ms /   255 runs   (  177.97 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   54871.49 ms /   792 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.56 ms /   256 runs   (    0.15 ms per token,  6471.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9265.46 ms /   580 tokens (   15.97 ms per token,    62.60 tokens per second)\n",
      "llama_print_timings:        eval time =   45949.99 ms /   255 runs   (  180.20 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:       total time =   56077.97 ms /   835 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.63 ms /   256 runs   (    0.15 ms per token,  6460.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7707.97 ms /   480 tokens (   16.06 ms per token,    62.27 tokens per second)\n",
      "llama_print_timings:        eval time =   45118.26 ms /   255 runs   (  176.93 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   53712.23 ms /   735 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.55 ms /   256 runs   (    0.15 ms per token,  6473.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8890.62 ms /   564 tokens (   15.76 ms per token,    63.44 tokens per second)\n",
      "llama_print_timings:        eval time =   45686.86 ms /   255 runs   (  179.16 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   55449.47 ms /   819 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.80 ms /   256 runs   (    0.16 ms per token,  6432.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7263.84 ms /   489 tokens (   14.85 ms per token,    67.32 tokens per second)\n",
      "llama_print_timings:        eval time =   44257.13 ms /   255 runs   (  173.56 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   52396.97 ms /   744 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.50 ms /   256 runs   (    0.15 ms per token,  6481.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7068.53 ms /   456 tokens (   15.50 ms per token,    64.51 tokens per second)\n",
      "llama_print_timings:        eval time =   44919.08 ms /   255 runs   (  176.15 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   52849.51 ms /   711 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.84 ms /   256 runs   (    0.15 ms per token,  6590.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7817.97 ms /   481 tokens (   16.25 ms per token,    61.52 tokens per second)\n",
      "llama_print_timings:        eval time =   46101.63 ms /   255 runs   (  180.79 ms per token,     5.53 tokens per second)\n",
      "llama_print_timings:       total time =   54767.63 ms /   736 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.60 ms /   256 runs   (    0.15 ms per token,  6631.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8141.03 ms /   522 tokens (   15.60 ms per token,    64.12 tokens per second)\n",
      "llama_print_timings:        eval time =   46035.09 ms /   255 runs   (  180.53 ms per token,     5.54 tokens per second)\n",
      "llama_print_timings:       total time =   55032.35 ms /   777 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.61 ms /   256 runs   (    0.15 ms per token,  6463.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9892.31 ms /   617 tokens (   16.03 ms per token,    62.37 tokens per second)\n",
      "llama_print_timings:        eval time =   42638.53 ms /   255 runs   (  167.21 ms per token,     5.98 tokens per second)\n",
      "llama_print_timings:       total time =   53430.56 ms /   872 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.09 ms /   256 runs   (    0.16 ms per token,  6385.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8996.90 ms /   569 tokens (   15.81 ms per token,    63.24 tokens per second)\n",
      "llama_print_timings:        eval time =   45681.40 ms /   255 runs   (  179.14 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   55579.87 ms /   824 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.49 ms /   256 runs   (    0.15 ms per token,  6482.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7441.35 ms /   502 tokens (   14.82 ms per token,    67.46 tokens per second)\n",
      "llama_print_timings:        eval time =   44332.38 ms /   255 runs   (  173.85 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   52682.76 ms /   757 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.38 ms /   256 runs   (    0.15 ms per token,  6500.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7898.39 ms /   504 tokens (   15.67 ms per token,    63.81 tokens per second)\n",
      "llama_print_timings:        eval time =   44451.52 ms /   255 runs   (  174.32 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   53224.36 ms /   759 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      30.16 ms /   186 runs   (    0.16 ms per token,  6168.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7396.03 ms /   475 tokens (   15.57 ms per token,    64.22 tokens per second)\n",
      "llama_print_timings:        eval time =   30592.25 ms /   185 runs   (  165.36 ms per token,     6.05 tokens per second)\n",
      "llama_print_timings:       total time =   38658.36 ms /   660 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.80 ms /   256 runs   (    0.16 ms per token,  6431.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8172.19 ms /   533 tokens (   15.33 ms per token,    65.22 tokens per second)\n",
      "llama_print_timings:        eval time =   44545.89 ms /   255 runs   (  174.69 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   53611.92 ms /   788 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      34.77 ms /   224 runs   (    0.16 ms per token,  6441.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7453.78 ms /   473 tokens (   15.76 ms per token,    63.46 tokens per second)\n",
      "llama_print_timings:        eval time =   39423.43 ms /   223 runs   (  176.79 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   47661.92 ms /   696 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      28.84 ms /   185 runs   (    0.16 ms per token,  6415.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7359.79 ms /   470 tokens (   15.66 ms per token,    63.86 tokens per second)\n",
      "llama_print_timings:        eval time =   31714.65 ms /   184 runs   (  172.36 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:       total time =   39708.56 ms /   654 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.13 ms /   256 runs   (    0.15 ms per token,  6541.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8055.90 ms /   508 tokens (   15.86 ms per token,    63.06 tokens per second)\n",
      "llama_print_timings:        eval time =   45049.92 ms /   255 runs   (  176.67 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   53976.33 ms /   763 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      30.75 ms /   201 runs   (    0.15 ms per token,  6535.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7708.52 ms /   465 tokens (   16.58 ms per token,    60.32 tokens per second)\n",
      "llama_print_timings:        eval time =   36047.53 ms /   200 runs   (  180.24 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:       total time =   44419.52 ms /   665 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.08 ms /   256 runs   (    0.15 ms per token,  6549.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10415.80 ms /   639 tokens (   16.30 ms per token,    61.35 tokens per second)\n",
      "llama_print_timings:        eval time =   42448.05 ms /   255 runs   (  166.46 ms per token,     6.01 tokens per second)\n",
      "llama_print_timings:       total time =   53750.02 ms /   894 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      34.78 ms /   226 runs   (    0.15 ms per token,  6498.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7118.24 ms /   430 tokens (   16.55 ms per token,    60.41 tokens per second)\n",
      "llama_print_timings:        eval time =   38990.54 ms /   225 runs   (  173.29 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   46878.73 ms /   655 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.96 ms /   256 runs   (    0.15 ms per token,  6570.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7263.62 ms /   479 tokens (   15.16 ms per token,    65.95 tokens per second)\n",
      "llama_print_timings:        eval time =   44082.48 ms /   255 runs   (  172.87 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   52214.38 ms /   734 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      29.38 ms /   193 runs   (    0.15 ms per token,  6568.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7677.48 ms /   511 tokens (   15.02 ms per token,    66.56 tokens per second)\n",
      "llama_print_timings:        eval time =   34158.96 ms /   192 runs   (  177.91 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   42487.75 ms /   703 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.92 ms /   256 runs   (    0.15 ms per token,  6578.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5828.22 ms /   384 tokens (   15.18 ms per token,    65.89 tokens per second)\n",
      "llama_print_timings:        eval time =   44945.63 ms /   255 runs   (  176.26 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   51657.01 ms /   639 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.31 ms /   256 runs   (    0.15 ms per token,  6512.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9710.00 ms /   621 tokens (   15.64 ms per token,    63.95 tokens per second)\n",
      "llama_print_timings:        eval time =   44626.13 ms /   255 runs   (  175.00 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   55247.21 ms /   876 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.58 ms /   157 runs   (    0.16 ms per token,  6387.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8143.85 ms /   529 tokens (   15.39 ms per token,    64.96 tokens per second)\n",
      "llama_print_timings:        eval time =   27642.09 ms /   156 runs   (  177.19 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   36332.20 ms /   685 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      30.56 ms /   199 runs   (    0.15 ms per token,  6511.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8272.49 ms /   526 tokens (   15.73 ms per token,    63.58 tokens per second)\n",
      "llama_print_timings:        eval time =   35196.08 ms /   198 runs   (  177.76 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   44163.78 ms /   724 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.66 ms /   256 runs   (    0.15 ms per token,  6455.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7732.91 ms /   489 tokens (   15.81 ms per token,    63.24 tokens per second)\n",
      "llama_print_timings:        eval time =   43387.32 ms /   255 runs   (  170.15 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   52039.12 ms /   744 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.76 ms /   256 runs   (    0.16 ms per token,  6438.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7323.67 ms /   496 tokens (   14.77 ms per token,    67.73 tokens per second)\n",
      "llama_print_timings:        eval time =   42914.86 ms /   255 runs   (  168.29 ms per token,     5.94 tokens per second)\n",
      "llama_print_timings:       total time =   51129.59 ms /   751 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.81 ms /   256 runs   (    0.16 ms per token,  6429.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8657.75 ms /   549 tokens (   15.77 ms per token,    63.41 tokens per second)\n",
      "llama_print_timings:        eval time =   44229.79 ms /   255 runs   (  173.45 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   53820.22 ms /   804 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.41 ms /   256 runs   (    0.15 ms per token,  6496.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9518.82 ms /   592 tokens (   16.08 ms per token,    62.19 tokens per second)\n",
      "llama_print_timings:        eval time =   44109.26 ms /   255 runs   (  172.98 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   54534.52 ms /   847 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      26.34 ms /   169 runs   (    0.16 ms per token,  6416.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7407.70 ms /   494 tokens (   15.00 ms per token,    66.69 tokens per second)\n",
      "llama_print_timings:        eval time =   30509.20 ms /   168 runs   (  181.60 ms per token,     5.51 tokens per second)\n",
      "llama_print_timings:       total time =   38483.32 ms /   662 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.60 ms /   256 runs   (    0.15 ms per token,  6465.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8048.30 ms /   518 tokens (   15.54 ms per token,    64.36 tokens per second)\n",
      "llama_print_timings:        eval time =   46062.51 ms /   255 runs   (  180.64 ms per token,     5.54 tokens per second)\n",
      "llama_print_timings:       total time =   55009.54 ms /   773 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.10 ms /   256 runs   (    0.15 ms per token,  6547.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8925.61 ms /   556 tokens (   16.05 ms per token,    62.29 tokens per second)\n",
      "llama_print_timings:        eval time =   45499.56 ms /   255 runs   (  178.43 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   55323.24 ms /   811 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      15.68 ms /   104 runs   (    0.15 ms per token,  6633.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10174.38 ms /   628 tokens (   16.20 ms per token,    61.72 tokens per second)\n",
      "llama_print_timings:        eval time =   18517.05 ms /   103 runs   (  179.78 ms per token,     5.56 tokens per second)\n",
      "llama_print_timings:       total time =   29029.45 ms /   731 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.60 ms /   256 runs   (    0.15 ms per token,  6631.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8475.24 ms /   545 tokens (   15.55 ms per token,    64.30 tokens per second)\n",
      "llama_print_timings:        eval time =   44774.85 ms /   255 runs   (  175.59 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   54106.95 ms /   800 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      24.39 ms /   155 runs   (    0.16 ms per token,  6355.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9765.19 ms /   606 tokens (   16.11 ms per token,    62.06 tokens per second)\n",
      "llama_print_timings:        eval time =   26734.69 ms /   154 runs   (  173.60 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   37033.73 ms /   760 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      35.71 ms /   234 runs   (    0.15 ms per token,  6553.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9136.52 ms /   550 tokens (   16.61 ms per token,    60.20 tokens per second)\n",
      "llama_print_timings:        eval time =   40823.92 ms /   233 runs   (  175.21 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   50775.69 ms /   783 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.24 ms /   256 runs   (    0.16 ms per token,  6361.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8067.23 ms /   525 tokens (   15.37 ms per token,    65.08 tokens per second)\n",
      "llama_print_timings:        eval time =   45591.14 ms /   255 runs   (  178.79 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =   54557.03 ms /   780 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.34 ms /   256 runs   (    0.15 ms per token,  6506.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8246.45 ms /   520 tokens (   15.86 ms per token,    63.06 tokens per second)\n",
      "llama_print_timings:        eval time =   45275.83 ms /   255 runs   (  177.55 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   54421.04 ms /   775 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.24 ms /   256 runs   (    0.15 ms per token,  6523.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7619.20 ms /   480 tokens (   15.87 ms per token,    63.00 tokens per second)\n",
      "llama_print_timings:        eval time =   45084.12 ms /   255 runs   (  176.80 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   53573.97 ms /   735 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      29.21 ms /   193 runs   (    0.15 ms per token,  6606.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4748.05 ms /   296 tokens (   16.04 ms per token,    62.34 tokens per second)\n",
      "llama_print_timings:        eval time =   32605.90 ms /   192 runs   (  169.82 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =   38004.83 ms /   488 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.99 ms /   256 runs   (    0.15 ms per token,  6565.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6839.62 ms /   442 tokens (   15.47 ms per token,    64.62 tokens per second)\n",
      "llama_print_timings:        eval time =   46067.01 ms /   255 runs   (  180.65 ms per token,     5.54 tokens per second)\n",
      "llama_print_timings:       total time =   53780.19 ms /   697 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      37.04 ms /   243 runs   (    0.15 ms per token,  6560.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8438.87 ms /   539 tokens (   15.66 ms per token,    63.87 tokens per second)\n",
      "llama_print_timings:        eval time =   43436.74 ms /   242 runs   (  179.49 ms per token,     5.57 tokens per second)\n",
      "llama_print_timings:       total time =   52701.34 ms /   781 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.39 ms /   256 runs   (    0.15 ms per token,  6499.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9993.94 ms /   622 tokens (   16.07 ms per token,    62.24 tokens per second)\n",
      "llama_print_timings:        eval time =   45838.09 ms /   255 runs   (  179.76 ms per token,     5.56 tokens per second)\n",
      "llama_print_timings:       total time =   56735.32 ms /   877 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.07 ms /   256 runs   (    0.15 ms per token,  6552.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9130.53 ms /   559 tokens (   16.33 ms per token,    61.22 tokens per second)\n",
      "llama_print_timings:        eval time =   45325.06 ms /   255 runs   (  177.75 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   55337.14 ms /   814 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.66 ms /   256 runs   (    0.15 ms per token,  6455.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7781.32 ms /   486 tokens (   16.01 ms per token,    62.46 tokens per second)\n",
      "llama_print_timings:        eval time =   46425.27 ms /   255 runs   (  182.06 ms per token,     5.49 tokens per second)\n",
      "llama_print_timings:       total time =   55105.43 ms /   741 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.93 ms /   256 runs   (    0.15 ms per token,  6575.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9480.46 ms /   593 tokens (   15.99 ms per token,    62.55 tokens per second)\n",
      "llama_print_timings:        eval time =   45449.57 ms /   255 runs   (  178.23 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:       total time =   55825.11 ms /   848 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.37 ms /   256 runs   (    0.15 ms per token,  6501.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9283.24 ms /   557 tokens (   16.67 ms per token,    60.00 tokens per second)\n",
      "llama_print_timings:        eval time =   44812.83 ms /   255 runs   (  175.74 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   54993.27 ms /   812 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.13 ms /   256 runs   (    0.16 ms per token,  6379.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7771.72 ms /   506 tokens (   15.36 ms per token,    65.11 tokens per second)\n",
      "llama_print_timings:        eval time =   44238.92 ms /   255 runs   (  173.49 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   52912.70 ms /   761 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.97 ms /   256 runs   (    0.16 ms per token,  6405.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9118.01 ms /   586 tokens (   15.56 ms per token,    64.27 tokens per second)\n",
      "llama_print_timings:        eval time =   45807.63 ms /   255 runs   (  179.64 ms per token,     5.57 tokens per second)\n",
      "llama_print_timings:       total time =   55836.14 ms /   841 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      40.03 ms /   256 runs   (    0.16 ms per token,  6395.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10047.63 ms /   638 tokens (   15.75 ms per token,    63.50 tokens per second)\n",
      "llama_print_timings:        eval time =   45295.13 ms /   255 runs   (  177.63 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   56216.41 ms /   893 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      26.43 ms /   169 runs   (    0.16 ms per token,  6395.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10966.08 ms /   624 tokens (   17.57 ms per token,    56.90 tokens per second)\n",
      "llama_print_timings:        eval time =   30492.95 ms /   168 runs   (  181.51 ms per token,     5.51 tokens per second)\n",
      "llama_print_timings:       total time =   42025.22 ms /   792 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.27 ms /   256 runs   (    0.15 ms per token,  6519.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2729.33 ms /   133 tokens (   20.52 ms per token,    48.73 tokens per second)\n",
      "llama_print_timings:        eval time =   42548.61 ms /   255 runs   (  166.86 ms per token,     5.99 tokens per second)\n",
      "llama_print_timings:       total time =   46151.99 ms /   388 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.41 ms /   256 runs   (    0.15 ms per token,  6494.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10446.47 ms /   666 tokens (   15.69 ms per token,    63.75 tokens per second)\n",
      "llama_print_timings:        eval time =   45074.44 ms /   255 runs   (  176.76 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   56433.30 ms /   921 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      26.44 ms /   171 runs   (    0.15 ms per token,  6468.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11077.97 ms /   702 tokens (   15.78 ms per token,    63.37 tokens per second)\n",
      "llama_print_timings:        eval time =   30558.41 ms /   170 runs   (  179.76 ms per token,     5.56 tokens per second)\n",
      "llama_print_timings:       total time =   42220.74 ms /   872 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.33 ms /   256 runs   (    0.15 ms per token,  6508.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9710.93 ms /   621 tokens (   15.64 ms per token,    63.95 tokens per second)\n",
      "llama_print_timings:        eval time =   44438.37 ms /   255 runs   (  174.27 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   55059.79 ms /   876 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.18 ms /   256 runs   (    0.15 ms per token,  6534.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7623.48 ms /   473 tokens (   16.12 ms per token,    62.05 tokens per second)\n",
      "llama_print_timings:        eval time =   43899.88 ms /   255 runs   (  172.16 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   52413.90 ms /   728 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.25 ms /   256 runs   (    0.15 ms per token,  6522.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8808.85 ms /   541 tokens (   16.28 ms per token,    61.42 tokens per second)\n",
      "llama_print_timings:        eval time =   44842.49 ms /   255 runs   (  175.85 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   54540.37 ms /   796 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      35.56 ms /   229 runs   (    0.16 ms per token,  6439.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5938.60 ms /   368 tokens (   16.14 ms per token,    61.97 tokens per second)\n",
      "llama_print_timings:        eval time =   38076.80 ms /   228 runs   (  167.00 ms per token,     5.99 tokens per second)\n",
      "llama_print_timings:       total time =   44803.88 ms /   596 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.70 ms /   256 runs   (    0.16 ms per token,  6448.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8008.55 ms /   519 tokens (   15.43 ms per token,    64.81 tokens per second)\n",
      "llama_print_timings:        eval time =   44816.34 ms /   255 runs   (  175.75 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   53727.84 ms /   774 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      38.55 ms /   256 runs   (    0.15 ms per token,  6640.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10083.48 ms /   577 tokens (   17.48 ms per token,    57.22 tokens per second)\n",
      "llama_print_timings:        eval time =   44642.74 ms /   255 runs   (  175.07 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =   55587.48 ms /   832 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      33.99 ms /   224 runs   (    0.15 ms per token,  6589.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6887.56 ms /   448 tokens (   15.37 ms per token,    65.04 tokens per second)\n",
      "llama_print_timings:        eval time =   39230.80 ms /   223 runs   (  175.92 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   46891.00 ms /   671 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.59 ms /   256 runs   (    0.15 ms per token,  6466.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7156.50 ms /   425 tokens (   16.84 ms per token,    59.39 tokens per second)\n",
      "llama_print_timings:        eval time =   44020.38 ms /   255 runs   (  172.63 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   52080.93 ms /   680 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.75 ms /   256 runs   (    0.16 ms per token,  6440.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12295.46 ms /   771 tokens (   15.95 ms per token,    62.71 tokens per second)\n",
      "llama_print_timings:        eval time =   45782.19 ms /   255 runs   (  179.54 ms per token,     5.57 tokens per second)\n",
      "llama_print_timings:       total time =   58971.24 ms /  1026 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.56 ms /   256 runs   (    0.15 ms per token,  6470.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6350.66 ms /   425 tokens (   14.94 ms per token,    66.92 tokens per second)\n",
      "llama_print_timings:        eval time =   42818.26 ms /   255 runs   (  167.91 ms per token,     5.96 tokens per second)\n",
      "llama_print_timings:       total time =   50068.87 ms /   680 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9472.76 ms\n",
      "llama_print_timings:      sample time =      39.19 ms /   256 runs   (    0.15 ms per token,  6532.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10051.44 ms /   630 tokens (   15.95 ms per token,    62.68 tokens per second)\n",
      "llama_print_timings:        eval time =   42991.96 ms /   255 runs   (  168.60 ms per token,     5.93 tokens per second)\n",
      "llama_print_timings:       total time =   53978.85 ms /   885 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming your results dictionary is named `results` and contains all the final data\n",
    "\n",
    "# Specify the file path where you want to save your results\n",
    "results_file_path = 'final_results.pkl'\n",
    "\n",
    "# Saving the results to a file\n",
    "with open(results_file_path, 'wb') as file:\n",
    "    pickle.dump(results, file)\n",
    "\n",
    "print(f\"Final results saved to {results_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the current device to CUDA device 1\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn import DataParallel\n",
    "import numpy as np\n",
    "\n",
    "class MistralEmbedder:\n",
    "    def __init__(self, model_name=\"Salesforce/SFR-Embedding-Mistral\", device='cuda:1'):\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of text documents using the Mistral model.\n",
    "        \n",
    "        Parameters:\n",
    "        - texts (List[str]): A list of texts to embed.\n",
    "        \n",
    "        Returns:\n",
    "        - List of embeddings as numpy arrays.\n",
    "        \"\"\"\n",
    "        # Ensure texts is a list for batch processing\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Generate embeddings and return them as numpy arrays\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings\n",
    "    def embed_query(self, query):\n",
    "        \"\"\"\n",
    "        Generate an embedding for a single query string using the Mistral model.\n",
    "        \n",
    "        Parameters:\n",
    "        - query (str): The query string to embed.\n",
    "        \n",
    "        Returns:\n",
    "        - A numpy array representing the embedding of the query.\n",
    "        \"\"\"\n",
    "        # Generate the embedding for the query\n",
    "        embedding = self.model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "        # Convert numpy array to list if necessary\n",
    "        embedding_list = embedding.tolist() if isinstance(embedding, np.ndarray) else embedding\n",
    "\n",
    "        # Return the embedding\n",
    "        return embedding_list\n",
    "\n",
    "# Initialize the embedding wrapper\n",
    "mistral_embedder = MistralEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = docs_texts.copy()\n",
    "\n",
    "# Build the vector store with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=mistral_embedder, persist_directory=\"MistralRAPTOREmbed/\")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# # Initialize Llama model\n",
    "# n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "# n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "# llm = LlamaCpp(\n",
    "#     model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "#     n_gpu_layers=n_gpu_layers,\n",
    "#     n_batch=n_batch,\n",
    "#     n_ctx=2048,\n",
    "#     f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load questions from questions.txt\n",
    "with open(\"SubmissionData/test/questions.txt\", \"r\") as f:\n",
    "    questions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Output directory for answers\n",
    "output_dir = \"SubmissionData/system_outputs/\"\n",
    "answer_file = os.path.join(output_dir, \"MistralEmbed.txt\")\n",
    "\n",
    "# Run the question-answering loop and save answers\n",
    "answers = []\n",
    "with tqdm(total=len(questions), desc=\"Answering questions\") as progress_bar:\n",
    "    with open(answer_file, \"w\") as f:\n",
    "        for question in questions:\n",
    "            response = qa_chain.invoke(question)\n",
    "            f.write(response + \"\\n\")\n",
    "            answers.append(response)\n",
    "            progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How to define a RAG chain? Give me a specific code example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group texts by their cluster ID\n",
    "clustered_texts = defaultdict(list)\n",
    "for _, row in df_clustered_texts.iterrows():\n",
    "    cluster_id = row['cluster'][0]  # Assuming each text belongs to one cluster\n",
    "    clustered_texts[cluster_id].append(row['text'])\n",
    "\n",
    "# Summarize texts within each cluster (simple join for this example)\n",
    "cluster_summaries = {cluster_id: ' '.join(texts) for cluster_id, texts in clustered_texts.items()}\n",
    "\n",
    "# For a more advanced summarization, you might use a model like BART or T5 loaded from HuggingFace Transformers\n",
    "# and pass `texts` to it for generating summaries. This is just a placeholder for your summarization logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not good empirical data on what the distribution for any particular topic should be if systemic biases did not exist in either Wikipedia or society (the world as it could and should be [2]). Therefore, in this track we adopted a compromise: we averaged the empirical distribution of groups among relevant documents with the world population (for location) or equality (for gender) to derive the target group distribution. We represented the group alignment of a document d with an alignment vector ad  [0, 1]|G|. adg is document ds alignment with group g. A  [0, 1]|D||G| is the alignment matrix for all documents. aworld denotes the distribution of the world.4 We considered fairness with respect to two group sets, Ggeo and Ggender. We operationalized this inter- sectional objective by letting G = Ggeo  Ggender, the Cartesian product of the two group sets. Further, alignment under either group set may be unknown; we represented this case by treating unknown as its own group (g?) in each set. In the product set, a documents alignment may be unknown for either or both groups.\n",
      "\n",
      "In all metrics, we use log discounting to compute attention weights: Within each work-needed bin of relevant documents, group exposure is fairly distributed according to the average of the distribution of relevant documents and the distribution of global population (the same average target as before). 1\n",
      "\n",
      "|L|\n",
      "\n",
      "X\n",
      "\n",
      "LL\n",
      "\n",
      "wL1\n",
      "\n",
      "d\n",
      "\n",
      "This forms an exposure vector (cid:15)  R|D|.\n",
      "\n",
      "It is aggregated into a group exposure vector , including\n",
      "\n",
      "unknown as a group:\n",
      "\n",
      " = AT(cid:15)\n",
      "\n",
      "Our implementation rearranges the mean and aggregate operations, but the result is mathematically\n",
      "\n",
      "equivalent. We then compare these system exposures with the target exposures (cid:15) for each query. This starts with the per-document ideal exposure; if mw is the number of relevant documents with work-needed level w  {1, 2, 3, 4}, then according to Diaz et al. [1] the ideal exposure for document d is computed as:\n",
      "\n",
      "(cid:15) d\n",
      "\n",
      "=\n",
      "\n",
      "1 mwd\n",
      "\n",
      "mwdX\n",
      "\n",
      "i=m>wd +1\n",
      "\n",
      "vi\n",
      "\n",
      "We use this to compute the non-averaged target distribution :\n",
      "\n",
      " = AT(cid:15) Since we include unknown as a group, we have a challenge with computing the target distribution by averaging the empirical distribution of relevant documents and the global population  global population does not provide any information on the proportion of relevant articles for which the fairness attributes are relevant. Our solution, therefore, is to average the distribution of known-group documents with the world population, and re-normalize so the nal distribution is a probability distribution, but derive the proportion of known- to unknown-group documents entirely from the empirical distribution of relevant documents. Extended to handle partially-unknown documents, this procedure proceeds as follows: Average the distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender).\n",
      "\n",
      "Average the distribution of documents with unknown location but known gender with the equality gender distribution.\n",
      "\n",
      "Average the distribution of documents with unknown gender but known location with the world pop- ulation. Implementation Now, to do this for every query, well use a function that takes a data frame for a querys relevant docs and performs all of the above operations:\n",
      "\n",
      "def query xalign(qdf):\n",
      "\n",
      "pages = qdf['page id'] pages = pages[pages.isin(page xalign.indexes['page'])] q xa = page xalign.loc[pages.values, :, :] q am = q xa.sum(axis=0)\n",
      "\n",
      "# clear and normalize q am[0, 0] = 0\n",
      "\n",
      "28\n",
      "\n",
      "q am = q am / q am.sum() 1.095016e-03 6.526425e-03 3.311463e-06\n",
      "\n",
      "Now with that function, we can compute the alignment vector for each query.\n",
      "\n",
      "train qtarget = train qrels.groupby('id').apply(query xalign) train qtarget\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5 page work = pages.set index('page id').quality score disc.astype(pd.CategoricalDtype(ordered=True)) page work = page work.cat.reorder categories(work order) page work.name = 'quality'\n",
      "\n",
      "A.6.1 Work and Target Exposure\n",
      "\n",
      "The rst thing we need to do to prepare the metric is to compute the work-needed for each topics pages, and use that to compute the target exposure for each (relevant) page in the topic. This is because an ideal ranking orders relevant documents in decreasing order of work needed, followed by irrelevant documents. All relevant documents at a given work level should receive the same expected exposure.\n",
      "\n",
      "First, look up the work for each query page (query page work, or qpw):\n",
      "\n",
      "qpw = qrels.join(page work, on='page id') qpw id 572 1 0 627 1 1 903 1 2 1193 1 3 1542 1 4 ... ... ... 2199072 150 63656179 2199073 150 63807245 2199074 150 64614938 2199075 150 64716982 2199076 150 65355704\n",
      "\n",
      "page_id quality C FA C B GA ... Start NaN C C C\n",
      "\n",
      "[2199077 rows x 3 columns]\n",
      "\n",
      "And now use that to compute the number of documents at each work level:\n",
      "\n",
      "qwork = qpw.groupby(['id', 'quality'])['page id'].count() qwork\n",
      "\n",
      "id 1\n",
      "\n",
      "quality Stub Start C B\n",
      "\n",
      "1527 2822 1603 610\n",
      "\n",
      "35\n",
      "\n",
      "GA\n",
      "\n",
      "240\n",
      "\n",
      "... 138 127 35 16 8 Name: page_id, Length: 636, dtype: int64\n",
      "\n",
      "150 Start C B GA FA\n",
      "\n",
      "Now we need to convert this into target exposure levels. This function will, given a series of counts for\n",
      "\n",
      "each work level, compute the expected exposure a page at that work level should receive.\n",
      "\n",
      "def qw tgt exposure(qw counts: pd.Series) -> pd.Series:\n",
      "\n",
      "if 'id' == qw counts.index.names[0]:\n",
      "\n",
      "qw counts = qw counts.reset index(level='id', drop=True) qwc = qw counts.reindex(work order, fill value=0).astype('i4') tot = int(qwc.sum()) da = metrics.discount(tot) qwp = qwc.shift(1, fill value=0) qwc s = qwc.cumsum() qwp s = qwp.cumsum() res = pd.Series(\n",
      "\n",
      "[np.mean(da[s:e]) for (s, e) in zip(qwp s, qwc s)], index=qwc.index\n",
      "\n",
      ") return res\n",
      "\n",
      "Well then apply this to each topic, to determine the per-topic target exposures:\n",
      "\n",
      "qw pp target = qwork.groupby('id').apply(qw tgt exposure) qw pp target.name = 'tgt exposure' qw pp target 0.114738 0.087373 0.081146 0.079298 0.078702 ... 0.154202 0.127359 0.120441 0.118827 0.118126\n",
      "\n",
      "Name: tgt_exposure, Length: 636, dtype: float32\n",
      "\n",
      "We can now merge the relevant document work categories with this exposure, to compute the target\n",
      "\n",
      "exposure for each relevant document:\n",
      "\n",
      "36\n",
      "\n",
      "qp exp = qpw.join(qw pp target, on=['id', 'quality']) qp exp = qp exp.set index(['id', 'page id'])['tgt exposure'] qp exp.index.names = ['q id', 'page id'] qp exp\n",
      "\n",
      "q_id page_id 1\n",
      "\n",
      "572 627 903 1193 1542\n",
      "\n",
      "150 63656179 63807245 64614938 64716982 65355704\n",
      "\n",
      "0.081146 0.078438 0.081146 0.079298 0.078702 ... 0.154202 NaN 0.127359 0.127359 0.127359\n",
      "\n",
      "Name: tgt_exposure, Length: 2199077, dtype: float32\n",
      "\n",
      "A.6.2 Geographic Alignment\n",
      "\n",
      "Now that weve computed per-page target exposure, were ready to set up the geographic alignment vectors for computing the per-group expected exposure with geographic data.\n",
      "\n",
      "Were going to start by getting the alignments for relevant documents for each topic: 0.0 0.0 0.0 0.0 0.0 ... 0.0 NaN 0.0 0.0 0.0\n",
      "\n",
      "Latin America and the Caribbean\n",
      "\n",
      "Northern America\n",
      "\n",
      "Oceania\n",
      "\n",
      "q_id page_id 572 1 627 903 1193 1542\n",
      "\n",
      "... 150 63656179 63807245\n",
      "\n",
      "0.0 0.0 0.0 0.0 0.0 ... 0.0 NaN\n",
      "\n",
      "0.0 0.0 0.0 0.0 0.0 ... 0.0 NaN\n",
      "\n",
      "0.0 0.0 0.0 0.0 0.0 ... 0.0 NaN\n",
      "\n",
      "37\n",
      "\n",
      "64614938 64716982 65355704\n",
      "\n",
      "0.0 0.0 0.0\n",
      "\n",
      "0.0 0.0 0.0\n",
      "\n",
      "0.0 0.0 0.0\n",
      "\n",
      "[2199077 rows x 8 columns]\n",
      "\n",
      "Now we need to compute the per-query target exposures. This starst with aligning our vectors: qp geo exp, qp geo align = qp exp.align(qp geo align, fill value=0)\n",
      "\n",
      "And now we can multiply the exposure vector by the alignment vector, and summing by topic - this is\n",
      "\n",
      "equivalent to the matrix-vector multiplication on a topic-by-topic basis.\n",
      "\n",
      "qp aexp = qp geo align.multiply(qp geo exp, axis=0) q geo align = qp aexp.groupby('q id').sum() Now things get a little weird. We want to average the empirical distribution with the world population to compute our fairness target. However, we dont have empirical data on the distribution of articles that do or do not have geographic alignments.\n",
      "\n",
      "Therefore, we are going to average only the known-geography vector with the world population. This\n",
      "\n",
      "proceeds in N steps: pages = qdf['page id'] pages = pages[pages.isin(page xalign.indexes['page'])] q xa = page xalign.loc[pages.values, :, :]\n",
      "\n",
      "# now we need to get the exposure for the pages, and multiply p exp = qp exp.loc[qdf.name] assert p exp.index.is unique p exp = xr.DataArray(p exp, dims=['page'])\n",
      "\n",
      "# and we multiply! q xa = q xa * p exp\n",
      "\n",
      "# normalize into a matrix (this time we don't clear) q am = q xa.sum(axis=0) q am = q am / q am.sum()\n"
     ]
    }
   ],
   "source": [
    "print(cluster_summaries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "all_texts = docs_texts.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Chroma' has no attribute 'from_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming embeddings are in the shape (number_of_documents, embedding_dimension)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create the vector store with Chroma using embeddings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m(embeddings)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Quick check to ensure vectorstore is set up correctly\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# This is a simple check; adjust based on how you can best verify your vectorstore setup\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Chroma' has no attribute 'from_embeddings'"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Assuming embeddings are in the shape (number_of_documents, embedding_dimension)\n",
    "# Create the vector store with Chroma using embeddings\n",
    "vectorstore = Chroma.from_documents(docs_text, embeddings)\n",
    "\n",
    "# Quick check to ensure vectorstore is set up correctly\n",
    "# This is a simple check; adjust based on how you can best verify your vectorstore setup\n",
    "try:\n",
    "    test_query_embedding = embeddings[0]  # Use the first document's embedding as a test query\n",
    "    test_results = vectorstore.similarity_search(test_query_embedding, top_k=1)\n",
    "    print(\"Chroma vectorstore is set up correctly. Test query result:\", test_results)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "# Setup LLaMA\n",
    "n_gpu_layers = 1  # Adjust as needed\n",
    "n_batch = 512     # Adjust based on hardware capabilities\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Retrieval and Prompting setup for RAPTOR\n",
    "prompt_template = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use one sentence maximum and keep the answer CONCISE. Question: {question} \\nContext: {context} \\nAnswer:\"\n",
    "prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=prompt_template))\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "rag_prompt.messages = [prompt]\n",
    "\n",
    "# Convert the vectorstore to a retriever for RAPTOR\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define the document formatting function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc['text'] for doc in docs)\n",
    "\n",
    "# Define the RAPTOR QA chain\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Load questions\n",
    "with open(\"SubmissionData/test/questions.txt\", \"r\") as f:\n",
    "    questions = [line.strip() for line in f]\n",
    "\n",
    "# File to save answers\n",
    "answer_file = \"SubmissionData/system_outputs/answers.txt\"\n",
    "\n",
    "# Initialize or clear the answer file\n",
    "with open(answer_file, \"w\") as f:\n",
    "    pass\n",
    "\n",
    "# Generate and save answers\n",
    "from tqdm import tqdm\n",
    "\n",
    "for question in tqdm(questions):\n",
    "    response = qa_chain.invoke(question)\n",
    "    with open(answer_file, \"a\") as f:\n",
    "        f.write(response + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-rag",
   "language": "python",
   "name": "nlp-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
