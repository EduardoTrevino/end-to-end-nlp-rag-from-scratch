Group Membership Bias
Ali Vardasbi
University of Amsterdam
Amsterdam, The Netherlands
a.vardasbi@uva.nlMaarten de Rijke
University of Amsterdam
Amsterdam, The Netherlands
m.derijke@uva.nl
Fernando Diaz
Google
Montréal, Canada
diazf@acm.orgMostafa Dehghani
Google Brain
Amsterdam, The Netherlands
dehghani@google.com
ABSTRACT
When learning to rank from user interactions, search and recom-
mendation systems must address biases in user behavior to provide
a high-quality ranking. One type of bias that has recently been
studied in the ranking literature is when sensitive attributes, such
as gender, have an impact on a user’s judgment about an item’s
utility. For example, in a search for an expertise area, some users
may be biased towards clicking on male candidates over female
candidates. We call this type of bias group membership bias or group
bias for short.
Increasingly, we seek rankings that not only have high utility but
are also fair to individuals and sensitive groups. Merit-based fairness
measures rely on the estimated merit or utility of the items. With
group bias, the utility of the sensitive groups is under-estimated,
hence, without correcting for this bias, a supposedly fair ranking is
not truly fair. In this paper, first, we analyze the impact of group
bias on ranking quality as well as two well-known merit-based
fairness metrics and show that group bias can hurt both ranking
and fairness. Then, we provide a correction method for group bias
that is based on the assumption that the utility score of items in
different groups comes from the same distribution. This assumption
has two potential issues of sparsity and equality-instead-of-equity,
which we use an amortized approach to solve. We show that our
correction method can consistently compensate for the negative
impact of group bias on ranking quality and fairness metrics.
CCS CONCEPTS
•Information systems →Learning to rank .
KEYWORDS
Ranking fairness, Unbiased learning to rank
ACM Reference Format:
Ali Vardasbi, Maarten de Rijke, Fernando Diaz, and Mostafa Dehghani.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
©2023 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn2023. Group Membership Bias. In Proceedings of ACM Conference (Con-
ference’17). ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/
nnnnnnn.nnnnnnn
1 INTRODUCTION
Modern online search and recommender systems leverage user
interaction data to enhance their ranking quality. When using hu-
man interactions, however, we need to account for human bias and
the possibility of learning unfair ranking policies. In the context
of learning to rank ( LTR), the term bias usually refers to unequal
treatment of items with equal utility by users [ 18]. For example,
position bias occurs when items at the top of a ranked list receive
more clicks than those relevant lower down: higher items in a list
absorb more exposure . Studies show that such a bias, if left uncor-
rected, degrades the ranking quality of a system trained on the
user interactions [ 2,19,48,51]. As a result, a system should return
rankings that strive to a certain extent for fairness of exposure.
There are different definitions for fairness of exposure in ranking,
leading to different metrics [ 11,40,43,52], however, the core idea is
the same: items with similar levels of utility should receive similar
exposures by the system. Studies show that without meeting fair-
ness of exposure, bias towards the privileged groups or individuals
is reinforced, both in what the system learns from the ongoing
interactions [ 14,16], and in users’ judgments about the utility of
items [22, 45].
Group bias. The system can only ensure that items with similar
utility receive comparable exposure to users, by arranging them
accordingly. However, this alone is insufficient. Studies show that
users’ judgments about the utility of items are affected by their
perception of the item’s group membership [ 22,25,45]. This means
that, even when the exposure of two high-utility items from two
different groups is the same, the users may judge them differently
and one group may receive more clicks than the other. We refer
to this behavior as group membership bias orgroup bias for short.
Our study focuses on the scenario of two groups, where the term
“affected” refers to the group whose items are prone to underesti-
mation and receive fewer clicks than they ideally should. Group
bias is closely related to the concept of implicit bias, defined as
unconscious and unintentional preference of individuals based on
their membership in particular groups [ 23]. Implicit bias is a special
case of group bias, as group bias includes intentional biases towards
particular groups as well.arXiv:2308.02887v1  [cs.IR]  5 Aug 2023Conference’17, July 2017, Washington, DC, USA Ali Vardasbi, Maarten de Rijke, Fernando Diaz, and Mostafa Dehghani
Overall Affected group Non-affected group
Tabular (training labels) LTR outputNDCG@10
1.0 0.8 0.60.40.60.81.0
1.0 0.8 0.60.40.60.81.0
Group underestimation factor Group underestimation factor
Figure 1: The impact of group bias on ranking performance
for the Yahoo! dataset.
Theoretical and experimental analysis. We analyze the im-
pact of group bias on ranking quality and merit-based fairness of
exposure metrics. We consider clicks as the primary measure of user
interactions, which are used in two ways: (i) For head queries, clicks
are memorized and the ranking is performed via tabular search.
In this case, the training labels are directly used to generate the
ranking presented to the user, and user preferences can often be
obtained with high accuracy [ 36]. (ii) For tail queries the clicks are
used as supervision signals to train an LTR model. In this case, the
outputs of the LTR model are used to generate the ranking that
is shown to the user. With that in mind, we provide a theoretical
analysis of the impact of group bias on various metrics over the
training labels. This gives us an understanding of the impact of
group bias on the head queries directly, and on the tail queries indi-
rectly. Theoretically analyzing the output of a LTR model involves
considering the architecture and loss function of the LTR model,
which is beyond the scope of this paper. For the experimental part,
however, we analyze the impact of group bias both on the training
labels as well as the outputs of an LTR model.
Impact on ranking. Previous work on implicit bias [ 8,23] has
shown that similar to other types of bias, implicit bias can degrade
the ranking quality of systems. In this work, we add to their theoret-
ical results by quantifying this degradation with an approximation
formula for the normalized discounted cumulative gain ( NDCG )
metric. Furthermore, we provide an important part that is missing
in previous work, i.e., an experimental analysis of the impact of
group bias on the outputs of an LTR model. Fig. 1 shows an ex-
ample of the impact of group bias on the ranking performance,
measured by NDCG . In these plots, the bars associated with the
“(non-) affected group” label show the NDCG@10 when only the
relevant items from the (non-) affected group are considered as
relevant. Furthermore, a lower group underestimation factor means
a more severe group bias. On the left, we observe the impact on the
training labels, i.e., tabular search, while the right plot shows the
impact on the outputs of a general LTR model. In both plots, we
observe that the affected group is hurt by the group bias, while the
other group has gained. Importantly, in both regimes, the overall
ranking quality is degraded by increasing the group bias.
Impact on fairness. Unlike other types of bias that may affect
fairness indirectly, group bias has a direct impact on fairness: Clicks
suffering from group bias can lead the system to undervalue the
utility scores of a particular group (see, e.g., Fig. 1). Consequently,
when the expected exposure is assigned to groups based on these
biased estimates of the utility, the ranking may not be truly fair. For
Tabular (train labels) LTR outputDTR↑(ideal: 1) EEL ↓(ideal: 0)Exposure ratio
1.0 0.8 0.60.50.60.70.80.91.0
Exposureℓ2loss
1.0 0.8 0.60.000.250.500.751.001.251.50
Group underestimation factor Group underestimation factor
Figure 2: The impact of group bias on fairness metrics for
the Yahoo! dataset.
our analyses, we consider two widely used metrics for the fairness of
exposure, namely disparate treatment ratio ( DTR ) [43] and expected
exposure loss ( EEL) [4,11]. Each metric has a definition for the ideal
expected exposure in terms of the utility, that leads to the fairest
ranking. Distinguishing between the true (unbiased) utility and
observed (biased) utility, we provide formulas for the change in the
true fairness metrics, when the target expected exposure is obtained
from the biased utility. Fig. 2 shows an example of the impact of
group bias on the DTR andEEL fairness metrics. DTR (left plot)
is a multiplicative metric, so we measure the ratio between the
target exposure of the affected group, to the target exposure of the
non-affected group.1We then normalize this ratio with the ratio
obtained from the full information case, i.e., using true utilities
without group bias. A DTR score of 1means the fairest exposure.
EEL (right plot) is an additive metric, so we measure the ℓ2loss
between the target exposure vector in the biased case and the full
information case. For EEL, a loss of 0means the fairest exposure.
In both metrics, we observe that group bias leads to noticeable
deviations from the full information case.
Correction. To correct for group bias, it should first be modeled.
We follow previous work on implicit bias [ 23] and model group
bias with a multiplicative factor. This allows us to use the inverse
propensity scoring ( IPS) method to correct for the bias [ 19,51].
Measuring group bias, however, is not as simple as measuring posi-
tion or trust bias. We argue that group bias measurement requires
some assumptions on the distribution of the true utility scores of
different groups. Following [ 8,13,23], one can assume that the
true utility scores of both groups come from the same distribu-
tion. Naively assuming that the utility scores for each query should
come from the same distribution, leads to a trivial solution with
equality instead of equity. In other words, equity is based on the
premise that exposure should be distributed based on utility, i.e.,
merit-based fairness. Assuming that the utility of different groups
is equal for each query, means that different groups should receive
equal exposure all the time, which means equality. To counter this,
we propose to aggregate the utility scores of items across all the
queries with similar expected group propensity and measure the
group bias parameter over this aggregated set of scores. We show
that our correction method based on the above amortized mea-
surement of the bias parameter is effective for restoring both the
ranking quality and fairness metrics.
Research questions. We answer the following questions:
(RQ1) What is the impact of group bias on the ranking quality and
1We follow [43] and always keep the ratio below unity.Group Membership Bias Conference’17, July 2017, Washington, DC, USA
the true fairness metric of head and tail queries?
(RQ2) How can we effectively correct for group bias, without sub-
stituting equality for equity?
2 RELATED WORK
There is an increasing number of studies indicating the existence
of group bias. Implicit bias, a special case of group bias, in which
the preference of one group over the other is unintentional, has
been widely studied in human behavior studies [e.g., 7,15,20].
More recently, implicit bias has been formalized in the set selection
problem [23] and extended to the ranking scenario [8, 13].
Here, we list a small number of example studies indicating that
group membership affects users’ judgment. Studies by Lyness and
Heilman [31] on performance evaluations and promotions of man-
agers indicate that standards for women promotion were stricter,
e.g., women had to show roughly twice as much evidence of compe-
tence as men to be seen as equally competent. In [ 22], it is observed
in a user study that in a career search, results that are consistent
with stereotypes for a career are rated higher. Sühr et al . [45] pose
the important question whether “fair ranking improve[s] minority
outcomes?” and arrive at the result that persistent gender pref-
erences of employers can limit the effectiveness of fair ranking
algorithms, and that fair ranking is more effective when the fea-
tures of an underrepresented candidate are similar to the average
overrepresented group features. Recently, Krieg et al . [25] in their
user study on gender sensitive queries from [ 24] show that per-
ceived gender bias affects judgment. Vlasceanu and Amodio [50]
conduct two user studies and observe that societal and algorithmic
gender bias affect each other: the algorithmic outputs of search
engines track pre-existing societal-level gender biases; and, at the
same time, exposure of users to these biased results shape users’
cognitive concepts and decisions.
All of the above examples confirm that group bias exists in user
interactions. In this paper, we study its impact on ranking and
fairness measures and propose a method to correct for it. Closest
to this paper is the work by Celis et al . [8] who show that implicit
bias degrades ranking quality and that by ensuring equality of
exposure, the ranking quality can be improved. What we add on
top of this work is to provide a formalization of the change of
ranking and merit-based fairness metrics as a result of group bias.
We also provide extensive experimental analyses of the impact of
group bias on the output of an LTR model.
The idea of our amortized correction to counter sparsity and
equality-instead-of-equity has similarities to the notion of amor-
tized fairness of exposure [ 6] where the exposures and utilities
of individuals (or groups) are aggregated across multiple queries
and the fairness metric is calculated according to the aggregated
exposure and utility. This corresponds to fairness evaluation. In
contrast, we aggregate the items associated with multiple queries to
find the group bias parameter that minimizes the distance between
the utility distribution of the affected and non-affected groups. This
corresponds to group bias correction.
Remark 1. Our terminology of group bias should not be confused
with the in-group bias, where a user favors members from their
own group over out of the group members [ 34,54]. In this study,
we follow a body of previous work on implicit and explicit biasand only focus on the group membership of the items and do not
consider the group of the users. Consequently, the issues related
to in-group bias, such as loyalty versus neutrality, are out of the
scope of this paper.
3 GROUP MEMBERSHIP BIAS
3.1 Definitions
As discussed in Section 2, prior work shows that the judgment of a
user about the relevance of an item may be affected by the item’s
group. Either unconsciously (as in implicit bias [ 13,23]) or due to
stereotypical bias [ 22,25], users tend to rate one group higher than
the other. In this paper, we do not aim to deal with the source of
this biased behavior and only focus on its impact on algorithms and
metrics. We call this behavior the group membership bias . Following
the well-known examination hypothesis [ 10] that says: An item is
clicked by a user if it is (i) examined; and (ii) found attractive by
that user, one can attribute group bias to the attractiveness part:
𝑃(𝐴|𝑞,𝑑,𝑔)=𝑓 𝑃(𝑅|𝑞,𝑑),𝑔, (1)
where𝐴,𝑅,𝑞and𝑑stand for attractiveness, relevance, query and
document, respectively, and 𝑔is the group of which 𝑑is a member.
Eq.(1)states that the attraction of an item to the user not only de-
pends on the item’s true relevance to the query, but is also a function
of the item’s group. Following the literature on implicit bias [ 23], we
assume this dependency to have a multiplicative form as follows:
𝑃(𝐴|𝑞,𝑑,𝑔)=𝛽𝑔·𝑃(𝑅|𝑞,𝑑). (2)
We call𝛽𝑔thegroup propensity . Notice that 𝛽𝑔is not necessarily
fixed across all queries.
Remark 2. Here we consider one sensitive attribute for simplicity
of notation. Extending our discussions to more attributes with
intersectional groups is possible using the formulation in [8, 33].
3.2 Ranking Regimes
Here, we distinguish between two LTR regimes: (i) tabular search
for head queries; and (ii) general LTR model for tail queries. Note
that the majority of previous studies focused only on the general
LTR regime [e.g., 35,44], or the tabular regime [e.g., 6,43]. In
contrast, we follow [49] and consider both LTR regimes.
Tabular search for head queries. In tabular search, users’ his-
torical interactions with the head queries are directly used to es-
timate items’ utilities [ 21,27–30,39,55]. In this regime, we assume
that𝑃(𝐴)can accurately be inferred from clicks: other types of bias
such as position and trust bias are corrected for and only group
bias remains in the signals. Our theoretical results on the impact
of group bias on different metrics, lie in this regime.
General LTR model for tail queries. For new queries and ones
that a tabular model is not confident about, an LTR model is used.
We assume that this LTR model is trained over the accurate estima-
tions of𝑃(𝐴)from the head queries. Writing 𝑟𝑞,𝑑for the relevance
of item𝑑to query𝑞, ranking metrics per query can usually be
expressed in the following form:
Δ𝑞=∑︁
𝑑𝜆𝑞,𝑑·𝑟𝑞,𝑑, (3)Conference’17, July 2017, Washington, DC, USA Ali Vardasbi, Maarten de Rijke, Fernando Diaz, and Mostafa Dehghani
where𝜆is a metric-specific coefficient, e.g., for discounted cumu-
lative gain ( DCG ) we have𝜆𝐷𝐶𝐺
𝑞,𝑑=−log 1+rank(𝑑)−1. Using
attractiveness instead of the true relevance to train an LTR model,
means that instead of Δ𝑞, the following metric is being optimized:
ˆΔ𝑞=∑︁
𝑑𝜆𝑞,𝑑·𝑃(𝐴|𝑞,𝑑)(2)=∑︁
𝑔𝛽𝑔∑︁
𝑑∈𝑔𝜆𝑞,𝑑·𝑃(𝑅|𝑞,𝑑).(4)
Comparing Eq. (3) and Eq. (4), it is easy to see that ˆΔ𝑞is biased:
E𝑅
Δ𝑞
=∑︁
𝑑𝜆𝑞,𝑑·𝑃(𝑅|𝑞,𝑑)≠ˆΔ𝑞, (5)
unless𝛽𝑔is equal across all groups, i.e., there is no group bias.
Similar to studies on position and trust bias [ 1,19,46–48], in our
experiments, we analyze the effect of group bias on the ranking
quality of the LTR model (RQ1). Due to the relationship between
group bias and fairness concerns, we go one step further and assess
how leaving the group bias uncorrected affects the optimization of
fairness metrics.
4 THEORETICAL RESULTS
We assume that there are two groups 𝐺A(affected) and 𝐺N(non-
affected), with group propensities 𝛽A<1and𝛽N=1. We further
assume binary relevance, i.e., 𝑟∈{0,1}, and assume that within
each group, attractiveness is monotone with respect to relevance:
∀𝑑,𝑑′∈𝐺𝑖,if𝑟𝑑=1 &𝑟𝑑′=0,then𝑃(𝐴|𝑞,𝑑)>𝑃(𝐴|𝑞,𝑑′).(6)
For brevity, we write 𝑎𝑑for the attractiveness probability of item
𝑑, assuming that there is no confusion about the query. Let the
number of candidate items for a query be 𝑛, out of which 𝑛Aand
𝑛Nitems belong to groups 𝐺Aand𝐺N, respectively. We indicate
the number of relevant items with 𝑛+
Aand𝑛+
N. To assess the impact
of group bias on different metrics, we measure the change in the
target metric when the observable attractiveness probabilities are
considered as a proxy for the true relevance scores.
4.1 Ranking Quality
For ranking quality, we calculate the NDCG of the list obtained from
sorting items based on their attractiveness probability and measure
its deviance from the ideal NDCG , i.e., 1. By definition, group bias
affects the attractiveness probabilities for one group (here 𝐺A). Let
𝑎∗be the minimum attractiveness value for the relevant items of
group𝐺N:
𝑎∗=min
𝑑∈𝐺N{𝑎𝑑|𝑟𝑑=1}. (7)
Items of𝐺Awith higher attractiveness values than 𝑎∗, are ranked
correctly with probability 1: Group bias has dampened their attrac-
tiveness probabilities, but still none of the non-relevant items is
ranked higher than them. We define an auxiliary random variable
𝜈to be the fraction of relevant items from the affected group 𝐺A
that are ranked correctly with probability 1:
𝜈=|{𝑑|𝑑∈𝐺A&𝑟𝑑=1 &𝑎𝑑>𝑎∗}|
|{𝑑|𝑑∈𝐺A&𝑟𝑑=1}|. (8)For uniformly distributed scores in the interval of [0,1], the mean
value of𝜈has a closed form as follows:
E[𝜈]=(
2−1
𝛽A,if𝛽A>0.5
0, otherwise.(9)
Theorem 4.1. In the presence of group bias, for uniformly dis-
tributed attractiveness scores, the change in the NDCG of the list,
sorted based on the items attractiveness, can be approximated by a
linear function of E[𝜈], i.e., the fraction of affected relevant items
that are still as attractive as the non-affected relevant items.
Proof. Our monotonicity assumption of the within-group at-
tractiveness (Eq. (6)) ensures that no relevant item is ranked lower
than non-relevant items of 𝐺A. This means that the 1−𝜈fraction
of the affected relevant items lies somewhere between 𝑛+
N+𝜈𝑛+
A
and𝑛N+𝑛+
Apositions. The expected DCG of the list would be as
follows:
E[𝐷𝐶𝐺]=𝑛+
N+𝜈𝑛+
A∑︁
𝑖=11
log(1+𝑖)+𝑛N+𝑛+
A∑︁
𝑖=𝑛+
N+𝜈𝑛+
A+1𝜉𝑖
log(1+𝑖),(10)
where𝜉𝑖depends on the distribution of the attractiveness scores.
For a uniform distribution, we have:
𝜉𝑖=(1−𝜈)𝑛+
A
𝑛N−𝑛+
N+(1−𝜈)𝑛+
A. (11)
Finally, using numerical analysis to approximate the average DCG
in Eq. (10) by a linear function of 𝜈, leads to a small approximation
error, e.g., a relative error of at most 5%in a top- 20setup. □
4.2 Merit-Based Fairness Metrics
Next, to see the impact of group bias on fairness metrics, we analyze
two well-known merit-based fairness of exposure metrics, namely
EEL andDTR . For both metrics, we calculate the target exposure in
two cases: the full information case where the true relevance scores
are used to compute the target exposure; and, the group biased
case where the attractiveness probabilities are used as proxies for
relevance to compute the target exposure. By change in true target
exposure we mean the difference between the above two cases.
4.2.1 EEL. In the next theorem, we calculate the change in the
target exposure of 𝐺Nas a result of group bias.
Theorem 4.2. In the presence of group bias, assuming the Position-
Based Model ( PBM ) user browsing model with logarithmic decay of
exposure as in DCG ,2the change in the target exposure of EEL can be
approximated as follows:
Δ(EEL)=𝑐·log# True relevant items
# Perceived relevant items
, (12)
where𝑐is a constant depending on 𝑛+
N,𝑛N, and𝑛+
N.
Proof. As we are working with two groups, and the sum of
the group exposures is fixed, to measure the change in the target
exposure vector, it is sufficient to measure the change in the target
exposure of one group and multiply it by 2.
To compute the expected exposure for EEL, the utility values
2Here we follow [ 12,49] for this assumption. Similar analysis can be performed for
other exposure models.Group Membership Bias Conference’17, July 2017, Washington, DC, USA
should be discrete. With a slight abuse of notation, we assume that
𝑎∗(instead of Eq. (7)) is the threshold used for discretization of the
attractiveness probabilities,3and we use ¯𝑎𝑑for the discretized value
of𝑎𝑑. Since𝛽N=1, we assume that ¯𝑎𝑑=𝑟𝑑for𝑑∈𝐺N. However,
for the affected items, because of 𝛽A<1, not all the scores are
necessarily correct. We re-use 𝜈from Eq. (8) to show the fraction
of affected relevant items that are still recognized as relevant.
For the average exposure of the relevant and non-relevant items
we use the following two approximations:
1
𝑚𝑚∑︁
𝑖=11
log(1+𝑖)≈𝛼log(𝑚)+𝑐 (13)
1
𝑛−𝑚𝑛∑︁
𝑖=𝑚1
log(1+𝑖)≈𝛼′log(𝑚)+𝑐′, (14)
where𝛼and𝛼′are constants, obtained by numerical analysis. For
example, for 𝑛=20,𝛼=−0.146and𝛼′=−0.022lead to relative ap-
proximation errors of at most 5%. In the full information case, there
are a total of 𝑛+
N+𝑛+
Arelevant items, i.e., 𝑚=𝑛+
N+𝑛+
Ain Eq. (13)
and(14). But with group bias, only 𝑚=𝑛+
N+𝜈𝑛+
Aof the items
are recognized as relevant. Consequently, the change in the target
exposure as a result of group bias can be approximated as follows:
Δ(EEL)=2 𝛼𝑛+
N+𝛼′(𝑛N−𝑛+
N)log 
𝑛+
N+𝑛+
A
𝑛+
N+𝜈𝑛+
A!
.□
4.2.2 DTR. DTR is a multiplicative metric. To have a meaningful
measure for the change in DTR in the presence of group bias, one
has to compute the ratio of the target expected exposure in the full
information ( 𝐸) and group-biased ( ˜𝐸) settings.
Theorem 4.3. In the presence of group bias, the change in the
target exposure of DTR, equals the fraction of affected relevant items
that are still as attractive as the non-affected relevant items.
Proof. Using the same notation as in the previous sections, and
noting that because of the binary relevance assumption, the utility
of each group is equal to the number of its relevant items, this ratio
is computed as follows:
𝜌(DTR)=˜𝐸A
˜𝐸N·𝐸N
𝐸A=𝜈𝑛+
N+𝜈𝑛+
A
𝑛+
N+𝑛+
A=𝜈. □
5 GROUP BIAS CORRECTION
Our multiplicative formulation of group bias in Eq. (2) allows us
to use IPSto correct for group bias, once we know the value of
the propensity 𝛽. The unbiasedness proof of IPS for this case is
exactly the same as that of position bias in [ 19,51]. However, similar
to position bias, the unbiasedness proof depends entirely on the
accurate estimation of the bias parameter [47].
Unlike position bias, group bias cannot be measured by interven-
ing in the ranked list of items. The reason is that the bias attribute
in position bias can be changed without modifying the content of
the items: Each item can be shown in different positions, hence,
detaching propensity from relevance. In contrast, for group bias,
the bias attribute, i.e., group membership, is a characteristic of the
item that cannot be changed. As such, users’ interactions with items
3Usually𝑎∗=0.5is the least controversial threshold.cannot be measured for different values of the bias attribute.
Instead, to measure group bias, previous work on implicit bias
(with the same problem formulation as Eq. (2)), assumes that the
utility scores of different groups come from the same distribution [ 8,
13,23]. Here, we use the same assumption, but extend it to an
amortized criterion.
5.1 Measurement
LetAAandANbe the set of (observed) attractiveness scores, and
RAandRNbe the set of (latent) relevance scores of 𝐺Aand𝐺N,
respectively. Let ΔDbe a non-parametric test for the equality of
one-dimensional probability distributions such as the Kolmogorov-
Smirnov ( KS) [32] test. The assumption that the utility scores of
the two groups come from the same distribution means that:
lim
|RA|,|RN|→∞ΔD(RA,RN)=0. (15)
Assuming Eq. (2) to be the relation between AAandRA, the best
estimation of 𝛽Ais given by the following optimization problem:
ˆ𝛽A=argmin
𝛽AΔDAA
𝛽A,AN
, (16)
where AA/𝛽Ais the set obtained by dividing all the scores in AA
by𝛽A. In our experiments, we choose the KStest for ΔDand use
grid search to solve the one-dimensional optimization of Eq. (16).
It only remains to define how AAandANsets should be con-
structed. Naively constructing these sets per query has two issues:
(i)Sparsity : Usually, we do not have a large number of items with
non-zero exposure, associated with one query in real-world search
engines. On the other hand, statistical tests measuring the distance
between probability distributions work best with large numbers
of data points. This means that considering the items of one query
in Eq. (16) may not lead to reliable solutions due to high variance.
(ii)Equality-instead-of-equity : Assuming the same distribution for
the utility of different groups can make the notion of equity mean-
ingless, as the implicit assumption in merit-based fairness metrics
is that different groups may not necessarily have the same utility.
Correcting the utility estimations in such a way that the utility
scores of different groups are forced to have close distributions
makes the target exposure of different groups almost equal.
Remark 3. Our assumption that the utility scores come from the
same distribution comes from the principle of maximum entropy:
unless there are explicit and justified reasons indicating that dif-
ferent groups have different utility score distributions, it is only
reasonable to assume the same distribution. Prior work on implicit
bias [8, 13, 23] is based on this same assumption.
5.2 Amortized Correction
Instead of using Eq. (16) per-query, in the amortized correction
method, we first aggregate the items across queries with (almost)
the same group propensity. The sets AAandANcontain the at-
tractiveness scores of these aggregated items. This aggregation
addresses both issues mentioned in Section 5.1: (i) With multiple
queries, the size of the sets AAandANgrows, reducing the vari-
ance. (ii) Amortized equality does not force per-query equality. As
an example, assume there are two queries with the same group
propensity 0.8, each with two items from different groups. TheConference’17, July 2017, Washington, DC, USA Ali Vardasbi, Maarten de Rijke, Fernando Diaz, and Mostafa Dehghani
relevance probabilities of the items from the affected group and
non-affected group are 0.5and1for the first query, and 1and0.5
for the second, respectively. In the per-query approach, two dif-
ferent propensities are inferred for the two queries to make the
corrected utilities of the two groups equal in each query, leading to
equality of exposure. However, in the amortized approach, we have
AA={0.4,0.8}andAN={1,0.5}. Solving Eq. (16) gives us a single
value for𝛽. In this case, groups will have different utility scores after
correction, and equity of exposure has not been replaced by equality.
The amortized correction, however, introduces a new challenge:
How to detect queries with almost the same group propensity,
before measuring their group propensity? One way to break this
cyclic dependency is by using extra knowledge. Notice that in order
to detect queries with almost the same group propensity, it is only
required to have a clustering of queries. Previous work shows that
such a clustering exists for a number of group attributes such as
gender [ 24]. In this paper, we assume that there exists a given
clustering of queries into clusters with almost the same group
propensity. After confirming the effectiveness of our amortized
correction method, we further show in our experiments that even
loosely clustering the queries when an accurate and more specific
clustering is not available, improves the ranking quality and fairness
metrics over the naive case of not correcting for group bias.
5.3 Upshot
We relied on prior studies for the existence of group bias in user
interactions and provided theoretical results about its impact on
the ranking and merit-based fairness metrics. Then, we proposed
an amortized correction method for group bias. Next, we test our
theoretical findings and arguments experimentally.
6 EXPERIMENTAL RESULTS
We investigate the following questions regarding group bias in our
experiments: (i) Is the impact of group bias on degrading the rank-
ing quality and fairness metrics consistent for different sensitive
attributes and in different datasets? (ii) Can our correction method
effectively correct for group bias? (iii) How does the amortized ap-
proach compare to the per-query approach for correction? (iv) How
robust is our correction method to the accuracy of clustering the
queries based on their group propensity?
6.1 Setup
Dataset. We use four datasets with provided sensitive attributes
and two with synthesized sensitive attributes. (i) IIT-JEE : The
dataset comprises the scores of candidates who took the Indian
Institutes of Technology Joint Entrance Exam (IIT-JEE) in 2009.
This information was made public in June 2009, following a Right
to Information request [ 26]. It contains the scores of about 385𝑘
students, the student’s gender ( 98𝑘women and 287𝑘men), their
birth category (see [ 3]), and zip code. This dataset was used in
prior work on implicit bias [e.g., 8]. We normalize the scores to
the[0,1]interval using min-max normalization. Furthermore, we
simulate queries by grouping the students based on their birth
category and zip code. This gives 48.6𝑘queries, among which we
only keep the ones with both genders and at least one normalized
score above 0.5and one below 0.5. The filtering gives us 2.9𝑘queries
Yahoo! MSLR
𝐺A/𝐺N
Population Relevance0.51.01.52.02.5
Figure 3: Ratio of affected to non-affected group members in
terms of population and average utility score (relevance) for
different sensitive attributes in Yahoo! and MSLR datasets.
with a total of 205𝑘scores. (ii–iii) TREC 2019 and 2020 : The
academic search dataset provided by the TREC Fair Ranking track
2019 and 2020 [ 5]. These datasets come with 632and200train
queries, respectively, with an average of 6.7and23.5documents
per query. Following [ 42,49], we divide the items (i.e., papers) into
two groups based on their authors’ h-index. (iv) MovieLens 1𝑀:
The classic movie recommendation dataset comprising 1𝑀movie
ratings that were provided by 6𝑘users for 3.9𝑘different movies.
We scraped IMDB to obtain the country of origin and box office
cumulative worldwide gross values for each item (i.e. movie). For
the sensitive attributes, we consider two groupings as follows. In
MovieLens[𝐶𝑜.], we divide the movies based on their first listed
country of origin into United States (US) and non-US groups with
2.7𝑘and1.2𝑘movies and 807𝑘and193𝑘ratings, respectively. In
MovieLens[𝐵𝑂], we divide the movies based on their box office
with a threshold of 100𝑀$ into high and low-grossing groups with
388and3.5𝑘movies and 324𝑘and676𝑘ratings, respectively.
LTR dataset. To analyze the impact of group bias in the general
LTR regime, following prior work on unbiased LTR research [ 17,19,
47,48], we use two well-known LTR datasets: Yahoo! Webscope [ 9]
and MSLR-WEB30k [ 38]. The Yahoo! and MSLR datasets are repre-
sented by query-document feature vectors of lengths 501 and 131,
respectively, and both have graded relevance labels from 0to4. For
our experiments on head queries and a tabular regime, we use the
training set of the Yahoo! and MSLR datasets, with 20k queries and
473k documents and 19k queries and 2.2M documents, respectively.
The Yahoo! dataset contains 6.7k test queries and 163k test docu-
ments; MSLR contains 6k and 749k queries and documents in its
test set. Test queries are considered tail queries in our experiments
and we analyze the impact of group bias as well as the effectiveness
of our correction method on the LTR outputs over these queries.
Sensitive attribute for LTR datasets. We extend prior work [ 11,
49, 53] and utilize a data-driven approach for selecting features as
sensitive attributes and dividing items into two groups based on
some threshold on that feature. Our criterion for selecting a feature
as a sensitive attribute is as follows: For each feature we divide
the items in two groups based on a threshold equal to the mean
minus one standard deviation of that feature. If more than 95%
of queries have at least one item from both groups, we select the
feature as a candidate for sensitive attribute. Based on this crite-
rion, we have selected features [5,88,100,141,155,264,393,426]
and[11,14,15,126,127,130,131,132]from the Yahoo! and MSLRGroup Membership Bias Conference’17, July 2017, Washington, DC, USA
Overall Affected group Non-affected group
Tabular (training labels) LTR outputNDCG@10 change
0.8 0.60.00.51.01.5
0.8 0.60.00.51.01.5
Group propensity Group propensity
Figure 4: The impact of group bias on ranking quality for the
Yahoo! and MSLR datasets with different sensitive attributes.
Tabular (train labels) LTR output
DTR↑(ideal: 1) EEL ↓(ideal: 0)𝜌(DTR)
0.8 0.60.50.60.70.80.91.0
Δ(EEL)
0.8 0.60246
Group propensity Group propensity
Figure 5: The impact of group bias on fairness metrics for the
Yahoo! and MSLR datasets with different sensitive attributes.
datasets.4Fig. 3 gives an overview of the ratio of affected to non-
affected group members in terms of population and average util-
ity score. In what follows we use, e.g., Yahoo![426]for the Yahoo!
dataset with feature number 426as the sensitive feature. For each
feature, we assume two groupings based on two thresholds: (i) mean
value; and (ii) mean minus one standard deviation. This gives us a
total of 32different setups.
Bias simulation. To simulate group bias, we use Eq. (2), but
to make the simulation more realistic, we add a normal noise to
the𝛽Avalue for each query. We experiment with two propensities
𝛽A∈{0.6,0.8}and use𝜎𝛽=0.1for the standard deviation of the
normal noise. In Sec. 6.4.2, to add to the uncertainty of the setup,
we also experiment with higher noise variances of 𝜎𝛽∈{0.2,0.3}.
For our correction method, we found that adding a amount of small
noise to the scores for breaking the ties, without swapping the order
of the grades, helps to have a smoother curve for 𝛽in Eq. (16).
LTR model. For the general LTR model (for tail queries) we use
a neural network with attention and LambdaRank Loss as in [37].
6.2 Impact of Group Bias
First, we show that group bias, on both tabular and LTR regimes,
consistently has a negative impact on the ranking quality and fair-
ness metrics. To do so, we run experiments on two datasets, namely
Yahoo! and MSLR, each with 8different features as the sensitive
attribute, and two different thresholds for separating the groups
(see the “Sensitive attribute” paragraph on Section 6.1). This gives
us a total of 32different setups. For each setup, we simulate the
attractiveness probabilities with 𝛽A∈ {0.8,0.6}5and compare
NDCG@10, DTR and EEL metrics against the full information case.
4Feature numbers start from 1.
5We report results for these two values as mild andsevere cases of group bias. Our
experiments with other values led to consistent results.Fig. 4 shows a summary of the impact of group bias on the
ranking performance of both tabular and LTR regimes. These results
show that the observation of Fig. 1 on one specific sensitive attribute
is consistent across different datasets and other sensitive attributes:
Group bias degrades the ranking quality of the affected group in the
tabular regime and this damage is also reflected in the LTR output,
trained over the biased training labels. As a result of pushing down
the relevant members of the affected group, the non-affected group
gains ranking quality, i.e., the NDCG of the non-affected group
with group bias is higher than the full information case. However,
the overall ranking is worsened with group bias. Comparing the
tabular (left) and LTR output (right) plots in Fig. 4, we observe that
increasing the severity of group bias from 0.8to0.6, affects the
tabular regime more. This may be because unlike on the tabular
regime, the impact of group bias on the LTR outputs is indirect.
Fig. 5 shows a summary of the change in fairness metrics of both
tabular and LTR regimes as a result of group bias. For example, a
value of𝜌(DTR)=0.5in the left plot means that on average, the
target (i.e., ideal) exposure computed by the biased attractiveness
scores differs from the true target exposure computed in the full
information case by a factor of 0.5. Similarly, a value of Δ(EEL)=3
in the right plot means that on average, the target exposure of the
biased case has an ℓ2distance of 3to the full information target
exposure. These results show that the observation of Fig. 2 on one
specific sensitive attribute is consistent across different datasets and
other sensitive attributes: Group bias changes the target exposure
in the tabular regime and this change is reflected in the LTR output,
trained over the biased training labels. Consequently, when the
system distributes the exposure according to the target exposure
to make a ranking fair, if the scores are suffering from group bias,
the result is not truly fair.
6.3 Amortized Correction
So far, our theoretical results in Section 4 and empirical results in
Section 6.2 confirm the negative impact of group bias on ranking
and fairness metrics. To correct for this bias, we have proposed
an amortized correction method (Section 5). In the next set of ex-
periments, we show the effectiveness of our proposed correction
method in compensating for the negative effect of group bias.
Table 1 compares the ranking quality, in terms of NDCG@10, as
well as two merit-based fairness metrics, DTR and EEL, between the
biased and corrected cases in the tabular regime. In all datasets and
both bias parameter values, our correction method improves the
ranking quality and fairness metrics over the biased case. With some
exceptions for the ranking quality with mild group bias ( 𝛽=0.8),
the improvements are significant. For each bias parameter value,
we have also included the estimated value obtained from Eq. (16).
We observe that our estimated bias values, i.e. ˆ𝛽A, are close to their
corresponding true (but unknown during the correction) values 𝛽.
We further analyze the effectiveness of our correction method in
the general LTR regime. Table 2 contains the comparison of ranking
quality and fairness metrics between the biased and corrected cases
in our tested LTR datasets, i.e., Yahoo! and MSLR. We also report the
full information case in the table. Here, we only report the results for
one sensitive attribute for each dataset, noting that the results for
other sensitive attributes lead to similar observations. Similar to theConference’17, July 2017, Washington, DC, USA Ali Vardasbi, Maarten de Rijke, Fernando Diaz, and Mostafa Dehghani
Table 1: The impact of our amortized group bias correction on ranking and fairness metrics in the tabular regime. ˆ𝛽Ashows the
estimated value of the bias parameter as in Eq. (16). For each metric, the columns “ B” and “C” show the “ Biased ” and “Corrected”
performances, respectively. Superscripts∗indicate a significant improvement over the biased case with 𝑝<0.001.
𝛽=0.8 𝛽=0.6
ˆ𝛽ANDCG @10↑𝜌(DTR)↑ Δ(EEL)↓ˆ𝛽ANDCG @10↑𝜌(DTR)↑ Δ(EEL)↓
B C B C B C B C B C B C
Yahoo![426] 0.825 0.987 0.996∗0.820 0.955∗0.447 0.120∗0.626 0.885 0.988∗0.641 0.941∗1.809 0.172∗
MSLR[127] 0.843 0.975 0.991∗0.813 0.948∗1.687 0.308∗0.648 0.780 0.966∗0.627 0.926∗7.146 0.471∗
IIT-JEE 0.727 0.989 0.991 0.799 0.906∗0.504 0.341∗0.547 0.970 0.985∗0.600 0.901∗1.401 0.410∗
MovieLens[𝐶𝑜.]0.822 1.000 1.000 0.800 0.962∗1.101 0.513∗0.612 0.998 1.000∗0.602 0.958∗7.113 1.908∗
MovieLens[𝐵𝑂]0.781 1.000 1.000 0.799 0.974∗2.330 0.895∗0.579 0.994 1.000∗0.600 0.961∗24.800 2.831∗
TREC 2019 0.838 0.997 1.000 0.888 0.954∗0.041 0.020∗0.634 0.986 0.999∗0.771 0.937∗0.129 0.028∗
TREC 2020 0.821 0.995 0.999 0.815 0.954∗0.356 0.114∗0.614 0.945 0.995∗0.627 0.941∗1.152 0.137∗
Table 2: The impact of our amortized group bias correction on ranking and fairness metrics in general LTR regime. For each
metric, the columns “ B” and “C” show the “ Biased ” and “Corrected” performances, respectively. Superscripts∗indicate a
significant improvement over the biased case with 𝑝<0.001.
Full info. 𝛽=0.8 𝛽=0.6
NDCG𝜌(DTR)
Δ(EEL) NDCG @10↑𝜌(DTR)↑ Δ(EEL)↓ NDCG @10↑𝜌(DTR)↑ Δ(EEL)↓
B C B C B C B C B C B C
Yahoo![426]0.649 0.477 0.673 0.615 0.645∗0.319 0.428∗1.970 0.741∗0.580 0.645∗0.313 0.456∗2.711 0.728∗
MSLR[127]0.320 0.681 1.779 0.283 0.313∗0.675 0.684 4.380 2.016∗0.265 0.309∗0.671 0.683 5.803 2.147∗
tabular regime, here we also observe performance improvements
as a result of our correction method, compared to the biased case.
Except for the DTR metric in MSLR, all the improvements are
significant with 𝑝<0.001. Compared with the full information
case, we observe that in the Yahoo! dataset, our correction method
leads to full recovery of NDCG@10, while in the MSLR dataset, there
remains a slight gap toward the full information quality. One reason
for this difference could be the distribution of relevant items in the
affected and non-affected groups: In Yahoo![426]the ratio between
the mean relevance of items in 𝐺Ato𝐺Nis1.05, whereas the
same quantity in MSLR[127]is2.21. Therefore, the assumption of
similar utility score distributions for both groups is closer to reality
inYahoo![426]than in MSLR[127]. Similarly to NDCG, we observe
that DTR and EEL are almost fully recovered from group bias in
the Yahoo! dataset, whereas in the MSLR dataset, there remains a
larger gap toward the full information case after correction.
Finally, Fig. 6 shows a summary of the ranking quality of biased
(left) and corrected (right) utility scores in the tabular regime on all
32setups of the LTR datasets mentioned in Sec. 6.2. In all but two
cases, we observe that our correction method effectively improves
the ranking quality over the biased case and achieves NDCG@10
close to 1. The two outlier cases correspond to Yahoo![5],6where
the ratio between the average utility of the affected group and the
non-affected group is as low as 0.3. This is the same outlier as in
Fig. 3. As our correction method is based on the same distribution
assumption, this severe violation leads to inferred propensities that
are noticeably lower than the actual propensity (i.e., 0.49and0.36
instead of 0.8and0.6). It is worth mentioning that in a slightly less
severe violation of the same distribution assumption, i.e., MSLR[130]
6For each feature, two different thresholds for separating the groups are used.
Overall Affected group Non-affected groupBiased CorrectedNDCG@10
0.8 0.60.40.60.81.0
0.8 0.60.40.60.81.0
Group propensity Group propensity
Figure 6: The ranking performance of biased (left) and cor-
rected (right) scores for the Yahoo! and MSLR datasets with
different sensitive attributes.
with a utility ratio of 0.45, our correction method is able to improve
the ranking quality over the biased case. One interesting future
direction would be to find out if this phenomenon, i.e., having the
true average utility of the underrepresented group considerably
lower than the other group, happens in real-world settings and how
to correct for the bias in such cases.
6.4 Ablation Study
6.4.1 Impact of Cluster Size on Correction. In Section 5.2 we argued
against measuring the group propensity for each query. Here, we
analyze the impact of cluster size on the correction method. We start
from the extreme case of one query per cluster and increase the clus-
ter size until the ranking quality converges. Fig. 7 shows the ranking
quality of the corrected scores as a function of the cluster size. The
overall ranking quality (red line) improves as the cluster size grows
and it converges to its final value at around a cluster size of 10.
For the severe group bias ( 𝛽A=0.6), which we omit due to space
restrictions, the same pattern is observed, but with a convergenceGroup Membership Bias Conference’17, July 2017, Washington, DC, USA
Overall
Affected groupNon-affected group
Biased (not corrected)NDCG@10
1 10 100 10000.750.800.850.900.951.00
1 10 100 10000.50.60.70.80.9
Cluster size Cluster size
a) Ranking quality b) Inferred 𝛽values
Figure 7: The impact of cluster size of group propensity on the
amortized correction for group bias ( 𝛽A=0.8) on ranking
performance for the Yahoo![426]dataset. Ranking quality
of corrected scores (a), and accuracy of the inferred group
propensity (b).
point of 100. In both cases, using a cluster size below the conver-
gence point leads to corrected rankings that are even worse than
the biased ranking. Comparing the ranking quality of the affected
group (black line) with the non-affected group (golden line), we
observe that smaller clusters result in over-compensation of group
bias. The reason is revealed in Fig. 7(b): for smaller cluster sizes, the
inferred propensity is under-estimated, leading to larger corrected
scores for the affected group members. Consequently, the scores of
the affected group are boosted more than they really should. One
other interesting observation in Fig. 7(b) is the high variance of the
inferred propensity for small clusters (issue (i) in Section 5.2).
6.4.2 Impact of Clustering Accuracy. Finally, we address the chal-
lenge of inaccurate clustering of queries based on their group
propensity that we raised at the end of Section 5.2. The main goal
of the following sets of experiments is to show that our correction
method, even when accurate clustering of queries is not available,
is still effective in improving the ranking quality over the biased
case. To confirm this, we add to the uncertainty of our simulation
setup in two different ways: (i) Higher variance : We increase the
variance of the group propensity when simulating the attractive-
ness probabilities. We consider 𝜎𝛽∈{0.2,0.3}. With the increased
variance, the group propensity of queries can go far from the mean
value, and, as we correct the queries with a single inferred value
for the propensity, the probability of a mismatch between the ac-
tual propensity and the propensity used for correction increases.
(ii)Two modes : Instead of using a unimodal normal distribution
to simulate group propensity, we use a mixture model with two
modes{0.6,0.8}. This means that for half of the queries, the group
propensity follows a normal distribution with a mean of 0.6while
for the other half, the normal distribution has a mean of 0.8and
during inference, we are not given the information about which
query belongs to which mode.
Fig. 8 shows the ranking quality of the corrected scores with
respect to different cluster sizes in the increased uncertainty setups
described above. In both plots, we observe that increasing the vari-
ance of the simulated group propensity both increases the negative
impact of group bias on ranking (dotted lines) and makes it harder
to correct for the bias (solid lines). The important result of these
experiments, however, is that even though the uncertainty about
group propensity is high, our amortized correction method almost
always improves the ranking quality over the biased case. Note
σβ=0.1, corrected
σβ=0.2, correctedσβ=0.3, corrected
Biased (not corrected)𝜇𝛽=0.8 𝜇𝛽∈{0.6,0.8}NDCG@10
1 10 100 10000.850.900.951.00
1 10 100 10000.850.900.951.00
Cluster size Cluster size
Figure 8: Effectiveness of our amortized correction method
when accurate clustering based on group propensity is not
available. Yahoo! [426]dataset.
that in all setups, per-query correction as well as clusters with a
small size lead to worse ranking qualities than the biased scores.
Interestingly, when there are two modes of group propensity (right
plot), our correction method, oblivious to the mode membership
and assuming a fixed propensity, is able to correct the scores and
achieve a ranking performance higher than the biased case.
7 CONCLUSION AND FUTURE WORK
In this paper, we have addressed group membership bias, which is
based on the observation that a user’s perception of an item’s group
membership may affect their judgment about the utility of an item.
We have provided extensive theoretical and empirical analyses of
the impact of group bias on the ranking quality and two fairness of
exposure metrics, DTR and EEL. By utilizing an auxiliary variable
𝜈as the fraction of affected relevant items that are still as attractive
as the non-affected relevant items, we have shown that, in the
presence of group bias, NDCG and DTR change linearly with 𝜈,
while the change in EEL has a more complex form in terms of 𝜈.
Correcting for group bias, which is a type of content-based bias,
is not as easy as context-based types of bias such as position and
trust bias. To measure group bias, assumptions based on fairness
constraints should be made about the utility distribution of differ-
ent groups. However, such assumptions can potentially make the
equity-based notion of fairness meaningless. Amortized correction
for the group bias is our solution to this issue, as global equality
does not contradict local equity. We have experimentally confirmed
that our correction method, when its assumptions are met, is able
to fully recover the scores suffering from group bias, in the sense
that the ranking and fairness metrics after correction achieve the
values of the full information case.
Our amortized correction, however, raises a challenge of its own,
as it is not easy to cluster queries with almost the same group
propensity without the knowledge of group propensity. Our experi-
ments have shown that even when an accurate clustering of queries
is not available, loosely clustering the queries for the amortized
correction still leads to better rankings compared to the biased
scores. More interestingly, per-query correction as well as clusters
of small size lead to worse ranking qualities than the biased case.
There are several future directions to this study. Here, we an-
alyzed a multiplicative model of group bias, and our theoretical
and empirical results are based on this formulation. One way to
extend our results is to consider more complex models for group
bias. Another possible future direction is to propose measurementConference’17, July 2017, Washington, DC, USA Ali Vardasbi, Maarten de Rijke, Fernando Diaz, and Mostafa Dehghani
and correction methods that perform better with increased uncer-
tainty of group propensity. Moreover, as group bias is based on
users’ perception of group membership, it can change over time.
Analyzing group bias in a dynamic setting is therefore another
interesting future direction. This study deals with the impact of
group bias on fair exposure and, hence, we only consider the so-
called treatment-based fairness metrics. In contrast, some studies
focus on impact-based fairness [ 41], where the objective is to make
sure that items receive a fair amount of impact, e.g., clicks. While
our work suggests a way to correct for group bias in the historical
clicks in order to make the exposure in future rankings fair, a next
direction would be to account for group bias when optimizing for
impact-based fairness.
CODE AND DATA
To ensure the reproducibility of the reported results, this work
only made use of publicly available data and our experimental
implementation can be accessed publicly at https://github.com/
AliVard/groupbias.
ACKNOWLEDGMENTS
This research was supported by Elsevier and the Netherlands Or-
ganisation for Scientific Research (NWO) under project nr 612.-
001.551, and by the Hybrid Intelligence Center, a 10-year pro-
gram funded by the Dutch Ministry of Education, Culture and
Science through the Netherlands Organisation for Scientific Re-
search, https://hybrid-intelligence-centre.nl.
All content represents the opinion of the authors, which is not
necessarily shared or endorsed by their respective employers and/or
sponsors.Group Membership Bias Conference’17, July 2017, Washington, DC, USA
REFERENCES
[1]Aman Agarwal, Xuanhui Wang, Cheng Li, Michael Bendersky, and Marc Najork.
2019. Addressing Trust Bias for Unbiased Learning-to-Rank. In The World Wide
Web Conference . ACM, 4–14.
[2]Qingyao Ai, Keping Bi, Cheng Luo, Jiafeng Guo, and W Bruce Croft. 2018. Unbi-
ased Learning to Rank with Unbiased Propensity Estimation. In The 41st Interna-
tional ACM SIGIR Conference on Research & Development in Information Retrieval .
ACM, 385–394.
[3]Surender Baswana, Partha Pratim Chakrabarti, Sharat Chandran, Yashodhan
Kanoria, and Utkarsh Patange. 2019. Centralized Admissions for Engineering
Colleges in India. In Proceedings of the 2019 ACM Conference on Economics and
Computation . 323–324.
[4]Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sergey Feldman, and Sebastian
Kohlmeier. 2021. Overview of the TREC 2020 Fair Ranking Track. In The Twenty-
Ninth Text REtrieval Conference Proceedings (TREC 2020) .
[5]Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, and Sebastian Kohlmeier.
2019. Overview of the TREC 2019 Fair Ranking Track. In The Twenty-Eighth Text
REtrieval Conference (TREC 2019) Proceedings .
[6]Asia J. Biega, Krishna P. Gummadi, and Gerhard Weikum. 2018. Equity of
Attention: Amortizing Individual Fairness in Rankings. In The 41st International
ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR
’18). Association for Computing Machinery, New York, NY, USA, 405–414. https:
//doi.org/10.1145/3209978.3210063
[7]Michael Brownstein. 2017. Implicit Bias. In Stanford Encyclopedia of Philosophy ,
Edward Zalta (Ed.).
[8]L. Elisa Celis, Anay Mehrotra, and Nisheeth K. Vishnoi. 2020. Interventions for
Ranking in the Presence of Implicit Bias. In Proceedings of the 2020 Conference on
Fairness, Accountability, and Transparency (FAT* ’20) . Association for Comput-
ing Machinery, New York, NY, USA, 369–380. https://doi.org/10.1145/3351095.
3372858
[9]Olivier Chapelle and Yi Chang. 2011. Yahoo! Learning to Rank Challenge
Overview. In Proceedings of the Learning to Rank Challenge (Proceedings of Ma-
chine Learning Research) , Vol. 14. PMLR, 1–24. https://proceedings.mlr.press/
v14/chapelle11a.html
[10] Aleksandr Chuklin, Ilya Markov, and Maarten de Rijke. 2015. Click Models for
Web Search . Morgan & Claypool Publishers.
[11] Fernando Diaz, Bhaskar Mitra, Michael D. Ekstrand, Asia J. Biega, and Ben
Carterette. 2020. Evaluating Stochastic Rankings with Expected Exposure. In
Proceedings of the 29th ACM International Conference on Information & Knowledge
Management (CIKM ’20) . Association for Computing Machinery, New York, NY,
USA, 275–284. https://doi.org/10.1145/3340531.3411962
[12] Michael D. Ekstrand, Graham McDonald, Amifa Raj, and Isaac Johnson. 2022.
Overview of the TREC 2021 Fair Ranking Track. In The Thirtieth Text REtrieval
Conference (TREC 2021) Proceedings .
[13] Vitalii Emelianov, Nicolas Gast, Krishna P. Gummadi, and Patrick Loiseau. 2020.
On Fair Selection in the Presence of Implicit Variance. In Proceedings of the
21st ACM Conference on Economics and Computation (EC ’20) . Association for
Computing Machinery, New York, NY, USA, 649–675. https://doi.org/10.1145/
3391403.3399482
[14] Alessandro Fabris, Alberto Purpura, Gianmaria Silvello, and Gian Antonio Susto.
2020. Gender Stereotype Reinforcement: Measuring the Gender Bias Conveyed by
Ranking Algorithms. Information Processing & Management 57, 6 (2020), 102377.
[15] Chloë FitzGerald and Samia Hurst. 2017. Implicit Bias in Healthcare Professionals:
A Systematic Review. BMC medical ethics 18, 1 (2017), 1–18. https://bmcmedethics.
biomedcentral.com/articles/10.1186/s12910-017-0179-8
[16] Sara Hajian, Francesco Bonchi, and Carlos Castillo. 2016. Algorithmic Bias: From
Discrimination Discovery to Fairness-Aware Data Mining. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD ’16) . Association for Computing Machinery, New York, NY, USA,
2125–2126. https://doi.org/10.1145/2939672.2945386
[17] Rolf Jagerman, Harrie Oosterhuis, and Maarten de Rijke. 2019. To Model or to
Intervene: A Comparison of Counterfactual and Online Learning to Rank from
User Interactions. In Proceedings of the 42nd International ACM SIGIR Conference
on Research & Development in Information Retrieval . ACM, 15–24.
[18] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay.
2005. Accurately Interpreting Clickthrough Data as Implicit Feedback. In Pro-
ceedings of the 28th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval . ACM, 154–161.
[19] Thorsten Joachims, Adith Swaminathan, and Tobias Schnabel. 2017. Unbiased
Learning-to-Rank with Biased Feedback. In Proceedings of the Tenth ACM Inter-
national Conference on Web Search and Data Mining . ACM, 781–789.
[20] Christine Jolls and Cass R Sunstein. 2006. The Law of Implicit Bias. Calif. L. Rev.
94 (2006), 969.
[21] Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, and Zheng Wen. 2016.
DCM Bandits: Learning to Rank with Multiple Clicks. In International Conference
on Machine Learning . 1215–1224.
[22] Matthew Kay, Cynthia Matuszek, and Sean A. Munson. 2015. Unequal Repre-
sentation and Gender Stereotypes in Image Search Results for Occupations. InProceedings of the 33rd Annual ACM Conference on Human Factors in Computing
Systems (CHI ’15) . Association for Computing Machinery, New York, NY, USA,
3819–3828. https://doi.org/10.1145/2702123.2702520
[23] Jon Kleinberg and Manish Raghavan. 2018. Selection Problems in the Presence
of Implicit Bias. In 9th Innovations in Theoretical Computer Science Conference
(ITCS 2018) . Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
[24] Klara Krieg, Emilia Parada-Cabaleiro, Gertraud Medicus, Oleg Lesota, Markus
Schedl, and Navid Rekabsaz. 2022. Grep-BiasIR: A Dataset for Investigating
Gender Representation-Bias in Information Retrieval Results. arXiv preprint
arXiv:2201.07754 (2022).
[25] Klara Krieg, Emilia Parada-Cabaleiro, Markus Schedl, and Navid Rekabsaz. 2022.
Do Perceived Gender Biases in Retrieval Results Affect Relevance Judgements?
arXiv preprint arXiv:2203.01731 (2022).
[26] Mr. Rajeev Kumar. 2009. Rti complaint. Decision No. CIC/SG/C/2009/001088/5392,
Complaint No. CIC/SG/C/2009/001088.
[27] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. 2015. Cascad-
ing Bandits: Learning to Rank in the Cascade Model. In International Conference
on Machine Learning . 767–776.
[28] Paul Lagrée, Claire Vernade, and Olivier Cappé. 2016. Multiple-play Bandits in
the Position-based Model. In Advances in Neural Information Processing Systems .
1597–1605.
[29] Tor Lattimore and Csaba Szepesvári. 2020. Bandit Algorithms . Cambridge Uni-
versity Press.
[30] Chang Li, Branislav Kveton, Tor Lattimore, Ilya Markov, Maarten de Rijke, Csaba
Szepesvári, and Masrour Zoghi. 2020. BubbleRank: Safe Online Learning to Re-
rank via Implicit Click Feedback. In Uncertainty in Artificial Intelligence . PMLR,
196–206.
[31] Karen S. Lyness and Madeline E. Heilman. 2006. When Fit is Fundamental: Per-
formance Evaluations and Promotions of Upper-level Female and Male Managers.
Journal of Applied Psychology 91, 4 (2006), 777. https://doi.org/10.1037/0021-
9010.91.4.777
[32] Frank J Massey Jr. 1951. The Kolmogorov-Smirnov Test for Goodness of Fit. J.
Amer. Statist. Assoc. 46, 253 (1951), 68–78.
[33] Anay Mehrotra, Bary S. R. Pradelski, and Nisheeth K. Vishnoi. 2022. Selection in
the Presence of Implicit Bias: The Advantage of Intersectional Constraints. In
2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22) .
Association for Computing Machinery, New York, NY, USA, 599–609. https:
//doi.org/10.1145/3531146.3533124
[34] Pascal Molenberghs. 2013. The Neuroscience of In-group Bias. Neuroscience &
Biobehavioral Reviews 37, 8 (2013), 1530–1536. https://doi.org/10.1016/j.neubiorev.
2013.06.002
[35] Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. 2020. Con-
trolling Fairness and Bias in Dynamic Learning-to-Rank. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR ’20) . Association for Computing Machinery, New York, NY, USA,
429–438. https://doi.org/10.1145/3397271.3401100
[36] Harrie Oosterhuis and Maarten de de Rijke. 2021. Robust Generalization and Safe
Query-Specializationin Counterfactual Learning to Rank. In Proceedings of the
Web Conference 2021 (WWW ’21) . Association for Computing Machinery, New
York, NY, USA, 158–170. https://doi.org/10.1145/3442381.3450018
[37] Przemyslaw Pobrotyn, Tomasz Bartczak, Mikolaj Synowiec, Radoslaw Bialo-
brzeski, and Jaroslaw Bojar. 2020. Context-Aware Learning to Rank with Self-
Attention. arXiv preprint arXiv:2005.10084 (2020).
[38] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets. arXiv preprint
arXiv:1306.2597 (2013).
[39] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. 2008. Learning Diverse
Rankings with Multi-armed Bandits. In Proceedings of the 25th International
Conference on Machine Learning . 784–791.
[40] Amifa Raj and Michael D. Ekstrand. 2022. Measuring Fairness in Ranked Results:
An Analytical and Empirical Comparison. In Proceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR ’22) . Association for Computing Machinery, New York, NY, USA, 726–736.
https://doi.org/10.1145/3477495.3532018
[41] Yuta Saito and Thorsten Joachims. 2022. Fair Ranking as Fair Division: Impact-
Based Individual Fairness in Ranking. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’22) . Association for
Computing Machinery, New York, NY, USA, 1514–1524. https://doi.org/10.1145/
3534678.3539353
[42] Fatemeh Sarvi, Maria Heuss, Mohammad Aliannejadi, Sebastian Schelter, and
Maarten de Rijke. 2021. Understanding and Mitigating the Effect of Outliers in
Fair Ranking. arXiv preprint arXiv:2112.11251 (2021).
[43] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of Exposure in Rankings.
InProceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining (KDD ’18) . Association for Computing Machinery, New
York, NY, USA, 2219–2228. https://doi.org/10.1145/3219819.3220088
[44] Ashudeep Singh and Thorsten Joachims. 2019. Policy Learning for Fairness
in Ranking. In Advances in Neural Information Processing Systems , H. Wallach,
H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32.Conference’17, July 2017, Washington, DC, USA Ali Vardasbi, Maarten de Rijke, Fernando Diaz, and Mostafa Dehghani
Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2019/
file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf
[45] Tom Sühr, Sophie Hilgard, and Himabindu Lakkaraju. 2021. Does Fair Ranking
Improve Minority Outcomes? Understanding the Interplay of Human and Algo-
rithmic Biases in Online Hiring (AIES ’21) . Association for Computing Machinery,
New York, NY, USA, 989–999. https://doi.org/10.1145/3461702.3462602
[46] Ali Vardasbi, Maarten de Rijke, and Ilya Markov. 2020. Cascade Model-Based
Propensity Estimation for Counterfactual Learning to Rank. In Proceedings of
the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR ’20) . Association for Computing Machinery, New
York, NY, USA, 2089–2092. https://doi.org/10.1145/3397271.3401299
[47] Ali Vardasbi, Maarten de Rijke, and Ilya Markov. 2021. Mixture-Based Correction
for Position and Trust Bias in Counterfactual Learning to Rank. In Proceedings
of the 30th ACM International Conference on Information & Knowledge Manage-
ment (CIKM ’21) . Association for Computing Machinery, New York, NY, USA,
1869–1878. https://doi.org/10.1145/3459637.3482275
[48] Ali Vardasbi, Harrie Oosterhuis, and Maarten de Rijke. 2020. When Inverse
Propensity Scoring Does Not Work: Affine Corrections for Unbiased Learning to
Rank. In Proceedings of the 29th ACM International Conference on Information &
Knowledge Management (CIKM ’20) . Association for Computing Machinery, New
York, NY, USA, 1475–1484. https://doi.org/10.1145/3340531.3412031
[49] Ali Vardasbi, Fatemeh Sarvi, and Maarten de Rijke. 2022. Probabilistic Permuta-
tion Graph Search: Black-Box Optimization for Fairness in Ranking. In Proceedings
of the 45th International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR ’22) . Association for Computing Machinery, New
York, NY, USA, 715–725. https://doi.org/10.1145/3477495.3532045[50] Madalina Vlasceanu and David M. Amodio. 2022. Propagation of Societal Gender
Inequality by Internet Search Algorithms. Proceedings of the National Academy
of Sciences 119, 29 (2022), e2204529119. https://doi.org/10.1073/pnas.2204529119
arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2204529119
[51] Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc
Najork. 2018. Position Bias Estimation for Unbiased Learning to Rank in Personal
Search. In Proceedings of the Eleventh ACM International Conference on Web Search
and Data Mining . ACM, 610–618.
[52] Haolun Wu, Bhaskar Mitra, Chen Ma, Fernando Diaz, and Xue Liu. 2022. Joint
Multisided Exposure Fairness for Recommendation. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR ’22) . Association for Computing Machinery, New York, NY, USA,
703–714. https://doi.org/10.1145/3477495.3532007
[53] Himank Yadav, Zhengxiao Du, and Thorsten Joachims. 2021. Policy-Gradient
Training of Fair and Unbiased Ranking Functions. In Proceedings of the 44th
International ACM SIGIR Conference on Research and Development in Information
Retrieval . Association for Computing Machinery, New York, NY, USA, 1044–1053.
https://doi.org/10.1145/3404835.3462953
[54] Bozena Zdaniuk and John M. Levine. 2001. Group Loyalty: Impact of Members’
Identification and Contributions. Journal of Experimental Social Psychology 37, 6
(2001), 502–509. https://doi.org/10.1006/jesp.2000.1474
[55] Masrour Zoghi, Tomáš Tunys, Lihong Li, Damien Jose, Junyan Chen, Chun Ming
Chin, and Maarten de Rijke. 2016. Click-based Hot Fixes for Underperforming
Torso Queries. In Proceedings of the 39th International ACM SIGIR conference on
Research and Development in Information Retrieval . 195–204.