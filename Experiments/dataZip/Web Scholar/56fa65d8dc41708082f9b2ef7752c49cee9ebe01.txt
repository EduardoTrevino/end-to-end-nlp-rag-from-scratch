SCOTT: Self-Consistent Chain-of-Thought Distillation
Peifeng Wang1âˆ—, Zhengyang Wang2, Zheng Li2, Yifan Gao2, Bing Yin2, Xiang Ren1
1Department of Computer Science, University of Southern California,2Amazon.com Inc
{peifengw,xiangren}@usc.edu ,
{zhengywa,amzzhe,yifangao,alexbyin}@amazon.com
Abstract
Large language models (LMs) beyond a cer-
tain scale, demonstrate the emergent capability
of generating free-text rationales for their pre-
dictions via chain-of-thought (CoT) prompting.
While CoT can yield dramatically improved
performance, such gains are only observed for
sufficiently large LMs. Even more concerning,
there is little guarantee that the generated ra-
tionales are consistent with LMâ€™s predictions
or faithfully justify the decisions. In this work,
we propose SCOTT, a faithful knowledge distil-
lation method to learn a small, self-consistent
CoT model from a teacher model that is or-
ders of magnitude larger. To form better su-
pervision, we elicit rationales supporting the
gold answers from a large LM (teacher) by con-
trastive decoding, which encourages the teacher
to generate tokens that become more plausi-
ble only when the answer is considered. To
ensure faithful distillation, we use the teacher-
generated rationales to learn a student LM with
a counterfactual reasoning objective, which pre-
vents the student from ignoring the rationales
to make inconsistent predictions. Experiments
show that, while yielding comparable end-task
performance, our method can generate CoT ra-
tionales that are more faithful than baselines
do. Further analysis suggests that such a model
respects the rationales more when making de-
cisions; thus, we can improve its performance
more by refining its rationales.1
1 Introduction
Large language models (LMs) elicit strong reason-
ing capabilities through chain-of-thought (CoT)
prompting (Wei et al., 2022b), which asks LMs
to generate free-text rationale for explaining their
multi-step reasoning. However, CoT prompting
does not guarantee that the rationale is consis-
tent with the prediction, rendering the rationale
âˆ—This work was done when Peifeng Wang was an intern at
Amazon. Zheng Li and Xiang Ren are corresponding authors.
1Code can be found at https://github.com/
wangpf3/consistent-CoT-distillation .
Can a Bengal cat survive eating only pancakes? The answer is no. Why?Is material from an aloe plant sometimes enclosed in petroleum-derived products? The answer is yes. Why?A Bengal cat cannot survive eating only pancakes.Aloe is a plant Plants are made of cells. Cells are made of molecules. Molecules are made of atoms.
ðŸ‘ŽError 1 (42%): Do not provide new information.
ðŸ‘ŽError 2 (37%): Do not justify the answer.
GPT-3GPT-3Figure 1: Vacuous rationales generated by a prompted
LM (GPT-3) for StrategyQA. In both types of error
cases, LM fails to give rationales consistent with the
answers due to hallucination.
useless for justifying the modelâ€™s behavior. In
this work, we present Self-Consistent Chain- Of-
Thought Dis Tillation ( SCOTT ), a knowledge dis-
tillation (KD) method for eliciting faithful CoT rea-
soning, where a small student model learns from a
large teacher model to generate CoT rationales that
are consistent to its own predictions.
Existing works (Shridhar et al., 2022; Li et al.,
2022a) propose learning to reason from large LMs
mainly for computation efficiency or task perfor-
mance. They prompt a large LM (the teacher)
to generate rationales for a downstream dataset,
which is then used to train a small LM (the stu-
dent). However, these works neglect the following
two issues which could undermine the faithfulness
of the rationales. First, LMs are prone to hallu-
cination, meaning they often generate text that is
not grounded by the input (Maynez et al., 2020; Ji
et al., 2022). Therefore, the teacher may not gen-
erate on-topic rationales, which fully support the
answer. In our pioneer study (Figure 1) over 100
random rationales generated by GPT-3, we found
42% of them not providing new information that
is not stated in the task input and 37% of them not
justifying the answer2. This inconsistency between
the rationale and answer would then be inherited
by the student. Second, the student may treat ra-
2Wiegreffe et al. obtains a similar observation on the ratio-
nales generated by GPT-3 for the CommonsenseQA dataset.arXiv:2305.01879v4  [cs.CL]  30 Aug 2023<question> = Could the Great Wall of China connect the Dodgers to the White Sox?<answer> = yes
The Great Wall of China is about 5,500 miles longâ€¦
Input:Would a vegan eat a traditional Paella dish?Output:Paella is a dish that traditionally contains seafood and meat ... Sothe answer is no.Teacher: Consistent Rationales Annotation via Contrastive DecodingStudent: Learning to Reason Faithfully             via Counterfactual Reasoning{Q,R,A}Small LMLarge LMPromptQ: Do hamsters provide food for any animals?A: The answer is yes. Hamsters are prey ...Q: Could Brooke Shields succeed at University of Pennsylvania?A: The answer is yes. Brooke Shields went to Princeton University ...Q: <question>A: The answer is <answer>.Figure 2: Overview of our knowledge distillation framework for faithful reasoning. (a) Teacher: A large LM
prompted to generate a consistent rationale given a question and the gold answer in the training set via contrastive
decoding. (b) Student: A small LM fine-tuned to generate a rationale and then answer via counterfactual reasoning.
tionale generation and answer prediction as two
independent processes. This is due to the spuri-
ous correlations between the question and answer,
which is exploited as a reasoning shortcut by the
student (Branco et al., 2021). The two issues to-
gether would lead to an unfaithful student which
learns to generate vacuous rationales and may make
predictions inconsistent with the rationales.
To address these issues, we propose to enhance
the vanilla KD process from two ends respectively.
To elicit more on-topic rationales from the teacher,
we propose to leverage contrastive decoding which
aims to ground each rationale to the answer (Â§ 3.1).
This technique encourages the teacher to generate
tokens that are more plausible only when the an-
swer is considered instead of the ones that are fairly
plausible even without the answer during decoding.
To train a faithful student, we ask the student to
conduct counterfactual reasoning, i.e., predicting
accordingly when the rationales are leading to dif-
ferent answers (Â§ 3.2). We obtain the training data
by asking the teacher to generate a rationale for a
sampled incorrect answer. The reasoning shortcut
between the question and the gold answer is thus
removed since now the student needs to give a dif-
ferent answer for the same question, according to
the rationales provided during training.
We conduct experiments on several open-domain
question answering tasks that require knowledge-
intensive reasoning. Experiments show that: (1)
Contrastive decoding can lead to a more consistent
teacher which generates rationales that are more
supportive of the gold answers. (2) Trained on the
more consistent rationale-answer pairs, the student
learns to better associate the answer prediction with
the rationale generation. (3) With counterfactual
reasoning as an auxiliary training objective, thestudent learns not to take the reasoning shortcut
and instead respect the rationale more. (4) Despite
being more faithful, our model performs compara-
bly to the baselines. (5) Ablation study shows that
although performing better, larger student models
are more prone to being inconsistent. Our method
robustly remedies the inconsistency regardless of
the size of the student model. (6) With a more faith-
ful student, we can better improve its performance
by correcting its rationale, demonstrating the utility
of our method in model refinement.
2 Chain-of-Thought Distillation
Our goal is to 1) elicit consistent rationales, i.e.,
those well justifying the gold answers, from a
large LM as supervision, and then 2) train a self-
consistent student model to reason faithfully, i.e.,
answer accordingly to its generated rationale. We
consider the task of language-based reasoning
where the required knowledge is not provided in the
task input. Specifically, we focus on open-domain
question answering (QA) which is the most general
setting adopted by prior works: given a question
q, a QA system is asked to predict the gold answer
aâˆ—. For interpretability, we also require the model
to provide a free-text rationale r, which justifies
its prediction. Below we describe the overview of
a vanilla KD framework as illustrated in Figure 2.
We then discuss the limitations and propose our
method in Â§ 3.
2.1 Generating Rationale Annotation
Instead of asking humans to annotate a rationale
for each question-answer tuple {q, aâˆ—}, we obtain
the rationale from a teacher model automatically
using in-context learning. The idea is to prompt
a frozen LM as the teacher with only a few an-notated examples as demonstration before a new
instance is provided. Each example consists of
a question qrandomly sampled from the training
set, the gold answer aâˆ—and a human-annotated ra-
tionale rwhich justifies why aâˆ—is correct. The
prompt pis structured in the format as shown in
Figure 2 (the Prompt in the left part). To obtain the
rationale for a new question q, one basic strategy
could be greedy decoding, which selects the most
plausible token at each step:
tâˆ—
i= arg max log P(ti|p, q, aâˆ—, t<i). (1)
2.2 Training a Student Model
Now with the annotated training data {q, r, aâˆ—}, we
can train a smaller model as the student. There
are many ways to implement a QA model that can
make a prediction as well as generate a rationale.
In this work, we focus on the self-rationalization
paradigm, where the student firstly generates a ra-
tionale and then predicts the answer conditioning
on the generated rationale. This is in contrast to
related works which conduct post-rationalization,
i.e., generating the rationale after the answer is
predicted, or multi-task learning, which treats ratio-
nale generation as an auxiliary task besides answer
prediction. The reason is that the generation of
the rationale for the latter two paradigms does not
affect the decision making by design, and therefore
the faithfulness of the rationale is not guaranteed
in the first place.
Given a question q, the student model is trained
to output a sequence of rationale tokens concate-
nated with the answer tokens as shown in Figure 2
(the output in the right part). One straightforward
implementation is simply fine-tuning a text-to-text
LM over the silver training data generated by the
teacher using standard language modeling loss:
Lfactual =âˆ’X
ilogP(ti|q, t<i), (2)
which we refer as factual reasoning loss.
3 Distilling a Self-Consistent Student
There are two vital issues with the vanilla KD pro-
cess described in the previous section. Firstly, neu-
ral LMs are known to suffer from the issue of hal-
lucination, meaning they often generate text that
is not grounded by the input (Maynez et al., 2020;
Ji et al., 2022). This would lead to the generated
rationale not supporting the given answer. The in-
consistency between the rationale and the answer
ðŸ¤–Q: Would someone with back pain enjoy picking strawberries?A: The answer is no.Q: Would someone with back pain enjoy picking strawberries?A: The answer is <perturbed_answer>.
Greedy Decoding 
ðŸ¤–:-The back is a part of the body. The back is not a fruit. Thus, someone with back pain would not enjoy picking strawberries.Contrastive Decoding
ðŸ¤–-:(a) <perturbed_answer> = empty string-Manual labor can cause back pain. Thus, someone with back pain would not enjoy picking strawberries.(b) <perturbed_answer> = yes-The spine is needed to support the body. If someone has back pain, they would not be able to pick strawberries.
Figure 3: Contrastive decoding for obtaining ratio-
nales that are more grounded by the gold answers, by
preferring tokens that are more plausible only when the
answer is considered.
would then be inherited by the student, which is
misled to think that the answer prediction is in-
dependent of the rationale generation. Secondly,
the student model would learn to predict the an-
swer by taking a reasoning shortcut (Branco et al.,
2021), without taking into account the generated ra-
tionale (even though the answer prediction is condi-
tioned on the rationale). This is due to the spurious
correlations between the question and the answer
which are found in various implicit reasoning task
datasets (Gururangan et al., 2018; Zellers et al.,
2019; Blodgett et al., 2020).
The two issues mentioned above would result
in an untrustworthy student whose generated ratio-
nales do not consistently justify its answers. To
mitigate this, we propose two corresponding tech-
niques as detailed below.
3.1 A Consistent Teacher: Contrastive
Decoding
To encourage the teacher to generate a more on-
topic rationale that supports the answer, our pro-
posed method extends a prior technique called
contrastive decoding for open-ended text gener-
ation (Li et al., 2022b). The core idea is to search
rationale tokens that are more plausible only when
the answer is considered instead of the ones that
are fairly plausible even without the answer during
decoding. To implement this idea, we firstly model
the hallucinating behavior by providing a perturbed
answer aâ€²to the same teacher and then obtain the
plausibility growth of any token tigiven the answeraâˆ—as
G(ti|aâˆ—) = logP(ti|p, q, aâˆ—, t<i)
P(ti|p, q, aâ€², t<i). (3)
We investigate two ways of perturbing the answer:
setting aâ€²as an empty string or an incorrect answer
other than aâˆ—3. The first way (with an empty string)
punishes tokens that are generally plausible when
the gold answer aâˆ—is not considered by a hallu-
cinated LM. The second way (with an incorrect
answer) takes a step further by encouraging the
teacher to generate a rationale that is more distinc-
tive between gold and wrong answers. Figure 3
shows the generations for an example question
from greedy decoding and contrastive decoding.
To strike a balance between language fluency
and the grounding with aâˆ—, we incorporate the plau-
sibility growth into Eq. 1 by aggregation as our
final contrastive decoding strategy:
tâˆ—
i= arg max log P(ti|p, q, aâˆ—, t<i) +G(ti|aâˆ—)
(4)
3.2 A Faithful Student: Counterfactual
Reasoning
To encourage the student to reason faithfully to-
wards its generated rationale, we train the student
to conduct counterfactual reasoning (Roese, 1997),
i.e., answer accordingly when the rationale is lead-
ing to a different answer. This would help remove
the reasoning shortcut between a question and the
gold answer (Figure 4) since now the student is
asked to answer differently for the same question.
To implement this idea, we firstly replace the gold
answer fed to the teacher in Eq. 4 with a wrong
answer aâ€²randomly (with the same sampling strat-
egy as in Â§ 3.1) as if aâ€²is correct. We thus obtain a
counterfactual rationale râ€²that leads to the wrong
answer aâ€². We then train the model to generate aâ€²
when râ€²is directly fed to the decoder as teacher-
forcing (the language modeling loss is only applied
to the answer tokens tiâˆˆaâ€²):
Lcounterfactual =âˆ’X
ilogP(ti|q, râ€², t<i).(5)
To avoid confusing the student about the task, we
indicate the training objective Eq. 2 (or Eq. 5) to the
student by appending the keyword [Factual]
(or[Counterfactual] ) at the beginning of
3For yes/no or true/false questions, we obtain the incor-
rect answer by flipping the gold answer. For multi-choice
questions, we randomly pick one incorrect answer.
QEA(a) Factual reasoningReasoning shortcut!QEâ€™Aâ€™Shortcut removed!(b) Counterfactual reasoning
Input: [Facutal] Do black-tailed jackrabbits fear the European wildcat?Output: [Factual] The European wildcat is not a predator of the black-tailed jackrabbit. Thus, the black-tailed jackrabbit does not fear the European wildcat. Sothe answer is no.Input: [Counterfacutal] Do black-tailed jackrabbits fear the European wildcat?Output: [Counterfacutal] The European wildcat is a predator of the black-tailed jackrabbit. Thus, the European wildcat is a threat to the black-tailed jackrabbit. Sothe answer is yes.Figure 4: Counterfactual reasoning for teaching the
student to reason faithfully, i.e., answer differently ac-
cording to the rationale.
both the input sequence to the encoder and the
output sequence to the decoder (see Figure 4 for
an example input and output). The overall training
loss is the sum of Eq. 2 and Eq. 5.
4 Experiments
We aim to answer the following research questions
in our experiments: (1) Can our contrastive de-
coding strategy lead to a more consistent teacher?
(2) Can a more consistent teacher and the counter-
factual reasoning objective lead to a student that
reasons more faithfully? (3) Can we have more
control over a self-consistent studentâ€™s predictions
by modifying its generated rationales?
4.1 Datasets
We experiment with several language-based rea-
soning tasks that are knowledge-intensive: (1)
CSQA (Talmor et al., 2018) is a five-choice QA
dataset that tests general commonsense about the
daily concepts. (2) StrategyQA (Geva et al., 2021)
is a binary (yes/no) QA dataset where the required
reasoning steps are implicit in the question. (3)
CREAK (Onoe et al., 2021) is a fact-checking
(true/false) dataset which tests commonsense rea-
soning about entity knowledge. (4) QASC (Khot
et al., 2020) is an eight-choice QA dataset which re-
quires both knowledge facts retrieval and the com-
mon sense for composing the facts. Since the test
labels for these datasets are not publicly available,
we treat the official development set as our test set,
while randomly splitting the official training set
into a new training set and development set.
4.2 Evaluation Metrics
(1) To evaluate the consistency between the ratio-
nales generated by the teacher and the gold answersHuman Greedy CD-
EmptyCD-
Wrong
CSQA10121416182022LAS16.5
12.418.721.1
Human Greedy CD-
EmptyCD-
Wrong
StrategyQA1520253035LAS
15.226.029.933.0
Human Greedy CD-
EmptyCD-
Wrong
CREAK14151617181920212223LAS
15.718.319.421.9
Human Greedy CD-
EmptyCD-
Wrong
QASC363840424446485052LAS45.6
39.850.852.1Figure 5: Simulatability (LAS) of the rationales generated from different teacher models as a measurement the
consistency between the rationales and the gold answers. { Greedy, CD-Empty, CD-Wrong } refer respectively to
using greedy decoding, contrastive decoding with empty/wrong answer to obtain rationale tokens from the teacher.
provided as input, we use the LAS metric (Hase
et al., 2020), whose core idea is to measure how
well the rationales assist a simulator to predict the
gold answers aâˆ—, computed as the difference be-
tween the task performance when the rationale
is provided as input vs. when it is not, namely
Acc(qrâ†’aâˆ—)âˆ’Acc(qâ†’aâˆ—). (2) To evaluate
the faithfulness of the rationales generated by the
student, we use LAS to measure how well the ra-
tionales help a simulator to predict a studentâ€™s pre-
dictions aâ€², namely Acc(qrâ†’aâ€²)âˆ’Acc(qâ†’aâ€²).
We implement each simulators with a fine-tuned
T5-large model (Raffel et al., 2020) respectively.
(3) To evaluate how well the student preserves its
task performance on the downstream datasets, we
use accuracy as the metric.
4.3 Implementation Details
We use GPT-neox (Black et al., 2022), a LM with
20B parameters as the teacher since the model
checkpoint is publicly available, which allows us to
host it offline and have access to token-wise proba-
bilities as required in our contrastive decoding. We
then implement two teacher variants by using an
empty string or a wrong answer as the perturbed
answer aâ€²in Eq. 4 respectively. The obtained ra-
tionales are then used to fine-tune two T5-3b LMs
as the students respectively. For both variants, we
train the student using the sum of factual training
loss Eq. 2 and counterfactual training loss Eq. 5.
4.4 Baselines
Chain-of-Thought (CoT) Since we elicit the ratio-
nales from GPT-neox (with 20b parameters) (Black
et al., 2022) to train the student, we prompt the
same model (GPT-neox) to firstly explain and then
predict using CoT prompting (Wei et al., 2022b).
Learn from Human To demonstrate the advantage
of our automatic way of generating rationale anno-tations, we implement this baseline as a fine-tuned
T5-3b LM over human-annotated rationales, which
are expensive to obtain and could be noisy.
Learn from Greedy Decoding We implement this
baseline as a fine-tuned T5-3b LM over the ratio-
nales obtained by greedy decoding using the same
LM as our main method. We also implement an-
other variant by adding the counterfactual reason-
ing loss when fine-tuning the student, where the
rationales for the wrong answers are obtained by
greedy decoding.
We also implement two baselines of our method
by training the student with the rationales obtained
by contrastive decoding with empty/wrong answers
based on factual reasoning only. We run all the
experiments for 5 times using a fixed set of random
seeds and report the average results.
4.5 Main Results
Can contrastive decoding lead to a more con-
sistent teacher? Figure 5 shows the consis-
tency between the rationales generated by differ-
ent teachers and the gold answers measured by
LAS. Across four datasets, contrastive decoding
with either empty or wrong answers yield more con-
sistent rationales compared to human annotation
and greedy decoding. This demonstrates the ef-
fectiveness of our contrastive decoding strategy in
encouraging the teacher to generate more on-topic
rationales. Moreover, using wrong answers is bet-
ter than using empty strings for contrastive decod-
ing. This shows that by contrasting with the wrong
answers, the teacher can generate more distinguish-
able rationales that lead to the gold answers, thus
obtain higher consistency. Greedy decoding yields
less consistent rationales compared to human anno-
tation, verifying our claim that LMs are prone to
generating text not grounded by the gold answers.CoT Human Greedy CD-Empty CD-Wrong05101520LAS11.7
8.710.613.216.015.217.7 17.9Factual Factual+Counterfactual
CoT Human Greedy CD-Empty CD-Wrong
CSQA4050607080Accuracy
38.475.7 75.2 74.9 75.3 75.0 74.9 75.4
CoT Human Greedy CD-Empty CD-Wrong05101520253035LAS
1.13.419.422.420.825.628.529.4Factual Factual+Counterfactual
CoT Human Greedy CD-Empty CD-Wrong
StrategyQA505254565860626466Accuracy55.363.6
61.5 61.4
60.6 60.761.560.8
CoT Human Greedy CD-Empty CD-Wrong5.07.510.012.515.017.520.022.525.0LAS
7.614.016.217.3 17.5
16.018.421.9Factual Factual+Counterfactual
CoT Human Greedy CD-Empty CD-Wrong
CREAK65.067.570.072.575.077.580.082.5Accuracy
67.781.4
79.979.580.279.779.378.5
CoT Human Greedy CD-Empty CD-Wrong30.032.535.037.540.042.545.047.550.0LAS
32.043.8
41.442.6 42.7
39.946.6
45.3Factual Factual+Counterfactual
CoT Human Greedy CD-Empty CD-Wrong
QASC3040506070Accuracy
32.668.6
65.0
62.164.0 63.7 63.0 63.5Figure 6: Faithfulness (LAS) and task performance (accuracy) of the compared methods on the experimented
datasets. The x-axis represents the CoT baseline and knowledge distillation methods that use Human Annotation,
Greedy Decoding, and Contrastive Decoding with empty strings/wrong answers as the teachers. For knowledge
distllation methods, we apply Factual training and Factual+Counterfactual training to train the students.
Table 1: Human evaluation on the rationales generated
by different teacher models for StrategyQA. A fair level
of agreement measured by Fleiss Kappa ( Îº=0.26) is
obtained among three annotators.
Teacher Model Grammaticality New Info Supports Answer
Greedy 0.99 0.65 0.48
Contrast.-Empty 0.97 0.77 0.58
Contrast.-Wrong 0.97 0.82 0.63
We also conduct a human evaluation over 100 ra-
tionales generated by different decoding strategies
for StrategyQA. Annotators are asked to judge the
rationales by 3 dimensions: 1) Grammaticality (Is
the rationale grammatical?) 2) New Info (Does the
rationale provide new information not expressed
in the question?) 3) Supports Answer (Does the
rationale justify the answer?). Table 1 confirms that
our two contrastive decoding strategies yield more
informative and on-topic rationales than greedy de-coding, with a slightly worse grammaticality. We
list examples in Table 2 (appendix) to showcase
how rationales from contrative decoding are more
consistent with gold answers than greedy decoding.
Can a more consistent teacher train a more faith-
ful student? Figure 6 (upper parts of each sub-
figure) shows the faithfulness of the students mea-
sured in LAS on the experimented datasets. First,
the CoT method often achieves much lower LAS
compared to the KD methods across four datasets,
showing that the generated rationales do not faith-
fully reflect the decision making in CoT. Second,
we observe that students trained with the rationales
from contrastive decoding with either empty strings
or wrong answers generally achieve higher LAS
scores compared to the baselines. Together with the
observation on the consistency of the teacher (Fig-
ure 5), this validates that a more consistent teacher
train a more faithful student and the inconsistencyin the training data generated by the teacher will be
inherited by the student.
Can couterfactual reasoning loss further im-
prove the faithfulness? Figure 6 shows the stu-
dents fine-tuned additionally with counterfactual
training loss achieve higher faithfulness than their
counterparts which are fine-tuned with factual train-
ing only. This validates that counterfactual reason-
ing can further improve the studentâ€™s faithfulness,
as it may still treat rationale generation and answer
prediction as two independent processes.
Can a faithful student still preserve its perfor-
mance? Figure 6 (lower parts of each sub-figure)
shows the performance of the students measured
in accuracy. First, CoT methods achieve lower
accuracy compared to the KD methods, showing
the benefit of combining the supervision from the
teacher (the rationales) and the labeled datasets
(the answers). Second, all the KD methods achieve
comparable performance. Together with the ob-
servation over faithfulness, this demonstrates our
method can improve faithfulness of the model
while not hurting its performance. Note that the stu-
dent which learns from human annotation achieves
slightly better results compared to other students.
This is because the human rationales are less con-
sistent with the answers (as evidenced in Figure 5).
Therefore, the student learns to generate the ratio-
nales and predict the answers more independently,
which allows it to exploit the spurious correlation
and achieve better performance. Our further analy-
sis (Â§ 4.7) shows that such performance gain is sus-
picious as changing the rationales does not change
the studentâ€™s predictions mostly.
4.6 Ablation on the student model size
We ablate the student model size to see how its
faithfulness and performance are affected. From
Figure 7, we observe that larger student models
achieve higher performance but lower faithfulness.
This confirms that it requires sufficient capacity for
storing knowledge necessary for reasoning (Wei
et al., 2022a), but larger models are also better at
answering the questions independently of the ra-
tionales. Still, our models are more faithful than
baselines and comparable in performance with dif-
ferent model sizes.
4.7 Controlling the behavior of the Student
One important utility of faithful rationales is that
we can have more control over the behavior of the
T5-base (220M) T5-Large (770M) T5-3B8101214161820LAS
Greedy:factual
Greedy:+counter.
CD-Empty:factual
CD-Empty:+counter.
CD-Wrong:factual
CD-Wrong:+counter.
T5-base (220M) T5-Large (770M) T5-3B
Student Model Size4050607080AccuracyGreedy:factual
Greedy:+counter.
CD-Empty:factual
CD-Empty:+counter.
CD-Wrong:factual
CD-Wrong:+counter.Figure 7: Faithfulness (LAS) and task performance
(accuracy) of the compared methods with different stu-
dent model sizes. Each model is named by the teacher
it learns from and the training objective as teacher
model:training objective .
student via changing its rationales. If the model can
make predictions consistent with its rationales, we
can either impair or improve the its performance by
perturbing or refining its rationales. To verify this,
we conduct two types of edition to the rationales
generated by the student, namely perturbation and
refinement as described below. We then feed the
edited rationales to the decoder of the student di-
rectly (as teacher forcing) and see if the student
will act accordingly, i.e., predict more badly (or
accurately) due to the worse (or better) rationales.
Rationales Perturbation For perturbing the ra-
tionales, we randomly replace 50% of the tokens
in the generated rationales from the student and
then feed the perturbed rationales râ€²back to the
decoder of the student. We finally calculate the
performance drop (or sensitivity), i.e., Acc(qrâ†’
aâˆ—)âˆ’Acc(qrâ€²â†’aâˆ—). Figure 8 (the lower parts)
shows the results on CSQA and CREAK. First, per-
turbing the rationales from the student that is fine-
tuned with human-annotation has little (down to
1.1%on CSQA) impact on its performance, mean-
ing that the student largely ignores the rationales
when making prediction. Second, learning from
rationales obtained by contrastive decoding with
empty or wrong answers leads to a student that is
more sensitive to the rationale perturbation com-
pared to learning from greedy decoding. This again
verifies the necessity of having a consistent teacher
in order to train a faithful student. Lastly, our coun-
terfactual training loss further improves the sensi-Human Greedy CD-Empty CD-Wrong222426Perf. Gain (%)22.823.724.424.2
23.824.5 24.6Standard Factual+Counterfactual
Human Greedy CD-Empty CD-Wrong
CSQA0204060Perf. Drop (%)
1.112.328.0
21.354.457.961.0
Human Greedy CD-Empty CD-Wrong10152025Perf. Gain (%)11.815.217.618.6
14.917.920.1Standard Factual+Counterfactual
Human Greedy CD-Empty CD-Wrong
CREAK05101520Perf. Drop (%)2.57.7
6.49.911.216.717.4Figure 8: Performance gain (drop) of the compared methods when the oracle (perturbed) rationales are fed to the
decoder of the model on CSQA and CREAK.
tivity of the student, demonstrating that the student
is more faithful towards the rationales.
Rationales Refinement As a proxy refinement, we
obtain the oracle rationales râˆ—automatically by
asking the teacher to rationalize for gold answers
using each compared decoding strategy. For the
student trained with human annotation, we directly
use the annotated rationales as the oracle. We then
calculate the performance gain, i.e., Acc(qrâˆ—â†’
aâˆ—)âˆ’Acc(qrâ†’aâˆ—). Figure 8 (the upper parts)
shows the results on CSQA and CREAK. First, we
observe that oracle human-annotated rationales do
not bring as much performance gain as machine-
generated rationales do. This demonstrates that
even trained with human annotation, the student
is still prone to being unfaithful to its rationales.
Second, we observe that contrastive decoding (with
either empty strings or wrong answers) leads to
higher performance gains from the student. By
adding counterfactual training, the performance
gains are further increased. This demonstrates the
advantage brought by our method, which is that we
can have more success in debugging a reasoning
model by refining its rationales.
5 Related Works
Free-text Rationales A variety of datasets have
been proposed to collect human-annotated ratio-
nales alongside each task instance (Camburu et al.,
2018; Rajani et al., 2019; Aggarwal et al., 2021),
aiming to train the downstream models to explain
their predictions in natural language. However, hu-
man annotation is expensive and the resulting ratio-nales are reported to be of poor quality (Aggarwal
et al., 2021; Sun et al., 2022). Our work leverages
a prompted LM to obtain rationales automatically
for supporting both correct and incorrect answers,
using only a few annotated examples as demonstra-
tion. The rationales for supporting the incorrect
answers further enable the student to conduct coun-
terfactual reasoning, which is not available from
existing human annotation.
Prompted Self-Rationalization Models Recent
works have been proposed to prompt large LMs
to generate a free-text rationale before making the
prediction (Nye et al., 2021; Wei et al., 2022b).
However, this technique relies on extremely large
LMs (with over 100B parameters) to work effec-
tively (Wei et al., 2022b,a), which requires sig-
nificant computation resources or expensive API
calls (Shridhar et al., 2022). Meanwhile, the ra-
tionales generated by such models are shown to
contradict the context (Ye and Durrett, 2022) and
fail to faithfully represent the underlying reasoning
process (Wang et al., 2022). In contrast, our student
is trained to be more faithful towards its generated
rationales using a smaller LM.
Knowledge Distillation There exist some works
that explore the idea of distilling rationales knowl-
edge from a large LM to a small LM as the student.
Chan et al. proposed to learn a student model that
only predicts answers from a teacher model that is
augmented with rationales. Eisenstein et al. pro-
posed to train the student to extract the sentence
containing the answer, which is not applicable to
reasoning tasks that require background knowledge.Shridhar et al. proposed to train the student to ask
and answer sub-questions necessary for decompos-
ing the main question, which is tailored to solve
math word problems (Cobbe et al., 2021) with an
equation generator for guiding the student while
we do not have such a constraint. Li et al. proposed
to train the student on the joint task of generating
the answers and the rationales, which only act as a
regularization and do not affect the studentâ€™s pre-
diction during inference. More importantly, both
Shridhar et al. and Li et al. do not consider the
faithfulness of the rationales, which is critical for
examining the behavior of the student.
6 Conclusion
This work presents a faithful KD framework for
learning a small, self-consistent CoT model from
a large teacher model. To ensure the student rea-
son faithfully, we propose (1) contrastive decoding
for obtaining a consistent teacher and (2) counter-
factual reasoning for teaching a faithful student.
Experiments show that these two techniques jointly
lead to a more faithful student compared to the
baselines, while preserving much performance ac-
curacy. Our further analysis shows that changing
the rationales has a larger impact on the studentâ€™s
behavior and thus we can have more success in
debugging the model by refining its rationales.
Limitations
Compared to a standard knowledge distillation pro-
cess, our method requires additional computation
when preparing training data and training the stu-
dent. First, our contrastive decoding needs to per-
form forward pass in the teacher model one time
more than greedy decoding does to obtain the per-
turbed plausibility for each token generated (Eq. 4).
Second, our KD process introduces additional train-
ing data for training the student with the counter-
factual reasoning objective (Eq.5). Besides compu-
tation cost, this work focuses on improving faith-
fulness of the rationales rather than performance,
which is complementary to prior works which lever-
ages rationales for improving the performance only.
Ethics Statement
Our KD process leverages large LMs to obtain
rationale annotation, which may expose social
bias encoded in these models (Lucy and Bamman,
2021). The bias may be further inherited by the
student model. Nevertheless, our method improvesthe faithfulness of the rationales, making the pre-
dictions from the student accountable. Without
the faithful rationales, it would be unclear to users
about whether the model is making predictions
based on some unintended bias.
References
Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet
Agrawal, Dinesh Khandelwal, Parag Singla, and Di-
nesh Garg. 2021. Explanations for commonsenseqa:
New dataset and models. In Workshop on Common-
sense Reasoning and Knowledge Bases .
Sid Black, Stella Biderman, Eric Hallahan, Quentin An-
thony, Leo Gao, Laurence Golding, Horace He, Con-
nor Leahy, Kyle McDonell, Jason Phang, Michael
Pieler, USVSN Sai Prashanth, Shivanshu Purohit,
Laria Reynolds, Jonathan Tow, Ben Wang, and
Samuel Weinbach. 2022. GPT-NeoX-20B: An open-
source autoregressive language model. In Proceed-
ings of the ACL Workshop on Challenges & Perspec-
tives in Creating Large Language Models .
Su Lin Blodgett, Solon Barocas, Hal DaumÃ© III, and
Hanna Wallach. 2020. Language (technology) is
power: A critical survey of" bias" in nlp. arXiv
preprint arXiv:2005.14050 .
Ruben Branco, AntÃ³nio Branco, Joao Rodrigues, and
Joao Silva. 2021. Shortcutted commonsense: Data
spuriousness in deep learning of commonsense rea-
soning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 1504â€“1521.
Oana-Maria Camburu, Tim RocktÃ¤schel, Thomas
Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-
ral language inference with natural language expla-
nations. Advances in Neural Information Processing
Systems , 31.
Aaron Chan, Zhiyuan Zeng, Wyatt Lake, Brihi Joshi,
Hanjie Chen, and Xiang Ren. 2022. Knife: Knowl-
edge distillation with free-text rationales. arXiv
preprint arXiv:2212.09721 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. 2021. Training veri-
fiers to solve math word problems. arXiv preprint
arXiv:2110.14168 .
Jacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael
Collins, and David Mimno. 2022. Honest students
from untrusted teachers: Learning an interpretable
question-answering pipeline from a pretrained lan-
guage model. arXiv preprint arXiv:2210.02498 .
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of theAssociation for Computational Linguistics , 9:346â€“
361.
Suchin Gururangan, Swabha Swayamdipta, Omer Levy,
Roy Schwartz, Samuel R Bowman, and Noah A
Smith. 2018. Annotation artifacts in natural language
inference data. arXiv preprint arXiv:1803.02324 .
Peter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal.
2020. Leakage-adjusted simulatability: Can models
generate non-trivial explanations of their behavior in
natural language? arXiv preprint arXiv:2010.04119 .
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,
Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea
Madotto, and Pascale Fung. 2022. Survey of halluci-
nation in natural language generation. arXiv preprint
arXiv:2202.03629 .
Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2020. QASC: A
dataset for question answering via sentence compo-
sition. In The Thirty-Fourth AAAI Conference on
Artificial Intelligence, AAAI 2020, The Thirty-Second
Innovative Applications of Artificial Intelligence Con-
ference, IAAI 2020, The Tenth AAAI Symposium on
Educational Advances in Artificial Intelligence, EAAI
2020, New York, NY, USA, February 7-12, 2020 ,
pages 8082â€“8090. AAAI Press.
Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,
Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian,
Baolin Peng, Yi Mao, et al. 2022a. Explanations
from large language models make small reasoners
better. arXiv preprint arXiv:2210.06726 .
Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy
Liang, Jason Eisner, Tatsunori Hashimoto, Luke
Zettlemoyer, and Mike Lewis. 2022b. Contrastive de-
coding: Open-ended text generation as optimization.
arXiv preprint arXiv:2210.15097 .
Li Lucy and David Bamman. 2021. Gender and rep-
resentation bias in GPT-3 generated stories. In Pro-
ceedings of the Third Workshop on Narrative Un-
derstanding , pages 48â€“55, Virtual. Association for
Computational Linguistics.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906â€“1919, On-
line. Association for Computational Linguistics.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, et al. 2021. Show your work: Scratch-
pads for intermediate computation with language
models. arXiv preprint arXiv:2112.00114 .
Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and
Greg Durrett. 2021. Creak: A dataset for common-
sense reasoning over entity knowledge. OpenReview .Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1â€“67.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019. Explain your-
self! leveraging language models for commonsense
reasoning. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 4932â€“4942, Florence, Italy. Association for
Computational Linguistics.
Neal J Roese. 1997. Counterfactual thinking. Psycho-
logical bulletin , 121(1):133.
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya
Sachan. 2022. Distilling multi-step reasoning ca-
pabilities of large language models into smaller mod-
els via semantic decompositions. arXiv preprint
arXiv:2212.00193 .
Jiao Sun, Swabha Swayamdipta, Jonathan May, and
Xuezhe Ma. 2022. Investigating the benefits of free-
form rationales. arXiv preprint arXiv:2206.11083 .
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2018. Commonsenseqa: A question
answering challenge targeting commonsense knowl-
edge. arXiv preprint arXiv:1811.00937 .
Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen,
and Xiang Ren. 2022. Pinto: Faithful language rea-
soning using prompt-generated rationales. arXiv
preprint arXiv:2211.01562 .
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022a. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,
Mark Riedl, and Yejin Choi. 2021. Reframing
human-ai collaboration for generating free-text ex-
planations. arXiv preprint arXiv:2112.08674 .
Xi Ye and Greg Durrett. 2022. The unreliability of
explanations in few-shot in-context learning. arXiv
preprint arXiv:2205.03401 .
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machine really finish your sentence? arXiv preprint
arXiv:1905.07830 .A Appendix
A.1 Example generations from teachers
Table 2: Examples where rationales generated by con-
trastive decoding with empty/wrong answers are rated
higher than rationales generated by greedy decoding in
our human evaluation.
Q: Do workers at Nissanâ€™s headquarters eat with chopsticks?
A: Yes.
Greedy : Workers at Nissanâ€™s headquarters eat with chopsticks.
Chopsticks are used in China. Thus, workers at Nissanâ€™s head-
quarters eat with chopsticks.
Contrastive-Empty : Workers at Nissanâ€™s headquarters are
Japanese. Japanese eat with chopsticks.
Contrastive-Wrong : Nissanâ€™s headquarters is in Japan. Japan
is a country where people eat with chopsticks. Thus, workers
at Nissanâ€™s headquarters eat with chopsticks.
Q: Would Constitution of the United States paper offend
PETA?
A: Yes.
Greedy : PETA is an animal rights group. The Constitution
of the United States is a document that protects the rights of
animals. Thus, PETA would be offended by the Constitution
of the United States.
Contrastive-Empty : PETA is against the use of animal prod-
ucts. The Constitution of the United States is written on animal
products. Thus, PETA would be offended by the Constitution
of the United States.
Contrastive-Wrong : PETA is against the use of animal prod-
ucts. The Constitution of the United States is written on animal
products. Thus, PETA would be offended by the Constitution
of the United States.
Q: Does Sam Harris worship Shiva?
A: No.
Greedy : Sam Harris is a Hindu. Shiva is a Hindu god. Thus,
Sam Harris does not worship Shiva.
Contrastive-Empty : Shiva is a Hindu god. Sam Harris is an
atheist. Thus, Sam Harris does not worship Shiva.
Contrastive-Wrong : Shiva is a Hindu god. Sam Harris is an
atheist. Thus, Sam Harris does not worship Shiva.
Q: Are a dozen pickles good for easing hypertension?
A: No.
Greedy : A dozen pickles have about 1,000 calories. A person
needs about 2,000 calories a day. Thus, a dozen pickles would
not be good for easing hypertension.
Contrastive-Empty : Pickles are acidic. Acidic foods are not
good for easing hypertension.
Contrastive-Wrong : The recommended daily intake of
sodium is 2,400 mg. A dozen pickles have about 0.5 g of
sodium. Thus, a dozen pickles would not be good for easing
hypertension.
Q: Would it be unusual to see frost in September in Texas?
A: Yes.
Greedy : Frost can occur in September in Texas. However, it is
unusual.
Contrastive-Empty : Frost occurs in the winter. Thus, it would
be unusual to see frost in September in Texas.
Contrastive-Wrong : Frost is usually seen in the winter. Thus,
it would be unusual to see frost in September in Texas.