Interpreting Training Aspects of Deep-Learned
Error-Correcting Codes
N. Devroye, A. Mulgund, R. Shekhar, Gy. Tur ´an∗, M. ˇZefran, and Y . Zhou
University of Illinois Chicago (UIC), Chicago, IL, USA
∗UIC and MTA-SZTE Research Group on Artiﬁcial Intelligence, ELRN, Szeged, Hungary
{devroye, mulgund2, rshekh3, gyt, yzhou238, mzefran }@uic.edu
Abstract —As new deep-learned error-correcting codes con-
tinue to be introduced, it is important to develop tools to
interpret the designed codes and understand the training process.
Prior work focusing on the deep-learned TurboAE has both
interpreted the learned encoders post-hoc by mapping these onto
nearby “interpretable” encoders, and experimentally evaluated
the performance of these interpretable encoders with various
decoders. Here we look at developing tools for interpreting the
training process for deep-learned error-correcting codes, focusing
on: 1) using the Goldreich-Levin algorithm to quickly interpret
the learned encoder; 2) using Fourier coefﬁcients as a tool for
understanding the training dynamics and the loss landscape;
3) reformulating the training loss, the binary cross entropy, by
relating it to encoder and decoder parameters, and the bit error
rate (BER); 4) using these insights to formulate and study a new
training procedure. All tools are demonstrated on TurboAE, but
are applicable to other deep-learned forward error correcting
codes (without feedback).
I. I NTRODUCTION
Coding theory aims to develop optimal encoder-decoder
pairs for various channels and optimization criteria. This
has traditionally been done more or less “by hand” using
theoretical insights and mathematical and algorithmic con-
structions. Recently, however, this has been attempted with
a new twist: use machine learning / deep-learning to learn
the encoding and/or decoding functions directly [1]–[11]. We
refer to such codes as deep-learned error-correcting codes
(DL-ECC). The approach has been successful, particularly for
channels with feedback [7], [10], [11], but also for point-to-
point Additive White Gaussian Noise (AWGN) channels, one
of the benchmarks for practical code performance [9], [12].
Deep learning provides computational tools for viewing the
task as an optimization problem and solving it efﬁciently by
training a neural network. The rapidly evolving toolkit of
deep learning offers many possible architectures and allows
(approximately) optimal codes to be found by using a training
procedure. For example, for channels with feedback, [7] uses
Recurrent Neural Networks, while [11] uses a transformer-
based architecture. For point-to-point channels, [9] mimics
the architecture of Turbo-codes, replacing convolutional codes
with Convolutional Neural Networks (CNN), while [12] uses
This work was supported by NSF under awards 1934915 and 1900911.
Computing resources for the experiments were provided in part by the NSF
award 1828265 (COMPaaS DLV). The authors are in alphabetic order.non-linear learned components in a Reed-Mueller-like con-
struction. Using deep learning and directly optimizing over
codes raises important new questions:
1) Can one understand deep learning training procedures in
terms of a search process in the space of codes? We suggest
looking at the Fourier expansion of a learned encoder as a
way to both understand the ﬁnal code, and tracking this, as
a way to understand the training dynamics. We propose an
efﬁcient way to ﬁnd the dominant Fourier coefﬁcients using
the Goldreich-Levin algorithm [13].
2) If we minimize a loss function over a set of codes
(those expressible by a given architecture) then what can we
say about the loss landscape [14]? We show that for certain
architectures and loss functions, parity functions appear to be
minimizers.
3) Loss functions are chosen to facilitate the training process
(e.g. binary cross-entropy, BCE), but how do they relate to the
performance of a code (e.g. bit error rate, BER)? We show tight
bounds connecting BCE and BER.
4) If optimization is viewed as working over the two-
dimensional (encoder, decoder) space, what can be said about
the structure of this space? For example, several training
procedures [7], [9] alternate training the encoder and decoder
– is this fundamentally needed, or just a practical way to
improve convergence? Is there a way to decompose the loss
function into an encoder-only and a decoder-only component
and exploit this? We show that there is such a decomposition
and suggest an approach that exploits this.
We attempt to study these issues through a mixture of
theoretical and experimental insights, focusing on the speciﬁc
DL-ECC termed TurboAE [9], which has been one of the
few DL-ECCs for which interpretability studies have been
initiated [15], [16]. Some of the questions we seek to answer
here were inspired by passing remarks in [15] and [16]; we
expand upon these in the next sections. Our experimental ob-
servations raise several algorithmic and theoretical questions.
This research direction aims to connect coding theory with
machine learning interpretability and deep-learning theory.
II. T URBO AE: BASICS AND PAST INTERPRETATIONS
The architecture of the DL-ECC termed “TurboAE” encoder
network [9] is based upon a classical rate1
3Turbo code, with
the three “constituent codes” replaced by CNN blocks fb,θ(·),arXiv:2305.04347v1  [cs.IT]  7 May 2023b∈ {1,2,3}as in Fig. 1 (adapted from [15]). Similarly,
the TurboAE decoder architecture replaces the iterations of
the BCJR decoder by CNNs gφ,1,gφ,2as in Fig. 1 (adapted
from [9]), where yb∈R100(we use bold font for vectors) and
yb=xAE,b+zb, forzbi.i.d. Gaussian noise of mean zero
and variance 1, for each stream b∈{1,2,3}. The network
is trained in an end-to-end fashion to obtain the network
parameters of the encoder and decoder CNNs jointly.
The input to the network is a sequence uof100 bits,
and the output of each block b∈ {1,2,3}is a sequence
xAE,b∈{± 1}100. The network has two versions, TurboAE-
cont (with real-valued encoder outputs, essentially performing
coding and modulation tasks jointly) and TurboAE-binary
(with Boolean encoder outputs which are then modulated for
transmission over an AWGN channel). The power control
modules are omitted, as is the treatment of the boundary ﬁrst
2 and last 2 bits, discussed in [16] (not needed here).
In [15] “interpretation” of TurboAE-binary was attempted
through both exact (non-linear) and approximate (linear, or
parity) approximations of the encoding functions fi,θ(·); these
are provided in Tables I and II in the Appendix. From these,
we see that TurboAE-binary’s encoders are non-recursive, non-
systematic, and f1,θ,f3,θare non-linear and f2,θis a linear
function of the 5 inputs at times j−2,···j+ 2.
In [15], besides ﬁnding the exact and best linear approxi-
mations to the encoder functions, several other “interpretation”
tools were suggested but not deeply explored. Among these is
1) the use of the Fourier representation of Boolean and pseudo-
Boolean functions to better understand the training dynamics,
and 2) the suggestion of using the Goldreich-Levin algorithm
to ﬁnd the largest Fourier coefﬁcient(s) as an approximation
algorithm for the encoder. We expand on these here, and
also look more deeply into the training dynamics, offering an
alternative training to that presented in the original TurboAE
[9]. All code will be posted on github if the paper is accepted.
III. I NTERPRETATIONS THROUGH THE FOURIER LENS
We will later investigate the tracking of Fourier coefﬁcients
(FC) of the encoder as a tool for understanding both the
training dynamics and the loss landscape. We ﬁrst discuss an
approach to estimate the dominant FC of the learned encoding
functions, which may have a large number of inputs.
Changing to the domain xi∈ {± 1}, and letting χS=∏
i∈Sxi, each Boolean ( Fn
2→F2) and pseudo-Boolean
(Fn
2→R) function has a unique Fourier representation [13]
f(x) =∑
S⊆{1,2,···,n}ˆf(S)χS,
where ˆf(S)is termed the FC for set S, and|ˆf(S)|2represents
the Fourier weight.
The Goldreich-Levin algorithm (GL) [17] seeks to output a
list of setsSfor which|ˆf(S)|is larger than a pre-speciﬁed
thresholdγ. It requires “query access”, which in our context
simply means evaluating the neural network on an input.
The theoretical underpinnings are presented in Theorem
1 [13], and our implementation is detailed in the Appendix.
<latexit sha1_base64="PnYoCHduSr2h7O4tfPDSnAUlkMo=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkVI9FLx4r2A9oQ9lsJ+3SzSbsboQS+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJaPZpqgH9GR5CFn1Fipk/WDkKSzQbniVt0FyDrxclKBHM1B+as/jFkaoTRMUK17npsYP6PKcCZwVuqnGhPKJnSEPUsljVD72eLcGbmwypCEsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66UmvPEzLpPUoGTLRWEqiInJ/Hcy5AqZEVNLKFPc3krYmCrKjE2oZEPwVl9eJ+2rqlev1h5qlcZtHkcRzuAcLsGDa2jAPTShBQwm8Ayv8OYkzovz7nwsWwtOPnMKf+B8/gAtaY96</latexit>u<latexit sha1_base64="/KXDt05vbrzY+1rkTETGbW+vEAM=">AAAB83icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeqCB4r2A9oQtlsN+3SzSbsbsQS8je8eFDEq3/Gm//GTZuDtj4YeLw3w8w8P+ZMadv+tkorq2vrG+XNytb2zu5edf+go6JEEtomEY9kz8eKciZoWzPNaS+WFIc+p11/cpP73UcqFYvEg57G1AvxSLCAEayN5KauH6CnQXp1m2WDas2u2zOgZeIUpAYFWoPqlzuMSBJSoQnHSvUdO9ZeiqVmhNOs4iaKxphM8Ij2DRU4pMpLZzdn6MQoQxRE0pTQaKb+nkhxqNQ09E1niPVYLXq5+J/XT3Rw6aVMxImmgswXBQlHOkJ5AGjIJCWaTw3BRDJzKyJjLDHRJqaKCcFZfHmZdM7qznm9cd+oNa+LOMpwBMdwCg5cQBPuoAVtIBDDM7zCm5VYL9a79TFvLVnFzCH8gfX5A86BkYw=</latexit>xAE
<latexit sha1_base64="JXGkx+NGafyorcn3FOKPkTRCaiQ=">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBbBg5RESvVYFcFjBfsBbSyb7aZdutmE3Y1aQv6HFw+KePW/ePPfuG1z0NYHA4/3ZpiZ50WcKW3b31ZuaXlldS2/XtjY3NreKe7uNVUYS0IbJOShbHtYUc4EbWimOW1HkuLA47Tlja4mfuuBSsVCcafHEXUDPBDMZwRrI90nXc9HT73k4vrESdNesWSX7SnQInEyUoIM9V7xq9sPSRxQoQnHSnUcO9JugqVmhNO00I0VjTAZ4QHtGCpwQJWbTK9O0ZFR+sgPpSmh0VT9PZHgQKlx4JnOAOuhmvcm4n9eJ9b+uZswEcWaCjJb5Mcc6RBNIkB9JinRfGwIJpKZWxEZYomJNkEVTAjO/MuLpHladqrlym2lVLvM4sjDARzCMThwBjW4gTo0gICEZ3iFN+vRerHerY9Za87KZvbhD6zPH62Okf0=</latexit>xAE,1
<latexit sha1_base64="3rZTYsyu+9SgdJzsb8p/SOS0BYA=">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBbBg5SkFPVYFcFjBfsBbSyb7aZdutmE3Y1aQv6HFw+KePW/ePPfuG1z0NYHA4/3ZpiZ50WcKW3b31ZuaXlldS2/XtjY3NreKe7uNVUYS0IbJOShbHtYUc4EbWimOW1HkuLA47Tlja4mfuuBSsVCcafHEXUDPBDMZwRrI90nXc9HT73k4vqkkqa9Ysku21OgReJkpAQZ6r3iV7cfkjigQhOOleo4dqTdBEvNCKdpoRsrGmEywgPaMVTggCo3mV6doiOj9JEfSlNCo6n6eyLBgVLjwDOdAdZDNe9NxP+8Tqz9czdhIoo1FWS2yI850iGaRID6TFKi+dgQTCQztyIyxBITbYIqmBCc+ZcXSbNSdk7L1dtqqXaZxZGHAziEY3DgDGpwA3VoAAEJz/AKb9aj9WK9Wx+z1pyVzezDH1ifP68Ukf4=</latexit>xAE,2
<latexit sha1_base64="z2VhO8qecvw4YTwY7P/YP8+dFrQ=">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBbBg5REi3qsiuCxgv2ANpbNdtMu3WzC7kYtIf/DiwdFvPpfvPlv3LY5aOuDgcd7M8zM8yLOlLbtbyu3sLi0vJJfLaytb2xuFbd3GiqMJaF1EvJQtjysKGeC1jXTnLYiSXHgcdr0hldjv/lApWKhuNOjiLoB7gvmM4K1ke6Tjuejp25ycX10kqbdYsku2xOgeeJkpAQZat3iV6cXkjigQhOOlWo7dqTdBEvNCKdpoRMrGmEyxH3aNlTggCo3mVydogOj9JAfSlNCo4n6eyLBgVKjwDOdAdYDNeuNxf+8dqz9czdhIoo1FWS6yI850iEaR4B6TFKi+cgQTCQztyIywBITbYIqmBCc2ZfnSeO47JyWK7eVUvUyiyMPe7APh+DAGVThBmpQBwISnuEV3qxH68V6tz6mrTkrm9mFP7A+fwCwmpH/</latexit>xAE,3
<latexit sha1_base64="Yv3vhztvCyWwLoZO8RPy4NuW4ZI=">AAAB/HicbVBNS8NAEN3Ur1q/oj16CRahgpREinosevFYwX5AE8Jmu2mXbrJhdyKEUP+KFw+KePWHePPfuG1z0NYHA4/3ZpiZFyScKbDtb6O0tr6xuVXeruzs7u0fmIdHXSVSSWiHCC5kP8CKchbTDjDgtJ9IiqOA014wuZ35vUcqFRPxA2QJ9SI8ilnICAYt+WY19HPn3IUxBTytu2Qo4Mw3a3bDnsNaJU5BaqhA2ze/3KEgaURjIBwrNXDsBLwcS2CE02nFTRVNMJngER1oGuOIKi+fHz+1TrUytEIhdcVgzdXfEzmOlMqiQHdGGMZq2ZuJ/3mDFMJrL2dxkgKNyWJRmHILhDVLwhoySQnwTBNMJNO3WmSMJSag86roEJzll1dJ96LhXDaa981a66aIo4yO0QmqIwddoRa6Q23UQQRl6Bm9ojfjyXgx3o2PRWvJKGaq6A+Mzx/zxpRV</latexit>f1,✓(·)
<latexit sha1_base64="XhjW0RtEORO7v2izd1N6mU0LuH8=">AAAB/HicbVDLSsNAFJ34rPUV7dLNYBEqSElKUZdFNy4r2Ac0IUymk3bo5MHMjRBC/RU3LhRx64e482+ctllo64ELh3Pu5d57/ERwBZb1baytb2xubZd2yrt7+weH5tFxV8WppKxDYxHLvk8UEzxiHeAgWD+RjIS+YD1/cjvze49MKh5HD5AlzA3JKOIBpwS05JmVwMsbFw6MGZBpzaHDGM49s2rVrTnwKrELUkUF2p755QxjmoYsAiqIUgPbSsDNiQROBZuWnVSxhNAJGbGBphEJmXLz+fFTfKaVIQ5iqSsCPFd/T+QkVCoLfd0ZEhirZW8m/ucNUgiu3ZxHSQosootFQSowxHiWBB5yySiITBNCJde3YjomklDQeZV1CPbyy6uk26jbl/XmfbPauiniKKETdIpqyEZXqIXuUBt1EEUZekav6M14Ml6Md+Nj0bpmFDMV9AfG5w/1WZRW</latexit>f2,✓(·)
<latexit sha1_base64="bStrKPsYOD03bO4X4xgKTU1qmcA=">AAAB/HicbVDLSsNAFJ34rPUV7dJNsAgVpCRa1GXRjcsK9gFNCJPJpB06yYSZGyGE+ituXCji1g9x5984fSy09cCFwzn3cu89QcqZAtv+NlZW19Y3Nktb5e2d3b198+Cwo0QmCW0TwYXsBVhRzhLaBgac9lJJcRxw2g1GtxO/+0ilYiJ5gDylXowHCYsYwaAl36xEfnFx5sKQAh7XXBIKOPXNql23p7CWiTMnVTRHyze/3FCQLKYJEI6V6jt2Cl6BJTDC6bjsZoqmmIzwgPY1TXBMlVdMjx9bJ1oJrUhIXQlYU/X3RIFjpfI40J0xhqFa9Cbif14/g+jaK1iSZkATMlsUZdwCYU2SsEImKQGea4KJZPpWiwyxxAR0XmUdgrP48jLpnNedy3rjvlFt3szjKKEjdIxqyEFXqInuUAu1EUE5ekav6M14Ml6Md+Nj1rpizGcq6A+Mzx/27JRX</latexit>f3,✓(·)<latexit sha1_base64="SszqgylynoqdSluBuqAHHUtxGgs=">AAACA3icbVC7SgNBFJ2N7/iK2mkzGAQLCbsSVLQRbLSLYBIhu4TZyV0dMju7zNwVwxKw8VdsLBSx9Sfs/Bsnj0KNBwYO55zLnXvCVAqDrvvlFKamZ2bn5heKi0vLK6ultfWGSTLNoc4TmejrkBmQQkEdBUq4TjWwOJTQDLtnA795B9qIRF1hL4UgZjdKRIIztFK7tOkj3GN+oRC0BGaje33qn1C/Jtqlsltxh6CTxBuTMhmj1i59+p2EZzEo5JIZ0/LcFIOcaRRcQr/oZwZSxrvsBlqWKhaDCfLhDX26Y5UOjRJtn0I6VH9O5Cw2pheHNhkzvDV/vYH4n9fKMDoKcqHSDEHx0aIokxQTOiiEdoQGjrJnCeNa2L9Sfss047YRU7QleH9PniSN/Yp3UKleVsunx+M65skW2Sa7xCOH5JSckxqpE04eyBN5Ia/Oo/PsvDnvo2jBGc9skF9wPr4Be+iXYg==</latexit>Interleaver,⇧}TurboAE(Binary) Encoder structures of
TurboAE(Binary) Decoder structure ofRepeat 6 times total<latexit sha1_base64="EpksDt98/ARABhZVVcOXk3wdVIg=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRbBU9ktpYqnghePFeyHtEvJptk2NMkuSVZYlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL4g508Z1v53CxubW9k5xt7S3f3B4VD4+6egoUYS2ScQj1QuwppxJ2jbMcNqLFcUi4LQbTG/nfveJKs0i+WDSmPoCjyULGcHGSo/ZIAhROqzNhuWKW3UXQOvEy0kFcrSG5a/BKCKJoNIQjrXue25s/Awrwwins9Ig0TTGZIrHtG+pxIJqP1scPEMXVhmhMFK2pEEL9fdEhoXWqQhsp8Bmole9ufif109MeO1nTMaJoZIsF4UJRyZC8+/RiClKDE8twUQxeysiE6wwMTajkg3BW315nXRqVa9Rrd/XK82bPI4inME5XIIHV9CEO2hBGwgIeIZXeHOU8+K8Ox/L1oKTz5zCHzifP1wqkBs=</latexit>y2<latexit sha1_base64="7tPL2mu6c5ECfhYpbQ5NMNWwLl0=">AAAB8HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqHgqePFYwX5IG8pmu2mX7m7C7kYIob/CiwdFvPpzvPlv3KY5aOuDgcd7M8zMC2LOtHHdb6e0tr6xuVXeruzs7u0fVA+POjpKFKFtEvFI9QKsKWeStg0znPZiRbEIOO0G09u5332iSrNIPpg0pr7AY8lCRrCx0mM2CEKUDr3ZsFpz624OtEq8gtSgQGtY/RqMIpIIKg3hWOu+58bGz7AyjHA6qwwSTWNMpnhM+5ZKLKj2s/zgGTqzygiFkbIlDcrV3xMZFlqnIrCdApuJXvbm4n9ePzHhtZ8xGSeGSrJYFCYcmQjNv0cjpigxPLUEE8XsrYhMsMLE2IwqNgRv+eVV0rmoe5f1xn2j1rwp4ijDCZzCOXhwBU24gxa0gYCAZ3iFN0c5L86787FoLTnFzDH8gfP5A1qlkBo=</latexit>y1CNN
<latexit sha1_base64="gCXn0TVoW3nt3YoV3eZ7DwfeK9w=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKqHgKePEYwTwgCWF20psMmZ0dZmaFsOQjvHhQxKvf482/cZLsQRMLGoqqbrq7QiW4sb7/7RXW1jc2t4rbpZ3dvf2D8uFR0ySpZthgiUh0O6QGBZfYsNwKbCuNNA4FtsLx3cxvPaE2PJGPdqKwF9Oh5BFn1DqplXXDiKhpv1zxq/4cZJUEOalAjnq//NUdJCyNUVomqDGdwFe2l1FtORM4LXVTg4qyMR1ix1FJYzS9bH7ulJw5ZUCiRLuSlszV3xMZjY2ZxKHrjKkdmWVvJv7ndVIb3fQyLlVqUbLFoigVxCZk9jsZcI3MiokjlGnubiVsRDVl1iVUciEEyy+vkuZFNbiqXj5cVmq3eRxFOIFTOIcArqEG91CHBjAYwzO8wpunvBfv3ftYtBa8fOYY/sD7/AEjaI9t</latexit>p<latexit sha1_base64="HhObgxkq9U/g9kxYMEqyjHtQ0Sg=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqHgqePFYwX5AG8pmO2mXbjZxdyOU0B/hxYMiXv093vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2d++wmV5rF8MJME/YgOJQ85o8ZK7awXhORx2i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5ufOyVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmf1OBlwhM2JiCWWK21sJG1FFmbEJlWwI3vLLq6R1UfUuq7X7WqV+k8dRhBM4hXPw4ArqcAcNaAKDMTzDK7w5ifPivDsfi9aCk88cwx84nz8k7Y9u</latexit>q<latexit sha1_base64="/GfEsdBXMJIKTaHgwFi5Mick6Kg=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqHgqePFY0X5AG8pmO2mXbjZhdyOU0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2d++wmV5rF8NJME/YgOJQ85o8ZKD72E98sVt+rOQVaJl5MK5Gj0y1+9QczSCKVhgmrd9dzE+BlVhjOB01Iv1ZhQNqZD7FoqaYTaz+anTsmZVQYkjJUtachc/T2R0UjrSRTYzoiakV72ZuJ/Xjc14bWfcZmkBiVbLApTQUxMZn+TAVfIjJhYQpni9lbCRlRRZmw6JRuCt/zyKmldVL3Lau2+Vqnf5HEU4QRO4Rw8uII63EEDmsBgCM/wCm+OcF6cd+dj0Vpw8plj+APn8wdQQ43O</latexit>⇡
<latexit sha1_base64="QhsFaUEkxFkt+TzD/ESbJ52Up+A=">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBahXkoipYqnghePFewHNLFstpt26WYTdjdKCP0fXjwo4tX/4s1/47bNQVsfDDzem2Fmnh9zprRtf1uFtfWNza3idmlnd2//oHx41FFRIgltk4hHsudjRTkTtK2Z5rQXS4pDn9OuP7mZ+d1HKhWLxL1OY+qFeCRYwAjWRnpwY1bNXD9A6cCZng/KFbtmz4FWiZOTCuRoDcpf7jAiSUiFJhwr1XfsWHsZlpoRTqclN1E0xmSCR7RvqMAhVV42v3qKzowyREEkTQmN5urviQyHSqWhbzpDrMdq2ZuJ/3n9RAdXXsZEnGgqyGJRkHCkIzSLAA2ZpETz1BBMJDO3IjLGEhNtgiqZEJzll1dJ56LmNGr1u3qleZ3HUYQTOIUqOHAJTbiFFrSBgIRneIU368l6sd6tj0VrwcpnjuEPrM8fdgaR0g==</latexit>⇡(y1)
<latexit sha1_base64="8V8bL60uMBrELOSAg0rUjDVCtII=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRbBU9nVUsVTwYvHCvZD2qVk02wbmmSXJCssS3+FFw+KePXnePPfmLZ70NYHA4/3ZpiZF8ScaeO6305hbX1jc6u4XdrZ3ds/KB8etXWUKEJbJOKR6gZYU84kbRlmOO3GimIRcNoJJrczv/NElWaRfDBpTH2BR5KFjGBjpcesH4QoHVxOB+WKW3XnQKvEy0kFcjQH5a/+MCKJoNIQjrXueW5s/Awrwwin01I/0TTGZIJHtGepxIJqP5sfPEVnVhmiMFK2pEFz9fdEhoXWqQhsp8BmrJe9mfif10tMeO1nTMaJoZIsFoUJRyZCs+/RkClKDE8twUQxeysiY6wwMTajkg3BW355lbQvql69WruvVRo3eRxFOIFTOAcPrqABd9CEFhAQ8Ayv8OYo58V5dz4WrQUnnzmGP3A+fwBdr5Ac</latexit>y3
<latexit sha1_base64="ocDOgrmKCAL5C1RK0Ik6xt34fhI=">AAAB8XicbVBNS8NAEJ34WetX1aOXxSJ4kJJIUfFU8OKxgv3ANpTNdtIu3WzC7kYoof/CiwdFvPpvvPlv3LY5aOuDgcd7M8zMCxLBtXHdb2dldW19Y7OwVdze2d3bLx0cNnWcKoYNFotYtQOqUXCJDcONwHaikEaBwFYwup36rSdUmsfywYwT9CM6kDzkjBorPQ56WTcZ8nNv0iuV3Yo7A1kmXk7KkKPeK311+zFLI5SGCap1x3MT42dUGc4ETordVGNC2YgOsGOppBFqP5tdPCGnVumTMFa2pCEz9fdERiOtx1FgOyNqhnrRm4r/eZ3UhNd+xmWSGpRsvihMBTExmb5P+lwhM2JsCWWK21sJG1JFmbEhFW0I3uLLy6R5UfEuK9X7arl2k8dRgGM4gTPw4ApqcAd1aAADCc/wCm+Odl6cd+dj3rri5DNH8AfO5w8yJJCX</latexit>g ,1<latexit sha1_base64="EojEitnnbN7xS6Zybi3+L8zIyMU=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgQcJuCCqeAl48RjAPTEKYncwmQ2Znl5leISz5Cy8eFPHq33jzb5wke9DEgoaiqpvuLj+WwqDrfju5tfWNza38dmFnd2//oHh41DRRohlvsEhGuu1Tw6VQvIECJW/HmtPQl7zlj29nfuuJayMi9YCTmPdCOlQiEIyilR6H/bQbj8RFZdovltyyOwdZJV5GSpCh3i9+dQcRS0KukElqTMdzY+ylVKNgkk8L3cTwmLIxHfKOpYqG3PTS+cVTcmaVAQkibUshmau/J1IaGjMJfdsZUhyZZW8m/ud1Egyue6lQcYJcscWiIJEEIzJ7nwyE5gzlxBLKtLC3EjaimjK0IRVsCN7yy6ukWSl7l+XqfbVUu8niyMMJnMI5eHAFNbiDOjSAgYJneIU3xzgvzrvzsWjNOdnMMfyB8/kDM6mQmA==</latexit>g ,2<latexit sha1_base64="h/+1reIGMh52fYselivJi2c8jjs=">AAAB+HicbVBNSwMxEJ31s9aPrnr0EiyCF8uuFBVPBS8eK9gP6K4lm6ZtaJJdkqxQl/4SLx4U8epP8ea/MW33oK0PBh7vzTAzL0o408bzvp2V1bX1jc3CVnF7Z3ev5O4fNHWcKkIbJOaxakdYU84kbRhmOG0nimIRcdqKRjdTv/VIlWaxvDfjhIYCDyTrM4KNlbpuKUjYQxZogTlHZ/6k65a9ijcDWiZ+TsqQo951v4JeTFJBpSEca93xvcSEGVaGEU4nxSDVNMFkhAe0Y6nEguowmx0+QSdW6aF+rGxJg2bq74kMC63HIrKdApuhXvSm4n9eJzX9qzBjMkkNlWS+qJ9yZGI0TQH1mKLE8LElmChmb0VkiBUmxmZVtCH4iy8vk+Z5xb+oVO+q5dp1HkcBjuAYTsGHS6jBLdShAQRSeIZXeHOenBfn3fmYt644+cwh/IHz+QMqp5LA</latexit>⇡ 1CNN
<latexit sha1_base64="gCXn0TVoW3nt3YoV3eZ7DwfeK9w=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKqHgKePEYwTwgCWF20psMmZ0dZmaFsOQjvHhQxKvf482/cZLsQRMLGoqqbrq7QiW4sb7/7RXW1jc2t4rbpZ3dvf2D8uFR0ySpZthgiUh0O6QGBZfYsNwKbCuNNA4FtsLx3cxvPaE2PJGPdqKwF9Oh5BFn1DqplXXDiKhpv1zxq/4cZJUEOalAjnq//NUdJCyNUVomqDGdwFe2l1FtORM4LXVTg4qyMR1ix1FJYzS9bH7ulJw5ZUCiRLuSlszV3xMZjY2ZxKHrjKkdmWVvJv7ndVIb3fQyLlVqUbLFoigVxCZk9jsZcI3MiokjlGnubiVsRDVl1iVUciEEyy+vkuZFNbiqXj5cVmq3eRxFOIFTOIcArqEG91CHBjAYwzO8wpunvBfv3ftYtBa8fOYY/sD7/AEjaI9t</latexit>p<latexit sha1_base64="gCXn0TVoW3nt3YoV3eZ7DwfeK9w=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKqHgKePEYwTwgCWF20psMmZ0dZmaFsOQjvHhQxKvf482/cZLsQRMLGoqqbrq7QiW4sb7/7RXW1jc2t4rbpZ3dvf2D8uFR0ySpZthgiUh0O6QGBZfYsNwKbCuNNA4FtsLx3cxvPaE2PJGPdqKwF9Oh5BFn1DqplXXDiKhpv1zxq/4cZJUEOalAjnq//NUdJCyNUVomqDGdwFe2l1FtORM4LXVTg4qyMR1ix1FJYzS9bH7ulJw5ZUCiRLuSlszV3xMZjY2ZxKHrjKkdmWVvJv7ndVIb3fQyLlVqUbLFoigVxCZk9jsZcI3MiokjlGnubiVsRDVl1iVUciEEyy+vkuZFNbiqXj5cVmq3eRxFOIFTOIcArqEG91CHBjAYwzO8wpunvBfv3ftYtBa8fOYY/sD7/AEjaI9t</latexit>p
<latexit sha1_base64="HhObgxkq9U/g9kxYMEqyjHtQ0Sg=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqHgqePFYwX5AG8pmO2mXbjZxdyOU0B/hxYMiXv093vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2d++wmV5rF8MJME/YgOJQ85o8ZK7awXhORx2i9X3Ko7B1klXk4qkKPRL3/1BjFLI5SGCap113MT42dUGc4ETku9VGNC2ZgOsWuppBFqP5ufOyVnVhmQMFa2pCFz9fdERiOtJ1FgOyNqRnrZm4n/ed3UhNd+xmWSGpRssShMBTExmf1OBlwhM2JiCWWK21sJG1FFmbEJlWwI3vLLq6R1UfUuq7X7WqV+k8dRhBM4hXPw4ArqcAcNaAKDMTzDK7w5ifPivDsfi9aCk88cwx84nz8k7Y9u</latexit>q<latexit sha1_base64="gCXn0TVoW3nt3YoV3eZ7DwfeK9w=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKqHgKePEYwTwgCWF20psMmZ0dZmaFsOQjvHhQxKvf482/cZLsQRMLGoqqbrq7QiW4sb7/7RXW1jc2t4rbpZ3dvf2D8uFR0ySpZthgiUh0O6QGBZfYsNwKbCuNNA4FtsLx3cxvPaE2PJGPdqKwF9Oh5BFn1DqplXXDiKhpv1zxq/4cZJUEOalAjnq//NUdJCyNUVomqDGdwFe2l1FtORM4LXVTg4qyMR1ix1FJYzS9bH7ulJw5ZUCiRLuSlszV3xMZjY2ZxKHrjKkdmWVvJv7ndVIb3fQyLlVqUbLFoigVxCZk9jsZcI3MiokjlGnubiVsRDVl1iVUciEEyy+vkuZFNbiqXj5cVmq3eRxFOIFTOIcArqEG91CHBjAYwzO8wpunvBfv3ftYtBa8fOYY/sD7/AEjaI9t</latexit>pSigmoid<latexit sha1_base64="0hfPG4k14LPeJ1LkBl9vDXTNJFU=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKqHgKePEYwTwgu4TZyWwyZPbhTG8gLPsdXjwo4tWP8ebfOEn2oIkFDUVVN91dfiKFRtv+tkpr6xubW+Xtys7u3v5B9fCoreNUMd5isYxV16eaSxHxFgqUvJsoTkNf8o4/vpv5nQlXWsTRI04T7oV0GIlAMIpG8jLXD4g7opiled6v1uy6PQdZJU5BalCg2a9+uYOYpSGPkEmqdc+xE/QyqlAwyfOKm2qeUDamQ94zNKIh1142PzonZ0YZkCBWpiIkc/X3REZDraehbzpDiiO97M3E/7xeisGNl4koSZFHbLEoSCXBmMwSIAOhOEM5NYQyJcythI2oogxNThUTgrP88ippX9Sdq/rlw2WtcVvEUYYTOIVzcOAaGnAPTWgBgyd4hld4sybWi/VufSxaS1Yxcwx/YH3+AAJUkj8=</latexit>ˆuFig. 1: Above is the rate R=1
3(u∈F100
2,xAE,j∈{± 1}100)
TurboAE-binary encoder structure. Functions fj,θ(·)are the con-
stituent codes implemented as CNNs. Below is the high level decoder
structure of TurboAE remade from [9]. The parameters of the CNNs
gφ,1,gφ,2are trained. The noisy channel outputs of the three encoded
streams x1,x2,x3are given by y1,y2,y3. The interleaver is π(and
its inverseπ−1). The decoder produces probabilities that each input
bit is 0or1, denoted by ˆu.
We explore the applicability of this algorithm in estimating the
largest FCs of the encoder of a deep-learned error-correcting
code (ﬁrst proposed for this purpose in [15]), an alternative
method to that used in [15] for ﬁnding the best parity-
approximation. The algorithm has been used in theoretical
domains such as cryptography [18], learning theory [19], and
coding theory [20], [21] (as a randomized list decoder for
Reed-Mueller RM(1,m) codes), but practical implementations
and experiments appear limited. We explore some practical
aspects of this algorithm.
Theorem 1. [13] Given query access to f:{±1}n→{± 1},
givenγ,δ> 0, there is apoly(n,1
γlog1
δ)-time algorithm that
outputs a list L={S1,···,Sm}such that (1) if|ˆf(S)|≥γ,
thenS∈L, and (2) if S∈L, then|ˆf(S)|≥γ
2holds with
probability 1−δ.
The theorem requires γto be speciﬁed in advance, which
holds in several applications. In general, γcan be expo-
nentially small (e.g., for bent functions [22] where every
coefﬁcient is 2−n/2forninputs), but it is polynomial, e.g., for
functions with small |L1|-norm. Ifγis too high then nothing
is returned and if it is too low then the running time increases
and more coefﬁcients are returned than desired. We are not
aware of any work which calculates explicit constants for the
number of queries needed in Theorem 1.
We apply GL to TurboAE-binary, exploring its feasibility
and the number of queries needed. Previous work [15], [16] is
based on the CNN architecture which suggests that the func-
tions in question depend on 9 (TurboAE-cont) or 5 (TurboAE-
binary) variables. Here we do not use such a priori knowledge.
Given each TurboAE-binary constituent code fi,θ :
{±1}100→ {± 1}100, we randomly select one output bit.Fig. 2: Goldreich-Levin approximation for TurboAE-binary.
We implemented a heuristic procedure for determining γfor
each block and computing the minimum number of queries
for each block to get the correct result. Details are given in
the Appendix. How to best pick γand a minimal number of
queries in a principled way is an interesting open question.
Table in Fig. 2 shows the experimental results. The number
of queries refers to the number of function evaluations for
estimating a single expectation.
IV. T RAINING DYNAMICS :EVOLUTION OF FOURIER
COEFFICIENTS (FC)
Continuing the theme of analyzing Boolean functions in
Fourier space, we explore Fourier representation as a tool for
understanding the training dynamics and the loss landscape.
A. Dominance and stability of a few Fourier coefﬁcients
The trained TurboAE was found to have a few dominant
Fourier coefﬁcients [15], [16]. One can hypothesize that this
might be a general phenomenon when training this network. To
investigate this question, we trained TurboAE-binary several
times from scratch as described in [9]. At convergence, the
Fourier space appears to almost always be dominated by a
few large FCs. In the randomly selected examples in Fig. 3(a)
95% of the total energy (sum of ˆf2(S)) is for at most 5 (out
of 32) FCs.
Furthermore we observed that at initialization, the dominant
FCs almost always correspond to one bit parities. However,
with training, higher degree parities emerge as dominant, see
Fig. 3(b). Although the setups differ, these observations may
be related to recently observed staircase properties [23].
We also trained TurboAE multiple times starting with the
same initialization of the neural net weights to evaluate how
stable the training process is at convergence. We found that it
issomewhat stable w.r.t. the dominant Fourier coefﬁcients,
as shown for some runs on Fig. 3(c). This begs further
questions about the loss landscape of Turbo-like codes, which
we propose to study also using a Fourier lens next.
B. Local Optimality of parities for Turbo Codes
The loss landscape of the TurboAE network is a function
over 150,000 parameters, depending on the network. The
network parameters of the encoder determine the FCs of the
(a)Largest FCs of Block 1 with energy ≥95% over 4 independent
training sessions. Always a few FCs dominate the Fourier space.
(b)FCs of block 3 over different epochs of a training session.
Initially, 1-bit parities always dominate, but later higher degree
parities emerge.
(c)FCs of block 2 after 4 training sessions with same initialization.
The most dominant FC is stable across all runs.
Fig. 3: Progression of TurboAE’s Fourier Coefﬁcients with Training.
Similar behaviors seen for all the blocks.
encoding function, and so local minima of the latter can be
helpful for understanding the former. Thus we study the loss
landscape in terms of the Fourier parameterization with 512
parameters, ﬁxing the decoder to BCJR.
To study the loss landscape of generic non-recursive Turbo
codes in Fourier space, we constructed a parametric Turbo
code of block length L(=10), memory 4, parameterized not
by CNNs, but by the FC of its constituent codes. It is not
clear whether the encoder part of the TurboAE network can
implement any triple of 5-variable Boolean functions, there-
fore the observations might not transfer directly to TurboAE.
For each block b∈{1,2,3}, use a pseudo-Boolean function
fb,Θb:{−1,1}5→R, as the constituent code fb,θ(·)of Fig.
1, which is completely determined, hence parameterized by
its FC Θb=ˆfb. These form the Turbo encoder EΘ. We use
a standard six iteration BCJR decoder for EΘas the Decoder
DΘ. We use expected binary cross entropy ( BCE ) between
the input and the decoded output as lossL, i.e.
L(Θ) = E
x∼UL
z∼NL[
1
L∑
i∈[L]BCE(
xi,DΘ(
EΘ(x) +z)
i)]
.
Herezis the i.i.d. noise sampled from AWGN channel i.e.
N0,σ, whereσcorresponds to SNR = 1dB. We control the
power by keeping the squared sum of the FCs to be 1, which
constrains the average power of each bit to also be 1 due to(a) Line Joining two parities. Local
minima at 0 and 1 show that both
parities are locally optimal on this
line.
(b) Line Joining two non-parity Bent
functions. Neither of the two are lo-
cally optimal.
Fig. 4: BCE landscape on parametric line λΘ′′+ (1−λ)Θ′joining
combinations of parity/non-parity functions. λ= 0,1correspond to
Θ′,Θ′′respectively.
Parseval’s Theorem: E
x∼U5[
fb(x)2]

avg power=∑
S⊆[5]ˆfb(S)2=∥Θb∥2
2.
Our hypothesis is that triples of different parity functions
are all local minima, but there are other triples that are not
local minima. We ran the following experiment, with results
consistent with the hypothesis.
Pick Θ′,Θ′′corresponding to triples of different parities
χΘ′= (χ′
1,χ′
2,χ′
3)andχΘ′′= (χ′′
1,χ′′
2,χ′′
3)respectively.
EvaluateLover several points on the line joining Θ′andΘ′′
with power re-normalization. Evaluating a point representing
a pseudo-Boolean function involves running BCJR for that
function and computing the BCE. We found that Θ′andΘ′′
were always local optima on this line. On the other hand, the
triple formed by three copies of the bent function x1x2⊕x3x4
on different subsets of 5 variables is not a local minimum. The
results are illustrated in Fig. 4, and Fig 8 in the Appendix.
V. T RAINING LOSS FUNCTIONS : BCE AND BER
We now investigate the implications of optimizing BCE
from theoretical and empirical perspectives. We consider op-
timizing an encoder function f:Fk
2→SwhereS⊂Rnis
bounded and R=k/n is our code rate. We take U∼Unif[Fk
2]
andY∈Rnto be random variables representing the input and
received sequence, respectively. Note Ydepends on f. Our
optimization problem is then:
Problem 1. Find encoder f:Fk
2→Sand soft decoder g∈
Rn→[0,1]kthat minimizes the expected BCE, C(f,g), where
C(f,g) =E
1
kk∑
i=1−Uilggi(Y)−(1−Ui) lg(1−gi(Y))]
.
A. Theoretical Analysis of BCE Minimization
We re-write the BCE as
C(f,g) =1
kk∑
i=1E[DKL(P[Ui= 1|Y]||gi(Y))] +H(Ui|Y)
(1)
where H(Ui|Y)is the conditional entropy and E[DKL(P[Ui=
1|Y]||gi(Y))is the expected Kullback-Leibler divergence with
the expectation taken over the distribution of received se-
quences. Note that for a ﬁxed channel, H(Ui|Y)only dependson the encoder f, and for a ﬁxed encoder the KL-Divergence
termE[DKL(P[Ui= 1|Y]||gi(Y))only depends on the
decoder,g. We can easily establish the following proposition
(proofs in Appendix):
Proposition 2. Consider a ﬁxed encoder f:Fk
2→S. The
decoderg:S→[0,1]kdeﬁned elementwise as gi(y):=
P(Ui= 1|Y=y)is the unique a.s. minimizer of C(f,g).
Note that this is exactly the soft-output MAP decoder, a
minimizer of the BER B(f,g), for a ﬁxed encoder f. This
means that, given f, minimizing BCE and BER both reduce to
ﬁnding a soft-MAP decoder for f. Thus we denote C(f),B(f)
to beC(f,gMAP (f)),B(f,gMAP (f)), wheregMAP (f)is the
soft-MAP decoder for f. Hence, ﬁnding a decoder that min-
imizes BCE ﬁnds a decoder that minimizes BER. The same
does nothold for the general problem of ﬁnding an encoder-
decoder pair; a memoryless counter-example (asymmetric in
the input, unlike the BSC or AWGN) is shown in the Ap-
pendix. The best bounds possible relating BER and BCE are
the following.
Proposition 3. LetBi,Cidenote the BER and BCE respec-
tively on the ithinput bit. Then for all choices of fand for
alli∈[k],2Bi(f)≤Ci(f)≤H2(Bi(f)), and in particular
2B(f)≤C(f)≤1
kk∑
i=1H2(Bi(f))
where H2(p)denotes the binary entropy function with parame-
terp∈[0,1]. Furthermore, these bounds are tight in the sense
that for any BER t∈[0,1
2]and side of the bound, there exists
a channel and an encoder which makes that side an equality.
B. Empirical Application of BCE Decomposition
In light of equation (1), it seems reasonable to decouple
the optimization process: optimize the encoder conditional
entropy ﬁrst, then optimize a soft decoder to minimize its
KL-divergence with respect to the good encoder. The major
obstacle is estimating the conditional entropy of the encoder
(or equivalently, estimating the KL-divergence of the decoder).
We opt for a naive approach to get around this issue, but
more sophisticated approaches can be found in [24], [25].
If we were optimizing convolutional codes, we could take
advantage of the fact that the BCJR [26] is a MAP decoder.
However, in the case of general Turbo codes, we have no
such efﬁcient MAP decoder. To get around this we propose
training a Turbo-like encoder (top of Fig. 1, but allowing for
real-valued outputs) at short block lengths, k= 16 , using
a brute-force marginalization MAP decoder. The encoder is
trained with many choices of interleaver so it may generalize
well at larger block lengths. Then, we train a neural decoder
on the same encoder at our block length k= 100 . This is
in direct contrast to the approach in [9], where the authors
alternated between training the encoder and decoder until they
jointly converged. See Algorithm 2 in the Appendix for a more
precise formulation of the training procedure.1) Methods: Like [9], we train a non-systematic, non-
recursive turbo code of rate R=1
3for use at block length
100. Rather than training a neural encoder, we directly train
the input-output table of a window w= 5 (memory 4),
possibly nonlinear, automata encoder, borrowing terminology
from [27]. That is, we parameterize our encoder by the outputs
of a function h:Fw
2→R3. This function is slid over the
input bits in Fk
2as in a non-recursive convolutional encoder.
The third stream is convolved over an interleaved copy of
the input instead. his initialized with a normal distribution
of mean 0 and variance 1. We also tried initializing hwith
a parity function, but found it to be at best as good as the
normal initialization. Details can be found in the Appendix.
To enforce the power constraint we use a different method
than in [9]. We instead analytically compute power using h.
We then center and rescale hafter each gradient update so that
its power is 1. Precise derivation of this power normalization
can be found in the Appendix. This helped reduce much of
the noise in the training process introduced by the power-
normalization in [9]. This method could also be applied when
training a neural encoder as in [9]. Once we had a trained
encoder, we then trained a neural decoder at larger block
length with the same architecture as in [9].
2) Results: The encoder converges fairly quickly (200
steps). The training curve is included in the Appendix
(Fig. 11). Over several training runs, we were unable to ﬁnd an
encoder with as low a conditional entropy as TurboAE-cont.
The discrepancy may come: (1) our encoder was trained at a
block length of 16, while TurboAE-cont at block length 100,
and (2) TurboAE-cont was trained by alternating back-and-
forth between optimizing the encoder and the decoder, which
may have avoided local optima our training scheme runs into.
We show the evolution of the FC for our ﬁnal encoder
in Fig. 5, and the coefﬁcient evolution of another trained
encoder in the Appendix. By the end, only a few dominant
FC remain, echoing what we saw in Section IV-A. The FC
change signiﬁcantly from the initialization.
When training the decoder, our optimization proceeds rel-
atively quickly compared to TurboAE-cont. See Fig. 11 in
the Appendix for the training curve. The total number of
steps required for our procedure is only 300,500 whereas
TurboAE-cont requires 480,000. In Fig. 6 we see that the
performance of our encoder-decoder is slightly worse than
TurboAE-cont above SNR 1.0. Observe, if we replace our
decoder with BCJR, the performance is almost exactly the
same. The decoder may have learned a BCJR-like decoding
algorithm which proved to be a local minimum. Nonetheless,
our encoder-decoder pair suggests that our training scheme is
a viable optimization strategy.
VI. C ONCLUSIONS
We presented several new tools which may help in the
interpretation of training aspects of DL-ECCs, including 1)
the application of the Goldreich-Levin algorithm to ﬁnding
the best parity / linear approximation to a black-box encoding
function, where its efﬁciency is useful when there are a
Fig. 5: Evolution of FC during training of the encoder from
Figure 11. The largest coefﬁcients are marked and their corresponding
parity is annotated in the respective subplot. Note how a few dominant
coefﬁcients emerge and persist during training.
Fig. 6: BER performance of (1) a benchmark RSC turbo code,
(2) our trained encoder paired with BCJR, (3) our trained encoder
paired with our neural decoder, (4) TurboAE-cont. TurboAE-cont
tends to perform worse at SNRs below 0.5, while showing signiﬁcant
outperformance at SNRs above 3.0.
large number of input variables but perhaps a more local
and/or sparse structure (outputs depend only on a few of
those inputs each) of the ﬁnal learned code; 2) the use of
FC to understand the loss landscape when training DL-ECCs
and also as a possible parameterization for learning codes;
3) observations relating the BCE and BER and a principled
alternative approach for training / optimizing DL-ECCs. While
our experiments showed the viability of our alternate training
scheme, there are still many more aspects to explore. In
particular, with good estimation of the conditional entropy of
an encoder at larger block lengths, we expect performance will
improve. In addition, the neural decoder of [9] was designed
to mimic an iterative BCJR decoder. However, designing the
neural network to mimic exact inference algorithms (e.g. a
junction tree [28]) could lead to a better approximation of the
MAP decoder. From a bigger-picture perspective, we hope that
this decomposition and the usage of Fourier coefﬁcients both
as an alternative representation and as a tool for understanding
training, will lead to a more principled approach toward the
training of deep-learned error-correcting codes. This couldopen doors to a more systematic way of ﬁnding such codes
for different channels.
REFERENCES
[1] H. Kim, S. Oh, and P. Viswanath, “Physical Layer Communication via
Deep Learning,” IEEE Journal on Selected Areas in Information Theory ,
vol. 1, no. 1, pp. 5–18, 2020.
[2] Y . Jiang et al. , “Learn codes: Inventing low-latency codes via recurrent
neural networks,” IEEE Journal on Selected Areas in Information
Theory , vol. 1, no. 1, pp. 207–216, 2020.
[3] T. J. O’Shea, K. Karra, and T. C. Clancy, “Learning to communicate:
Channel auto-encoders, domain speciﬁc regularizers, and attention,”
in2016 IEEE International Symposium on Signal Processing and
Information Technology (ISSPIT) , 2016, pp. 223–228.
[4] Y . Jiang et al. , “MIND: Model Independent Neural Decoder,” in 2019
IEEE 20th International Workshop on Signal Processing Advances in
Wireless Communications (SPAWC) , Jul. 2019, pp. 1–5.
[5] J. Whang et al. , “Neural Distributed Source Coding,” May 2022.
[Online]. Available: http://arxiv.org/abs/2106.02797
[6] R. K. Mishra et al. , “Distributed Interference Alignment for K-user
Interference Channels via Deep Learning,” in 2021 IEEE International
Symposium on Information Theory (ISIT) , Jul. 2021, pp. 2614–2619.
[7] H. Kim et al. , “Deepcode: Feedback codes via deep learning,” IEEE
Journal on Sel. Areas in Inf. Theory , vol. 1, no. 1, pp. 194–206, 2020.
[8] Y . Jiang et al. , “Joint channel coding and modulation via deep learning,”
in2020 IEEE SPAWC , 2020, pp. 1–5.
[9] ——, “Turbo autoencoder: Deep learning based channel codes for
point-to-point communication channels,” in Proceedings of the 33rd
International Conference on Neural Information Processing Systems ,
Dec. 2019, pp. 2758–2768.
[10] K. Chahine, R. Mishra, and H. Kim, “Inventing Codes for Channels
with Active Feedback via Deep Learning,” IEEE Journal on Selected
Areas in Information Theory , pp. 1–1, 2022.
[11] E. Ozfatura et al. , “All you need is feedback: Communication with
block attention feedback codes,” IEEE Journal on Selected Areas in
Information Theory , pp. 1–1, 2022.
[12] A. V . Makkuva et al. , “Ko codes: inventing nonlinear encoding and
decoding for reliable wireless communication via deep-learning,” ICML ,
2021.
[13] R. O’Donnell, Analysis of boolean functions . Cambridge University
Press, 2014.
[14] H. Li et al. , “Visualizing the loss landscape of neural nets,” Advances
in neural information processing systems , vol. 31, 2018.
[15] N. Devroye et al. , “Interpreting Deep-Learned Error-Correcting Codes,”
in2022 IEEE International Symposium on Information Theory (ISIT) ,
Jun. 2022, pp. 2457–2462.
[16] ——, “Evaluating interpretations of deep-learned error-correcting
codes,” in 2022 60th Annual Allerton Conference on Communication,
Control, and Computing (Allerton) , Sep. 2022.
[17] O. Goldreich and L. A. Levin, “A hard-core predicate for all one-way
functions,” in Proceedings of the twenty-ﬁrst annual ACM symposium
on Theory of computing , 1989, pp. 25–32.
[18] J. H ˚astad et al. , “A pseudorandom generator from any one-way func-
tion,” SIAM Journal on Computing , vol. 28, no. 4, pp. 1364–1396, 1999.
[19] E. Kushilevitz and Y . Mansour, “Learning decision trees using the fourier
spectrum,” in Proceedings of the twenty-third annual ACM symposium
on Theory of computing , 1991, pp. 455–464.
[20] A. Akavia, S. Goldwasser, and S. Safra, “Proving hard-core predicates
using list decoding,” in FOCS , vol. 44. Citeseer, 2003, pp. 146–159.
[21] A. S. Abdouli et al. , “The Goldreich-Levin algorithm with reduced
complexity,” in Thirteenth International Workshop on Algebraic and
Combinatorial Coding Theory (ACCT’12) , Pomorie, Bulgaria, Jun.
2012, pp. 7–14.
[22] O. S. Rothaus, “On “bent” functions,” Journal of Combinatorial Theory,
Series A , vol. 20, no. 3, pp. 300–305, May 1976.
[23] E. Abbe et al. , “The staircase property: How hierarchical structure can
guide deep learning,” in Advances in Neural Information Processing
Systems , vol. 34. Curran Associates, Inc., 2021, pp. 26 989–27 002.
[24] B. Poczos and J. Schneider, “Nonparametric Estimation of Conditional
Information and Divergences,” in Proceedings of the Fifteenth Inter-
national Conference on Artiﬁcial Intelligence and Statistics , 2012, pp.
914–923.[25] L. Paninski, “Estimation of entropy and mutual information,” Neural
Computation , vol. 15, no. 6, p. 1191–1253, Jun 2003.
[26] L. Bahl et al. , “Optimal decoding of linear codes for minimizing symbol
error rate (corresp.),” IEEE Transactions on Information Theory , vol. 20,
no. 2, pp. 284–287, 1974.
[27] L. Bazzi, M. Mahdian, and D. A. Spielman, “The minimum distance of
turbo-like codes,” IEEE Transactions on Information Theory , vol. 55,
no. 1, p. 6–15, Jan 2009.
[28] E. Castillo, J. M. Guti ´errez, and A. S. Hadi, Expert Systems and
Probabilistic Network Models , ser. Monographs in Computer Science.
New York, NY: Springer, 1997.
APPENDIX A
INTERPRETATIONS OF TURBO AE- BINARY FROM [15]
Block # Approximate expressions for output # i
1 1⊕u1⊕u2⊕u3⊕u4⊕u5
2u1⊕u3⊕u4⊕u5
3 Solution 1:u1⊕u2⊕u4
Solution 2: 1⊕u1⊕u2⊕u3⊕u4
Solution 3: 1⊕u1⊕u2⊕u4⊕u5
Solution 4: 1⊕u1⊕u2⊕u3⊕u4⊕u5
whereu1=xi+2,u2=xi+1,u3=xj,
u4=xi−1,u5=xi−2,x=inputs
TABLE I: Best afﬁne approximations for TurboAE-binary’s encoder.
Block 3 has four equally good approximations, from [15].
Block # Expression for output # i∈{3,4,···,98}
1 1⊕u1⊕¯u2⊕u3⊕¯u4⊕u5
⊕¯u2u3¯u4⊕u1¯u2u3¯u4u5
2u1⊕u3⊕u4⊕u5
3u1⊕u2⊕u4⊕¯u3¯u5
whereu1=xi+2,u2=xi+1,u3=xi,
u4=xi−1,u5=xi−2,x=inputs
TABLE II: Exact expressions for TurboAE-binary encoder’s non-
boundary bits, taken from [15].
APPENDIX B
IMPLEMENTATION OF THE GOLDREICH -LEVIN ALGORITHM
The Goldreich-Levin algorithm aims to ﬁnd sets Swith
Fourier coefﬁcient magnitude |ˆf(S)|> γ. The details of the
Goldreich-Levin algorithm are shown in Algorithm 1 [13].
To run this algorithm in practice, one needs to know γand
ensure that enough queries are made. In our application, we
simply want to ﬁnd the set with largest Fourier coefﬁcient
magnitude (or multiple sets if they all have roughly equal
Fourier coefﬁcient magnitudes). Thus, there is no prior-ﬁxed
γin existing applications. We detail how we experimentally
selectedγand the number of queries here without prior
knowledge.
The problem is one needs to jointly select γand the number
of queries. We could envision using a binary search for γ,
where for each γwe start with a small number of queries and
double it until we have a low variance in the output sets (which
are random given the randomzied nature of the algorithm).Algorithm 1 Goldreich-Levin Algorithm (length of input
sequencen, query function f, thresholdγ, conﬁdence δ)
1:Initialization: k= 0,U=∅
2:Randomly generate a list L←(∗,∗,···,∗), which is a
collection of sets such that L={U∪T:T⊆[n]\[k]}
3:foreachk∈[1,n]do
4: foreachB∈L,B= (a1,···,ak−1,∗,···,∗)do
5: LetBak= (a1,···,ak,∗,···,∗)forak= 0,1
6: Estimate the Fourier weight W(Bak)within±γ2
4
with probability at least 1−δ
7: RemoveBfromL
8: AddBakto L if the estimated weight W(Bak)≥γ2
2
9: end for
10:end for
11:Output: L
What we did in practice here was the following heuristic
approach: we ﬁrst ﬁx the number of queries to a reasonably
large number; in our experiments we took 800 queries (still
small compared to the 2100input space).1We ﬁrst setγ= 0.5
and ran GL to see whether a single set has a large Fourier
coefﬁcient. In our example, for γ > 0.5, here we test γ∈
{0.75,0.9}, only Block 1 and Block 2 have stable outputs
(stable means the same output set is consistently produced
when the randomized GL runs several times with different
initializations). This indicates there is a single dominant parity.
If the algorithm does not return a result for γ >0.5, theγis
lowered and a binary search on γ∈(0,0.5)is used to ﬁnd
the largestγthat produces a stable output set. In our example,
Block 3 does not have one dominant parity, but rather has
4. In running the binary search, we regard any re-estimated
Fourier weight less thanγ2
2as an error and an indication that
theγis too small (indicated by red dots). Taking threshold
γ∈{0.25,0.375,0.4375}, from the Fig. 7 (Left), it can be
seen that the output is not reliable until 0.4375 . Therefore, in
this work, we choose γ1= 0.8for Block 1, γ2= 0.9for
Block 2, and γ3= 0.45for Block 3.
To investigate the effect of the number of queries on
convergence of the Fourier weight, we test the Goldreich-Levin
algorithm on different numbers of queries (10, 25, 50, 100,
200, 400, 800), and show 10 runs for each. In Fig. 7 (Right) we
plot the weights of the found sets and see that Block 3 do not
have output lists when the number of queries is small, Block
2 converges quickly, and Block 1 roughly converges after 200
queries. When the number of queries is too small, there is an
error output list for Block 2 (indicated by red dots). Block 2’s
rapid convergence is likely due to the fact that the true function
is a parity and hence has one large Fourier coefﬁcient. That
the others converge more slowly is likely due to the fact that
their exact representations seen in Table II are more non-linear.
Block 3 has several equally large coefﬁcients.
1By [13, Problem 1.5], there can only be at most 1 Fourier coefﬁcient with
magnitude above 0.5.
Fig. 7: Estimated Fourier weights of Goldreich-Levin of three
TurboAE blocks as a function of γ(Left) and the convergence of
Fourier weight over the number of queries (Right).
APPENDIX C
LOSS LANDSCAPE PLOTS
See Fig. 4, referred to in Section IV.
APPENDIX D
PROOF OF PROPOSITION 2
Proof. Recall equation (1):
C(f,g) =1
kk∑
i=1E[
DKL(P[Ui= 1|Y]||gi(Y))]
+H(Ui|Y).
For a ﬁxed f, our choice of gdoes not affect H(Ui|Y), so
it only matters how it affects E[
DKL(P[Ui= 1|Y]||gi(Y))]
.
Recall by Gibb’s Inequality that KL-Divergence is always
nonnegative, and is 0 if and only if P[Ui= 1|Y=y] =gi(y).
Thus, by deﬁning P[Ui= 1|Y=y] =:gi(y)for each
i∈[k], we ensure1
kk∑
i=1E[
DKL(P[Ui= 1|Y]||gi(Y))]
= 0,
its minimum possible value, and Gibb’s Inequality ensures that
this is the almost surely unique choice of minimizing g.
APPENDIX E
PROOF OF 2-S IDED BOUND ON BCE BYBER 3
For the proofs below, we take as deﬁnition for BER of an
encoder-decoder pair f,g:
B(f,g) =E
1
kk∑
i=1Ui 1[g(Y)i≤1
2] + (1−Ui) 1[g(Y)i>1
2]
.
Whengis the soft-MAP decoder for f, this reduces to
B(f) =1
kk∑
i=1E[
min(P(Ui= 1|Y),P(Ui= 0|Y))]
.
The best bounds relating BER and BCE we can hope for
are in Prop. 3, whose proof is the following:
Proof. Leti∈[k]and ﬁxf. LetT= min( P(Ui=
1|Y),P(Ui= 0|Y)). Then
Bi(f) =E[T],Ci(f) =H(Ui|Y) =E[H2(T)](a)P,P′= (1,10,21),(1,10,23)
 (b)P,P′= (1,10,23),(1,10,22)
Fig. 8: Binary Cross Entropy landscape on parametric line λΘ′′+(1−λ)Θ′joining two parity combinations Θ′andΘ′′.λ= 0,1correspond
toΘ′,Θ′′respectively. Local minimums around λ= 0,1suggest that Θ′,Θ′′are locally optimal codes on the line joining Θ′,Θ′′.
Recall that entropy is a concave function, so all line segments
connecting two points of H2in[0,1]lie in the undergraph
ofH2. Since H2(0) = 0 andH2(1
2)
= 1, we know that
H2(s)≥2s∀s∈[
0,1
2]
. Thus, by monotonicity of expecation,
we have that Ci(f) =E[H2(T)]≥2E[T] = 2Bi(f)proving
the LHS inequality. For the second, note again that entropy is
a concave function, so we can apply Jensen’s Inequality. Then
Ci(f) =E[H2(T)]≤H2(E[T]) =H2(Bi(f))giving us the
RHS inequality. The second inequality follows by averaging
together the inequalities for each component.
A. Tightness of 2-sided bound
We will show that ∀t∈[0,1/2], for each of the statements
below, there exist encoders f:Fn
2→Sand a noise models
onSso that
1)2Bi(f) =Ci(f) = 2t
2)Ci(f) =H2(Bi(f)) =H2(t)
This will establish that our bound from Prop. 3 is tight. Note
that here BiandCiare functions of bothfand our noise
model.
1) Upper bound is tight:
Proof. Takef:F2→F2to be the identity function, and let
P:=(t 1−t
1−t t)
,
be the transition matrix of our channel (a binary symmetric
channel). Then it may be shown that B(f) =tandC(f) =
H(U|Y) =H2(t) =H2(B(f)), proving tightness of the upper
bound.
2) Lower bound is tight:
Proof. Takef:F2→[3]to be any injective encoder. We
specify our noise model by specifying P(U= 0|Y=i)and
P(Y=i), then use these to compute the symbol transition
probabilities. Denote P(U= 0|Y=i) =:xiandP(Y=
i) =:yi. Then our constraints are
3∑
i=1yixi=P(U= 0) =1
2,3∑
i=1yi= 1,and
B(f) =3∑
i=1yimin(xi,1−xi) =t
C(f) =3∑
i=1yiH2(xi) = 2t,
where the last constraint enforces that our lower bound holds
with equality. Consider x1= 1,x2= 0,x3= 1/2. Lety3= 2t
andy1=y2= 1/2−t. Sincet∈[0,1/2]this makes sense.
One can verify that our constraints are met.
Usingyandxwe explicitly construct our noise model
in terms of symbol transition probabilities. Denote pi,j:=
P(Y=i|U=j). By Bayes’ Theorem
pi,0=P(U= 0|Y=i)P(Y=i)
P(U=j)= 2xiyi
and
pi,1=P(U= 1|Y=i)P(Y=i)
P(U=j)= 2(1−xi)yi.
Substituting, our symbol transition probabilities can be ex-
pressed as
P:=
p1,0p1,1
p2,0p2,1
p3,0p3,1
=
1−2t 0
0 1−2t
2t 2t

Injectivity of fwas used to ensure we can assign different
symbol transition probabilities for different values of U∈F2.
APPENDIX F
COUNTEREXAMPLE THAT SHOWS BER ( AND BLER)
MINIMIZING ENCODERS ARE NOT BCE MINIMIZERS
Proposition 4. There exists a channel so that all encoder
minimizers of cross-entropy are not encoder minimizers of the
bit error rate.
To show this, we will take the following parameters
1)k= 1, soU∼Unif[F2]. In this case BER = BLER.
2) Encoder f:F2→[4]. DenoteX:=f(U). This is a
random variable.3) Random variable Y∈[4]represents the corrupted
channel output.
In our one-bit case, we have
B(f) =E[
min(P(U= 1|Y),P(U= 0|Y))]
C(f) =H(U|Y) =E[
H2(P(U= 1|Y))]
.
Our channel is represented as
P=
p1,1p1,2p1,3p1,4
p2,1p2,2p2,3p2,4
p3,1p3,2p3,3p3,4
p4,1p4,2p4,3p4,4

wherepijfori,j∈[4]represents P(Y=i|X=j). Using
P(U= 1) = P(U= 0) = 1/2and Bayes’ rule,
B(f) =4∑
i=1pif(1)+pif(0)
2min(pif(1),pif(0))
pif(1)+pif(0)
=1
24∑
i=1min(pif(1),pif(0))
andC(f)in terms ofPas:
C(f) =E[
H2(P(U= 1|Y))]
=−4∑
i=11
2pif(1)lg(
pif(1)
pif(1)+pif(0))
+1
2pif(0)lg(
pif(0)
pif(1)+pif(0))
=−1
24∑
i=1pif(1)lgpif(1)+pif(0)lgpif(0)
−(pif(1)+pif(0)) lg(pif(1)+pif(0)).
For explicit construction of the counterexample we take
advantage of the fact that min(x,1−x)andH2(x)weight
[0,1
2]differently. For min(x,1−x), an improvement of ton
one input and a worsening by ton another cancel each other
out no matter what the original xwas. On the other hand,
because of the curvature H2(x)the net result depends on the
value ofx. Let our transition matrix be
P=
0.24 0.15 0.24 0.056
0.26 0.15 0.26 0.343
0.2605 0.35 0.2605 0.25
0.2395 0.35 0.2395 0.35

Then we try each possible f, representing each fas
[f(0),f(1)]:f B(f)C(f)
[1,2] 0.4 0.969
[1,3] 0.5 1.0
[1,4] 0.40275 0.943
[2,3] 0.4 0.969
[2,4] 0.40300 0.949
[3,4] 0.40275 0.943
It is clear that [1,2],[2,3]are minima of the BER with value
0.4, while [1,4],[3,4]are minima of cross-entropy with value
0.943. Thus, they are not minimized by the same encoder,
decoder pair.
APPENDIX G
NEW PROPOSED TRAINING ALGORITHM FOR
TURBO AE- LIKE CODES
See algorithm 2 as referred to in Section V-B.
Algorithm 2 Alternative Training Scheme( ΘENC ,ΘDEC ,
kENC = 16 ,kDEC = 100 ,sENC = 500 ,s(1)
DEC = 150,000,
s(2)
DEC = 150,000)
1:Randomly initialize encoder parameters ΘENC
2:forsteps 1tosENC do
3: Sample a batch of inputs, noise, and sample an inter-
leaver
4: Empirically estimate the cost1
kENC∑kENC
i=1H(Ui|Y)
forΘENC using the batch
5: Update ΘENC using the gradient of the cost
6: Enforce power constraint on ΘENC using algorithm 3
7:end for
8:forsteps 1tos(1)
DEC do
9: Enforce power constraint on ΘENC for new block
lengthkDEC using algorithm 3
10: Sample a batch of inputs, noise, and interleavers
11: Empirically estimate the C(ΘENC,ΘDEC)using the
batch at block length kDEC
12: Update ΘDEC using the gradient of the cost
13:end for
14:Fix a randomly chosen interleaver πfor further training
15:forsteps 1tos(2)
DEC do
16: Sample a batch of inputs and noise
17: Empirically estimate the C(ΘENC,ΘDEC)using the
batch at block length kDEC and ﬁxed interleaver π
18: Update ΘDEC using the gradient of the cost
19:end for
20:return( ΘENC ,ΘDEC )
APPENDIX H
ANALYTIC DERIVATION OF POWER CONSTRAINT
We are given a nonrecursive Turbo code, f:Fk
2→R1/R×k,
of window size w(memoryw−1) and interleaver π:[k]→[k]
for input size kand rateR=k/n. This code is parameterizedby1/Rgenerating functions h(s):Fw
2→R, withs∈[1/R]
in the following sense for input u∈Fk
2andi∈[k]:
f(s)
i(u) =h(s)(ui−w+1:i)s∈[1/R−1]
f(s)
i(u) =h(s)(π(u)i−w+1:i)s= 1/R
where uj= 0 forj≤0. From here we can directly compute
the power of a code:
E
R
k1/R∑
s=1k∑
i=1f(s)
i(u)2

=R
k1/R∑
s=1k∑
i=1E[
f(s)
i(u)2]
=1/R−1∑
s=1k∑
i=1R
kE[
h(s)(ui−w+1:i)2]
+k∑
i=1R
kE[
h(1/R)(π(u)i−w+1:i)2]
Taking into account that the each ui∼Unif[F2]iid, we can
simplify the above expression and split into boundary terms +
main sequence terms:
R
k1/R∑
s=1w−1∑
i=1E[
h(s)(ui−w+1:i)2]
(2)
+R
k(k−w+ 1)1/R∑
s=1E[
h(s)(u1:w)2]
,
which can be directly computed for small w.
To meet the power constraint we can simply rescale hso
equation (2) evaluates to 1. However, rescaling by a larger
constant lowers the effective SNR of the code. Note that
for the AWGN channel, performance is determined by the
relative arrangement of the codewords in Euclidean space.
Thus, we wish to ﬁnd a translation constant Cthat minimizes
the rescaling constant Sneeded so (h−C)/Sproduces an
encoder with power 1. Observe that
R
k1/R∑
s=1k∑
i=1E[
(f(s)
i(u)−C)2]
=R
k1/R∑
s=1k∑
i=1E[
(f(s)
i(u)2]
−2CE[
f(s)
i(u)]
+C2
=C2−2CR
k1/R∑
s=1k∑
i=1E[
f(s)
i(u)]
+···
where the remaining terms are not relevant to ﬁnding the
optimum value of C. Note thatCis minimized when
C=R
k1/R∑
s=1k∑
i=1E[
f(s)
i(u)]
(3)
which we can directly compute from hin similar fashion to
(2). This gives us algorithm 3 for optimally enforcing thepower constraint during training.
Algorithm 3 Constrain Power(generator h, block length k)
1:Explicitly calculate equation (3) using hand block length
k, and assign to C
2:Explicitly calculate equation (2) using h−Cand block
lengthk, and assign to S
3:h←h−C
S
4:return(h)
APPENDIX I
ADDITIONAL INITIALIZATION TRIED FOR ENCODER
In addition to the initialization for hdescribed in section
V-B, we also tried initializing hwith a parity. That is, hwas
initialized with a uniformly random chosen parity function
fromFw
2→F3
2in which the wthinput bit has nonzero
inﬂuence on all 3 outputs. We show the training curve (Fig. 9)
and FC evolution (Fig. 10) for one of our runs. The example
shows that parities are not always local optima when training
with conditional entropy.
Fig. 9: Training curve for a parity initialized encoder trained with
conditional entropy at block length 16. The green bar shows the
estimated conditional entropy of TurboAE-cont at the same block
length. Its quick descent shows it is not a local optimum.
APPENDIX J
TRAINING CURVES FOR ENCODER AND DECODER
See Fig. 11 as referred to in Section V-B.Fig. 10: FC evolution during training of the parity initialized encoder
from Fig. 9. As a parity, it starts with 1 coefﬁcient, but during training
it develops other dominant FC.
Fig. 11: Left: Learning curve of a Turbo-like encoder during training
with conditional entropy at block length 16. The green bar shows the
estimated conditional entropy of TurboAE-cont at the same block
length. Right : Learning curve of a neural decoder paired with the
learned encoder from the left panel. Training was done at block length
100 and used binary cross-entropy as the cost function. The encoder
was ﬁxed during the training of the decoder.