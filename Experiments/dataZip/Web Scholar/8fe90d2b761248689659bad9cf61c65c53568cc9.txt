Machine learning methods for the search for L&T brown dwarfs in the
data of modern sky surveys.⋆
Aleksandra Avdeevaa,b,∗,1
aHSE University, 20 Myasnitskaya St., Moscow, 101000, Russia
bInstitute of Astronomy RAS, 48 Pyatnitskaya St., Moscow, 119017, Russia
ARTICLE INFO
Keywords :
brown dwarfs
machine learning
surveysABSTRACT
Accordingtovariousestimates,browndwarfs(BD)shouldaccountforupto25percentofallobjects
in the Galaxy. However, few of them are discovered and well-studied, both individually and as a
population. Homogeneous and complete samples of brown dwarfs are needed for these kinds of
studies.Duetotheirweakness,spectralstudiesofbrowndwarfsareratherlaborious.Forthisreason,
creating a significant reliable sample of brown dwarfs, confirmed by spectroscopic observations,
seems unattainable at the moment. Numerous attempts have been made to search for and create a set
ofbrowndwarfsusingtheircoloursasadecisionruleappliedtoavastamountofsurveydata.Inthis
work,weusemachinelearningmethodssuchasRandomForestClassifier,XGBoost,SVMClassifier
and TabNet on PanStarrs DR1, 2MASS and WISE data to distinguish L and T brown dwarfs from
objects of other spectral and luminosity classes. The explanation of the models is discussed. We also
compare our models with classical decision rules, proving their efficiency and relevance.
1. Introduction
Brown dwarfs are substellar objects that were theoreti-
cally predicted (Kumar, 1963; Hayashi and Nakano, 1963)
and then discovered 30 years later (Rebolo et al., 1995;
Nakajima et al., 1995). Since then, the search (Luhman,
2013; Burningham et al., 2013; Carnero Rosell et al., 2019)
and systematic study of known brown dwarfs (Kirkpatrick
et al., 1999; Skrzypek et al., 2016; Kirkpatrick et al., 2021)
has not stopped. Their mass is insufficient to start and
maintainstablehydrogenfusion,whichcausesthemtocool
over time. The peak of the radiation intensity falls into the
infraredrange,soobjectsareratherfaintinthevisiblespec-
trum. In the spectral classification, brown dwarfs occupy
spectral types L, T and Y.
According to studies (Mužić et al., 2017), the number
ofbrowndwarfsintheGalaxyrangesfrom25to100billion
objects(withthetotalnumberofobjectsrangingfrom100to
500billion).Homogeneousandcompletesamplesofbrown
dwarfs are needed for various kinds of studies: kinematic
studies (Smith et al., 2014), studies of binary stars with
brown dwarfs (Lodieu et al., 2014), and studies of the pa-
rametersoftheGalaxy.Browndwarfsoccupytheboundary
between stars and planets, and studying their properties
helpstorefineourunderstandingofthisboundary.Complete
and uniform catalogues enable to identify and characterize
browndwarfswithgreateraccuracy,allowingforabetterde-
terminationofthelowermasslimitforstellarformationand
the upper mass limit for planet formation. Moreover, brown
dwarfssharesimilaritieswithgiantexoplanets,makingthem
valuable analogs for studying exoplanetary atmospheres.
⋆This document is the results of the research project funded by Non-
commercial Foundation for the Advancement of Science and Education
INTELLECT.
∗Corresponding author
avdeeva@inasan.ru (A. Avdeeva)
ORCID(s):0000-0002-0513-4425 (A. Avdeeva)By studying the atmospheres of brown dwarfs, similar to
exoplanets, we can gain insights into the processes and
conditionsthatgovernexoplanetatmospheres,includingthe
presence of clouds, atmospheric composition, and thermal
profiles.
Probably the most topical issue regarding brown dwarfs
is the L/T transition (Artigau et al., 2009; Gillon et al.,
2013; Khandrika et al., 2013). The L/T transition in brown
dwarfs is a fascinating phenomenon that is characterized by
a sharp change in the near-infrared colours and brightness
of brown dwarfs. It is believed to be driven by several
possiblemechanisms.Cloudmodelslinkthesharptransition
to the sinking of dust clouds below the photosphere (Mar-
ley and Robinson, 2015; Charnay et al., 2018). Instability
in the carbon chemistry of brown dwarf atmospheres has
been proposed as another mechanism contributing to the
L/T transition (Tremblin et al., 2019). Adiabatic convection
triggeredbythisinstabilitycanleadtovariabilityacrossthe
L/T spectral sequence. Cloud dispersal has emerged as a
potential mechanism for the L/T transition (Tan and Show-
man, 2019). It has been suggested that clouds with larger
particlesizesdissipatemoreeasilythancloudswithsmaller
particle sizes. The shift from L to T spectral types may
be accompanied by a change from small to large particles,
leading to the fragmentation of clouds and a transition to
cloud-free atmospheres (Burningham et al., 2017; Saumon
and Marley, 2008). A detailed overview of the problem can
be found in Vos et al. (2019).
To uncover the nature of the phenomenon mentioned
above, as well as to refine the statistical characteristics of
brown dwarfs, complete and uniform catalogues of brown
dwarfs are required. While spectroscopy is essential for
confirming the nature of a brown dwarf and studying its
detailed properties, conducting spectroscopic observations
for a large number of objects across the entire sky is time-
consuming and resource-intensive. Photometric surveys, on
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 1 of 12arXiv:2308.03045v3  [astro-ph.SR]  18 Aug 2023ML for L&T Brown Dwarfs Search in Sky Surveys
the other hand, can cover a much larger area of the sky
and efficiently capture data on numerous celestial objects
simultaneously.
By employing colour selection techniques in photomet-
ric surveys, one can identify objects that exhibit colours
indicative of brown dwarf characteristics. The advantage of
using photometric surveys is that they allow for systematic
and wide-scale screening of potential brown dwarf candi-
dates, helping to identify promising targets for subsequent
spectroscopic observations.
As an illustration, Skrzypek et al. (2016) accomplished
this feat by effectively employing data from three surveys:
SDSS, UKIDSS, and WISE. They employed a specific
colour selection criterion, namely (𝑌−𝐽)𝑉𝑒𝑔𝑎>0.8and
𝐽 < 17.5, as a decision rule. Through this approach, they
were able to discern approximately 1300 brown dwarfs
within an area of 3000 square degrees, which corresponds
toapproximately7.5%ofthecelestialsphere.Anadditional
noteworthy implementation in the quest for brown dwarfs
is exemplified by Carnero Rosell et al. (2019). Their study
also incorporated a decision rule utilizing data from the
DES, VHS, and WISE surveys. The following criteria were
applied: (𝑖−𝑧)>1.2,(𝑧−𝑌)>0.15,(𝑌𝐴𝐵-𝐽𝑉𝑒𝑔𝑎)>1.6,and
𝑧 <22. The imposition of a magnitude limit on the 𝑧band
was necessary to ensure the completeness of the dataset,
thereby excluding any missing values. Within an area span-
ning2400squaredegreesabout5.8%ofthecelestialsphere,
approximately12thousandbrowndwarfsweresuccessfully
identified through their approach. Carnero Rosell et al.
(2019)alsopresentsacomprehensivereviewofothercolour
selection works.
When it comes to using colour selection to search for
browndwarfs,incorporatingmachinelearningmethodscan
providesignificantadvantages.Machinelearningtechniques
can enhance the effectiveness and efficiency of the colour
selection process by leveraging large datasets and complex
algorithms to identify patterns and make more accurate
predictions.
Machine learning methods can help uncover subtle
relationships and correlations in multi-dimensional colour
space,allowingfortheidentificationofdistinctcoloursigna-
turesassociatedwithbrowndwarfs.Thiscanbeparticularly
valuablewhendealingwithcomplexandoverlappingcolour
distributions between different objects.
ML methods are increasingly being used for classifying
astronomicalobjectsduetothevastamountofdatacollected
in the past decades. For example, Maravelias et al. (2022)
combined Support Vector Machine (SVM), Random Forest
(RF) and Multilayer Perceptron to classify massive stars in
nearby galaxies. The accuracy of the test dataset was 83%.
Applying it to other galaxies (not included in the dataset),
namelyIC1613,WLMandSextansA,achievedanaccuracy
result at the level of 70%, which the authors attribute to
differentmetallicityandextinctioneffects.Themissingdata
werefilledinwithsimpleaveragesandthe Iterative Imputer
method of the Scikit-learn library (Pedregosa et al., 2011).
The Iterative Imputer method calculates the missing valuesbased on the present values for the features in the same
manner as regression models do. This method, being at the
same time more robust, showed a better performance in the
work.
The interpretable machine learning techniques (Local-
izedGeneralMatrixLVQandRF)wereusedinMohammadi
et al. (2022) to detect extragalactic Ultra-compact dwarfs
and Globular Clusters. Authors analysed the importance of
featuresandcomparedthemwithfeaturesthatcarryphysical
information of the objects.
This work aims to develop an additional tool for the
search for brown dwarfs in large photometric surveys with
machine learning methods. That is, based on the set of
magnitudes and colours of the object, the model must de-
termine whether the given object is a brown dwarf or not.
We also compare our results with some classical decision
rules: Burningham et al. (2013) and Carnero Rosell et al.
(2019).ThesummaryoftheserulesisshowninTab.2.The
toolwillbeusedinfutureworkforthesearchforpreviously
undiscovered brown dwarfs.
This paper is organised as follows. Sec. 2 describes
the dataset and preprocessing of the data, including feature
engineering,augmentationandtheapproachtohandlingthe
missing values. In Sec. 3 we apply the machine learning
methods to the dataset and explore the importance of fea-
tures. Finally, in Sec. 4 we compare the performance of
machinelearningmodelsandtheclassicaldecisionrulesand
discuss the robustness of the models.
2. Building the dataset and preprocessing
The dataset is based on L and T brown dwarfs from
the Best et al. (2018) catalogue. The catalogue contains
information on 1601 L and T type brown dwarfs and 8287
M type red dwarfs, the spectral class closest in physical
characteristicstobrowndwarfs.Magnitudesin12photomet-
ric bands and their errors are provided: 𝑔,𝑟,𝑖,𝑧,𝑦 of Pan-
STARRS 1 (Chambers et al., 2016), 𝐽,𝐻,𝐾𝑠 of 2MASS
spacemission(Cutrietal.,2003)and 𝑊1,𝑊2,𝑊3,𝑊4of
WISE space mission (Cutri et al., 2021).
The catalogue also contains astrometrical information:
position,parallaxandpropermotion.Inaddition,references
to the literature are presented, from which data on proper
motion and parallax was taken.
For our Machine Learning models we consider brown
dwarfs to be a positive class. To create a representative
distributionofnegativeclassobjects,weexaminedthedistri-
butionof100thousandstarsfromGaiaDR3(Collaboration
etal.,2016,2022)byabsolutemagnitude 𝑀𝐺(Fig.2a).We
have selected 1791 objects from A0 to K9 spectral class
in the proportions observed in Fig. 2a from the Simbad1
databaseofastronomicalobjects.Objectsthatarepresented
in Simbad are usually well-studied and have solid spectral
classifications.GaiadataseemtobeshortregardingM-type
dwarfs, especially, after M3, so we adopt their distribution
1http://simbad.cds.unistra.fr/simbad/
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 2 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Response 
0.20.40.60.81.0
2MASSJ
H
Ks
103104105
Wavelength, Å0.20.40.60.81.0
WISEW1
W2
W3
W40.20.40.60.81.0
Pan-STARRSg
r
i
z
y
Figure 1: Response curves of photometric systems.
from Best et al. (2018). The distribution of the dataset we
have obtained is shown in Fig. 2b.
It should be noted, the obtained distribution is most
likely incomplete, both in M-dwarfs and in earlier spectral
classes. The deficiency in M-dwarfs could be due to Best
et al. (2018) being not complete for M-type dwarfs outside
of 10-pc. The objects of A0 to K9 might tend to be brighter
than the actual population and in fact have an underdensity
in the range between 12𝑚and15𝑚in𝑖𝑃𝑆1(Fig 3a), which
we,however,assumetobenotrelevantincaseofusingonly
colour indices as features.
The objects selected from Simbad were cross-matched
with data from the Pan-STARRS DR1, 2MASS, and ALL-
WISE catalogues. We chose the matching radius to be 1′′,
whichisareasonablevalueformostsurveysforobjectswith
low proper motions, including those used in this paper.
The resulting dataset contains 5669 objects, 1601 of
whichareofthepositivetargetclass.Thedatasetisavailable
online2. As the peak intensity of brown dwarfs falls in the
infrared part of the spectrum, their magnitudes in the opti-
cal photometric bands (bands 𝑔,𝑟,𝑖) are most likely almost
beyond the sensitivity limit of the telescope and therefore
are missing from the data. The 𝑔and𝑟bands of the Pan-
STARRSdataaremissingforalmostallobjects,sowedon’t
use these magnitudes as features. The 𝑖band values are
missing for about one-third of the objects of the positive
class and a small number of objects of the negative class.
Magnitude values in this band are important to us, also for
comparison with the classical rules, so we keep them. We
also remove 𝑊3and𝑊4as they are of poor quality, 90th
percentile of the error of magnitude measurement for both
magnitudesisabouthalfofamagnitude,whichisverynoisy
for the classification problem we want to solve. As a result,
we have 7 magnitudes for each object: 𝑖𝑃𝑆1,𝑧𝑃𝑆1,𝑦𝑃𝑆1,𝐽,
𝐻,𝐾𝑠,𝑊1,𝑊2.
2https://github.com/iamaleksandra/ML-Brown-Dwarfs/
10
 5
 0 5 10 15 20
MG025005000750010000125001500017500Number of objects(a)
10
 5
 0 5 10 15 20
MG0200400600800Number of objects(b)other types
BDFigure 2: Absolute colour distribution for the Gaia sample (a)
and the dataset used in this work (b).
2.1. Train, validation and test
Thedataisdividedintotraining(60%),validation(20%)
and test (20%) sets in a stratified fashion using using sci-kit
learn train_test_split method. The features are scaled using
StandardScaler and model hyperparameters are selected on
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 3 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
thevalidationsetusing optuna(Akibaetal.,2019).Thefinal
performance of the model is calculated on the test set.
2.2. Augmentation
As the number of objects of negative class is almost
2.5 times higher than the number of positive class objects,
we use augmentation with Gaussian-distributed noise as an
oversampling method to make the dataset more balanced.
The data was divided into training, validation and test sub-
samplespriortoaugmentationinordertonotletmodelssee
the augmented data in the test sample, while being trained
on the original prototype of these augmented objects.
We augment the data of positive and negative classes
separately,usingallobjectsofpositiveclassandonlyobjects
in the range between 12𝑚and15𝑚in𝑖𝑃𝑆1in negative class
objects.Foreachfeatureerror,wecalculatethemeanandthe
standard deviation. We then generate normally distributed
noise values with the same parameters of the distribution.
The noise is added to all values of the corresponding fea-
tures, which do not have any missed values. Thus we have
8364 objectsof which 4155are positive and4209 are nega-
tive.
2.3. Feature engineering
In a classification of astronomical objects, as in various
types of astronomical problems, the colours of objects are
evenmoreimportantthanthemagnitudes.Coloursarechar-
acteristic of the energy distribution in the spectrum and are
almost independent of distance. To take this into account
when classifying, we have added several features - colour
indices: (𝑖−𝑧)𝑃𝑆1,(𝑖−𝑦)𝑃𝑆1,(𝑧−𝑦)𝑃𝑆1,𝑧𝑃𝑆1−𝐽,
𝑦𝑃𝑆1−𝐽,𝐽−𝐻,𝐻−𝐾𝑠,𝐾𝑠−𝑊1,𝑊1 −𝑊2.Theyare
also often used as the colour selection to distinguish brown
dwarfs from other objects. We only use colours of the most
spectroscopicallyadjacentmagnitudes(seeFig.1).Thetwo
exceptionsare 𝑧𝑃𝑆1−𝐽sinceitiscommonlyusedasacolour
selection and proved to be useful, and (𝑖−𝑦)𝑃𝑆1since it
turnedouttobeextremelyusefulforclassification(seeSec.4
for details).
After this procedure, the table contains 17 features for
each of the 8364 objects, listed in Tab. 1. Fig. 3 shows how
objects of the target class look compared to objects of all
other classes in a two-dimensional slice of feature space.
We use all of the magnitudes and colours simultaneously
(even though the latter ones are the linear combination of
thefirstonesbydesign)sinceweprocessthemissingvalues
independently,whichsometimesviolatestheserelations(for
the details see Sec. 2.4).
As one can see (Fig. 3a), the upper limit of magnitude
𝑖𝑃𝑆1differsalotforourpositiveandnegativeobjects.Thisis
duetotheprocedureofbuildingthedataset,i.e.Simbadhas
fourtimesmoreobjectswith 𝑖𝑆𝐷𝑆𝑆>20andaspectraltype
≥𝐿0than objects with a spectral type < 𝐿0. Catalogues,
however,containlargeamountoffaintobjects,themajority
of which are not brown dwarfs. We therefore should avoid
models based primarily on PS1 magnitudes. Although our
dataset can be called balanced regarding other magnitudes
(from 2MASS and WISE), the magnitude is not only afunction of the luminosity of an object, but also a function
of the distance to it, so it is not advisable to rely on these
magnitudes as well.
Thus, three cases for each approach are investigated: all
magnitudes and colours are used as features (we call this
case "all features"), no Pan-STARRS magnitudes are used
("w/oPSmagnitudes")andnomagnitudesusedatall("only
colours").
2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
zPS1[mag]5.07.510.012.515.017.520.022.5iPS1[mag](a)BD
other types
4
 2
 0 2 4
(i-y)PS11.0
0.5
0.00.51.01.52.0(J-H)2MASS(b)BD
other types
Figure 3: Augmented data: objects of different classes on the
magnitude-magnitude (a) and colour-magnitude (b) diagrams.
Brown triangles are the objects of positive class (L&T brown
dwarfs), blue squares are the objects of negative class. Here
blue points are plotted on top of the brown triangles.
2.4. Processing of missing values
Asitwasmentionedearlier,thedatasethasasignificant
number of missing values. At the part of the spectrum with
longer wavelength, this is most likely due to the sensitivity
limitofthetelescope:browndwarfsareratherfaintobjects,
and their emission maximum falls into the infrared part
of the spectrum. Missing values at the shorter wavelength
part of the spectrum seem to occur due to poor-quality
measurements or artefacts.
Todealwiththemissingvalues,Maraveliasetal.(2022)
used imputing by means and the Iterative Imputer of the
Sckit-learn library, showing that Iterative Imputer provides
results more robust and effective in terms of classification.
We test the method by additionally throwing out the
magnitudevaluesfor5percentofobjects,whichdonothave
missing values of the particular feature. Then we impute
these values using the Iterative Imputer and compare the
results to the original feature values for the object.
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 4 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
Tab.1 shows the results of the imputation with the fol-
lowing parameters of the Iterative Imputer:
estimator=ExtraTreesRegressor
( n_estimators=150
max_features=14
max_depth=15
min_samples_split=12
initial_strategy= 'median '
max_iter=20)
Tab. 1 contains the information about the fraction of
missing values of a particular feature in the dataset and
the number of objects that were withheld for the testing.
We also compare the 90th percentile error of the feature
measurement (the error of magnitude is usually given in
the catalogue and the error of the colour index is calculated
as the square root of the sum of the squared errors of the
magnitudes involved) with the 90th percentile discrepancy
in the actual value of the feature and the value predicted by
the imputer.
In most cases, the 90th percentile for the discrepancy
between the imputed values and the original ones is com-
patible with the 90th percentile for the error of the feature.
EventhoughthecolourvaluescalculatedintheSec.2.3are
directly related to the magnitude values, it was decided to
apply Iterative Imputer to them independently. This allows
one to achieve better results and avoid large errors in the
calculation of colour values, as can be seen from Tab. 1.
InFigure4,wecanseeanexampleofimputing.Thetop
panel displays a magnitude-magnitude diagram containing
both the original and imputed data. It’s worth noting that
a significant portion of the missing data points are located
in the fainter range of the 𝑖magnitude. The bottom panel
compares the original data to the imputed values for the
same objects. Although there are some discrepancies up to
0.5minthe𝑦𝑃𝑆1−𝐽color,mostofthevaluesarepredicted
accurately.Specifically,for90%ofthestars, 𝑦𝑃𝑆1−𝐽value
have an error less than 0.063m.
3. Application of machine learning
Four approaches are tested during the work: Random
Forest (RF), Support Vector Machines (SVM), XGBoost
and TabNet. As it was said in Sec. 2, we investigate three
cases for each approach: "all features", "w/o PS magni-
tudes"and"onlycolours".Wecalculatethescoreandfeature
importance for each model, using SHAP(Lundberg and
Lee, 2017) for RF, XGBoost, SVM and TabNet. Although
TabNet has built-in capabilities of calculating importance
of features,based on the attention mechanism’s dynamic
selection of input features, we use SHAPas well so as we
could compare the results properly.
The SHAPmethod works by evaluating the model’s
prediction for each instance while permuting the values
of a specific feature. This involves shuffling the values of
the chosen feature while keeping the rest of the features
unchanged. The difference between the model’s prediction
with the original feature values and the prediction with the
2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
yPS1 [mag]5.07.510.012.515.017.520.022.5iPS1 [mag]no changes
originally missing
manually missing
0 1000 2000 3000 4000 5000 6000 7000 8000
index0.00.51.01.52.02.53.0yPS1-J
original
imputedFigure 4: An example of imputed values for y𝑃𝑆1−J and y𝑃𝑆1
features. (Top) Original data (pink circles), initially missing
data (green plus) and manually missing values (blue crosses)
on the colour-magnitude diagram. (Bottom) Comparison of
the original data (pink circles) and the values, imputed via the
Iterative Imputer method (white crosses). An index is used to
avoidcontaminationfromtheimputedvaluesofotherfeatures.
permuted feature values is used to calculate the Shapley
value.
We compare all models to the classical decision rules
Carnero Rosell et al. (2019) and Burningham et al. (2013).
The Matthews correlation coefficient (MCC) is chosen as
the primary metric since it takes into account both False
PositiveandFalseNegativepredictions.Itcanbecalculated
as follows:
MCC =TP×TN−FP×FN√
(TN+FN)(TN+FP)(TP+FP)(TP+FN)
where TP is the number of true positives, TN the number
of true negatives, FP the number of false positives and FN
thenumberoffalsenegatives.Wealsoprovidetheprecision
and recall scores for they are more intuitive. Precision is a
proportion of relevant instances among retrieved instances,
whilerecallistheproportionofrelevantinstancesthathave
been retrieved. They are defined as:
PRECISION =TP
TP+FPRECALL =TP
TP+FN
Theclassicaldecisionrulesandtheresultoftheirappli-
cation to the test dataset after augmentation and imputation
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 5 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
Table 1
Properties of the dataset and the results of the imputing test for every feature for the train part of the dataset. The table includes
the fraction of missing values in the dataset (after augmentation) and the number of objects which were withheld for the test
(5% of the objects for which the value of the feature was presented). We compare the 90th percentile for the discrepancy between
the imputed values and the original ones with the 90th percentile for the error in the measurement of the feature value. The error
of the colour index is calculated as the square root of the sum of the squared errors of the magnitudes involved.
Feature Fraction of Number of 90th percentile 90th percentile
missing values objects withheld of the error of discrepancy
i𝑃𝑆1 17% 208 0.050 0.070
z𝑃𝑆1 5.5% 237 0.050 0.091
y𝑃𝑆1 2.2% 245 0.060 0.109
J 8.8% 228 0.100 0.101
H 8.8% 228 0.120 0.107
Ks 8.9% 228 0.110 0.075
W1 2.1% 245 0.141 0.085
W2 2.0% 242 0.080 0.096
(i-z)𝑃𝑆118.4% 204 0.067 0.050
(i-y)𝑃𝑆118.0% 205 0.072 0.038
(z-y)𝑃𝑆16.7% 234 0.073 0.077
z𝑃𝑆1-J 12.3% 219 0.122 0.048
y𝑃𝑆1-J 10.9% 223 0.130 0.063
J-H 8.8% 228 0.164 0.154
H-Ks 9.1% 225 0.170 0.145
Ks-W1 9.9% 226 0.183 0.143
W1-W2 2.2% 245 0.168 0.176
aresummarizedinTab.2.TheMCCscorewascalculatedon
thetestpartofthedataset.Itshouldbementioned,thateven
though the filters in different surveys have similar names
(i.e.𝑦𝑃𝑆1and𝑌𝐷𝐸𝑆), they are not identical to each other.
Therefore,itisnotentirelycorrecttoapplytherulesmadefor
one survey to the magnitudes of the other survey. However,
wehaveestimatedthatforourdatasetthemagnitudesofthe
same name differ within 0.2mag which does not increase
the score in any case. Also, it is worth mentioning that
Burningham et al. (2013) was originally devoted to T-type
dwarfs exclusively, yet it shows great performance on L
dwarfs, so we use it as a decision rule for both L and T
dwarfs.
Although the performance of the decision rules is rea-
sonably high, the actual number of false positive and false
negative classifications grows with the number of objects,
and this becomes important when we have millions of ob-
jects, as in most modern sky surveys (PanSTARRS - 1.9
billion objects, 2MASS - 470 million objects, ALLWISE -
560 million objects), so it makes sense to make an effort to
increase the performance and these values (Tab. 2) are the
baselines we want to outperform.The work made use of the following Python software
packages: Sckit-learn 1.0.2 version, optuna 2.10.1 version,
Tabnet from Pytorch 4.0 version.
3.1. Random Forest
Thedecisiontreeconceptisquitesimilartotheclassical
colour selections that are traditionally used in astronomy
to classify objects. While automated decision trees can be
much more efficient than classic decision rules, they tend
to overfit, i.e. they learn too much about the data they are
trained on and can fail when applied to data they have not
seen before. The solution to this problem can be a random
forestapproach(RF)-anensembleofdecisiontrees.Inthis
case, the decision about the class an object belongs to, is
madebasedonwhichclassthegreaternumberoftreesvoted
for.
Using optuna, we selected the maximum tree depth, the
minimum number of samples required to split an internal
node, the criterion and the maximum number of features
in a node. Tuned parameters and corresponding scores are
presented in Tab. 3. Note that the scores are not guaranteed
and depend on the dataset the model is tested on, e.g. the
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 6 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
Table 2
Decision rules from the literature.
Author Rule MCC
Carnero Rosell et al. (2019) (𝑖−𝑧)>1.2,(𝑧−𝑌)>0.15,0.935
(𝑌𝐴𝐵−𝐽𝑉𝑒𝑔𝑎)>1.6, z< 22
Burningham et al. (2013) (𝑧−𝐽)𝑉𝑒𝑔𝑎>2.5,𝐽 < 17.50.921
/uni00000029/uni00000044/uni0000004f/uni00000056/uni00000048 /uni00000037/uni00000055/uni00000058/uni00000048
/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000029/uni00000044/uni0000004f/uni00000056/uni00000048 /uni00000037/uni00000055/uni00000058/uni00000048/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni0000001b/uni00000015/uni00000018 /uni00000014/uni00000018
/uni00000017/uni00000013 /uni0000001b/uni00000014/uni00000016
/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013
/uni00000029/uni00000044/uni0000004f/uni00000056/uni00000048 /uni00000037/uni00000055/uni00000058/uni00000048
/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000029/uni00000044/uni0000004f/uni00000056/uni00000048 /uni00000037/uni00000055/uni00000058/uni00000048/uni00000024/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000003/uni0000001b/uni00000015/uni00000015 /uni00000014/uni0000001b
/uni00000017/uni0000001c /uni0000001b/uni00000013/uni00000017
/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013
Figure 5: The confusion matrices for the decision rules
Carnero Rosell et al. (2019) on the top and Burningham et al.
(2013) on the bottom applied to the test part of the dataset.
imputingofmissingvalues.Foramoreobjectiveevaluation
of the model we use bootstrap technique (see confidence
intervals, Sec. 4 and Fig. 9).
In Fig. 6, an example of a two-dimensional cut shows
the separating boundary between classes defined by the RF
model. It should be noted that this is a slice in multidi-
mensional parameter space, so it does not represent the
number of mismatches. In fact, the performance is very
good,meaningthatthedecisionrulesunderlyingthatmodel
might be quite complex and cannot be represented on a 2D
diagram.
Fig. 10 shows the importance of features for the RF
model in all three cases. Seeing Pan-STARRS magnitudes,
the model primarily relies on 𝑖magnitudes. Without optical
magnitudes, 𝑖−𝑦becomesthemostimportantfeature.This
becomesevenmorepronouncedfortheRFmodelthatrelies
on colours only.
3
 2
 1
 0 1 2
i [mag]10.0
7.5
5.0
2.5
0.02.55.0z-yother classes
BDFigure 6: Slice of the separating boundary in the feature space
according to the RF model.
Table 3
Random forest hyperparameters for three sets of features.
Number of trees is 500 for all of the models. Number in max
features is the fraction of all available features.
Hyperparameter All features No PS magnitudes Only colours
Max depth 11 13 12
Min samples split 20 9 8
Max features sqrt sqrt 0.7
Criterion entropy entropy gini
MCC test score 0.983 0.986 0.975
MCC train score 0.987 0.990 0.990
Precision 0.992 0.992 0.988
Recall 0.992 0.994 0.987
3.2. XGBoost
Boosting is a very popular machine-learning technique.
It is a type of ensemble learning that uses the output of the
previous model as input to the next one. Instead of training
models individually, boosting trains models sequentially,
witheachnewmodeltrainedtocorrecttheerrorsofprevious
ones. At each iteration, correctly predicted results are given
lessweight,andincorrectlypredictedresultsaregivenmore
weight. It then uses the weighted average to get the final
result.
Wearealsointerestedinboostingfromthepointofview
that often a certain principle of handling missing values is
built into the algorithm. Two popular boosting models are
CatBoost (Prokhorenkova et al., 2017) and XGBoost (Chen
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 7 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
Table 4
XGBoost hyperparameters for three sets of features.
Hyperparameter All features No PS magnitudes Only colours
Max depth 15 15 10
Learning rate 0.340 0.126 0.033
Subsample 0.05 0.04 0.11
Gamma 0.323 0.548 0.996
Reg alpha 0.82 0.48 0.03
MCC test score 0.978 0.972 0.969
MCC train score 0.980 0.978 0.974
Precision 0.992 0.989 0.985
Recall 0.985 0.982 0.985
and Guestrin, 2016). CatBoost only has built-in filling in
the missing values with some specific values, but XGBoost
uses the following strategy: each node is assigned a default
solution, and this works well in many cases. Therefore,
we choose XGBoost for the task and also compare the
performance of the model on the missing values filled in by
thedefaultmethodofXGBoostandfillingwiththeIterative
Imputer.
On a test dataset with the default missing value algo-
rithm, XGBoost gives MCC = 0.96. When training and
testing on data in which we filled the missing values using
the Iterative Imputer, the performance reaches MCC =
0.986. Thus, we conclude that Iterative Imputer in this case
is not only a more robust method but also has a better effect
on model performance.
The number of estimators is fixed at 500. The following
hyperparameters were optimized with optuna: maximum
tree depth, learning rate (the step size reduction used dur-
ing the update to prevent overfitting), subsample ratio of
the training instances, reg_alpha (regularization term) and
gamma - the minimum loss reduction required to create a
furtherpartitiononaleafnodeofthetree.Optimizedvalues
are presented in Tab. 4.
ThefeatureimportancesispresentedinFig.10.Trained
on all features XGBoost relies mainly on the 𝑖magnitude
of the Pan-STARRS survey, as does RF. If Pan-STARRS
magnitudes are excluded from the feature list, 𝑖−𝑧,𝑧−𝐽
and𝑖−𝑦colours became the dominant features, however,
the𝐽magnitude of the 2MASS survey remains important.
3.3. Support Vector Machine
The support vector machine (Cortes and Vapnik, 1995)
is another widely used and well-developed method. The
principle of the support vector machine is to find a line,
surface or hypersurface that would separate classes in the
feature space. The fitting process maximizes the distance
fromeachpointtothedecisionboundary(referencevector).
WetunedtheregularizationparameterC,thekerneltype
and the kernel coefficient (’gamma’, for ’rbf’ kernel). The
decisionfunctionwassettoone-versus-one(’ovo’)sinceitis
3
 2
 1
 0 1 2
z [mag]3
2
1
0123z-Jother classes
BDFigure 7: Slice of the separating boundary in the space of
features according to the SVM model.
Table 5
SVM classifier hyperparameters for three sets of features.
Hyperparameter All features No PS magnitudes Only colours
Kernel rbf linear rbf
C 1.150 0.729 0.792
Gamma 0.298 0.066 0.453
MCC test score 0.981 0.984 0.958
MCC train score 0.982 0.980 0.968
Precision 0.989 0.986 0.972
Recall 0.992 0.998 0.986
abinaryclassificationandweightsofclasseswereautomat-
ically adjusted inversely proportional to class frequencies
in the input data. For tuned parameters in every case see
Tab.5.Itisseenfromthetablethatmodelsareverysimilar
for all of the cases and so are the most important features
(see Fig. 10). As SVM mainly rely on colour indices, the
importance distributions do not change significantly when
some or all of the magnitudes get excluded.
Fig. 7 shows a separating boundry constructed by the
SVMmodelusinga2Dcutasanexample.Itisworthnoting
thatthisisonlyasliceinwhichtheremainingfeaturevalues
are taken in some neighbourhood of the mean, so a large
numberofpointsthatfellintothewrongareadoesnotmean
an actual misclassification.
3.4. TabNet
TabNet(ArikandPfister,2019)isadeeplearningneural
networkthatutilisesattentiontoselectimportantfeaturesat
each step of the decision-making process so that only the
most important features are used. In this case, the choice of
features depends on the object, and, for example, it can be
differentforeachrowofthetrainingdataset.Intheend,you
can see which features the model has focused on the most.
TabNet consists of several steps, each step is a block of
components, with the number of steps being a hyperparam-
eter. Each step gets its vote in the final classification, which
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 8 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
Table 6
TabNet hyperparameters for the different sets of features.
Hyperparameter All features No PS magnitudes Only colours
N_d 16 12 12
N_a 16 28 12
Number of steps 3 3 2
Gamma 1.2 1.6 1.6
N_shared 2 2 1
MCC test score 0.983 0.986 0.975
MCC train score 0.987 0.990 0.990
Precision 0.992 0.992 0.988
Recall 0.992 0.994 0.987
3
 2
 1
 0 1 2 3
z-J4
2
024J-Hother classes
BD
Figure 8: Slice of the separating boundary in the space of
features according to the TabNet model.
mimics the ensemble classification. Other hyperparameters
are the width of the decision prediction layer (N_d), the
widthoftheattentionembeddingforeachmask(N_a),num-
berofsharedGatedLinearUnitsateachstep(N_shared)and
thecoefficientforfeaturereusageinthemasks(Gamma).We
fittedthe hyperparametersof TabNeton optunausing MCC
score as the metric. The fitted parameters are presented in
Tab. 6.
Themodelistrainedusinggradientdescentoptimization
withAdamastheoptimizer.Thevalidationpartofadataset
is used in training to prevent overfitting, so the result of the
model training is the configuration, that provides the best
scores, both on training and validation data. A separating
boundary is shown in Fig. 8, as it can be seen it is more
complex then the boundaries of other models.
4. Results and discussion
In this section, we summarise the results of the applica-
tionofmachinelearningmethodstothebrowndwarfsearch
problem.
We trained four models on the data: Random Forest
Classifier, SVM Classifier, XGBoost Classifier and TabNet
Classifier.ConfidenceintervalsfortheMCCscoresoftheobtained
models are presented in Fig. 9. Confidence intervals are
calculated via the bootstrap method with 100 samples half
the length of the test data set. The coloured box is the
interval from the 25th percentile to the 75th percentile and
the median value is represented by a black line. Error bars
showtheminimumandthemaximumvaluewiththeoutliers
marked as diamonds. The baseline values obtained using
decision rules from the literature are represented in Fig. 9
as dashed lines.
All models on the full set of features and the features
without PS1 magnitudes provide almost the same results,
butsomeperformedslightlybetter.Ifweuseonlymagnitude
features, the performance decreases. However, such models
still outperform the baselines.
Although the performance of the models is nearly the
same, they differ in terms of robustness. The importance
of features of the models is presented in Fig. 10. Random
Forest and XGBoost rely primarily on the 𝑖magnitude in
their decisions, if any. At the same time, SVM and Tabnet
on the full set of features seem more robust, as they rely
mainly on colour indices. Tabnet also tends to pay a lot of
attentiontothemagnitudes,althoughitperformsthebestin
the case of using only colours. Table 7 shows true positive,
true negative, false positive and false negative values for all
of the models obtained on the test part of the dataset.
According to CarneroRosell et al. (2019) andSkrzypek
et al. (2015), (𝑖−𝑧)𝑃𝑆1and𝑦𝑃𝑆1−𝐽colour indices are
expected to be the most important feature since they have
the largest variation across the M/L transition. While the
(𝑖−𝑧)𝑃𝑆1colour index is important to SVM classifiers and
insomecasesforRFandTabNet,mostofthemodelsdonot
consider them essential.
Itisexpectedthat 𝑧𝑃𝑆1−𝐽isthemostimportantfeature
asBurninghametal.(2013),whichweusedasabaseline,is
nearly entirely based on the above colour. However, it only
plays a secondary role in most of the models. Unlike the
previousworks,werevealedtheimportanceofthe (𝑖−𝑦)𝑃𝑆1
colour index. It is the most important feature in most cases
and the colour selection (𝑖−𝑦)𝑃𝑆1>1.88alone gives an
MCCscoreof 0.968onthetestingdata.Othercolourindices
could be important in the case of multi-class classification,
forexample,LandT-typedwarfsdifferdrasticallyin 𝑊1 −
𝑊2, although it has almost no relevance for the problem
investigated in the present article.
5. Conclusion
In this paper, we compiled a dataset of L and T type
browndwarfs(markedaspositiveclass)andobjectsofother
spectral types (marked as negative class) from literature.
It is important to acknowledge potential limitations and
biasesthatcouldimpactthedataset’srepresentativenessand
generalizability.Whilewehavecompiledthedatasetinsuch
a way as to reproduce the observed distribution in absolute
magnitudes, the distribution in apparent magnitudes may
be not representative, shifted towards brighter objects. This
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 9 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
Table 7
True positive (TP), true negative (TN), false positive (FP) and false negative (FN) values as well as Precison and Recall scores
for four models: Random Forest (RF), XGBoost, Support Vector Machine (SVM), and TabNet. Each model was trained on three
sets of features, labeled as "All features", "w/o PS magnitudes", and "only colours".
Model TP TN FP FN Precision Recall
Random Forest
All features 846 7 846 7 0.992 0.992
W/o PS magnitudes 848 7 833 5 0.992 0.994
Only colours 842 10 830 11 0.988 0.987
XGBoost
All features 840 6 834 13 0.992 0.985
W/o PS magnitudes 838 9 831 15 0.989 0.982
Only colours 840 13 827 13 0.985 0.985
SVM
All features 846 9 831 7 0.989 0.992
W/o PS magnitudes 851 12 828 2 0.986 0.998
Only colours 841 24 816 12 0.972 0.986
TabNet
All features 850 9 831 3 0.992 0.992
W/o PS magnitudes 851 12 828 2 0.992 0.994
Only colours 846 13 827 7 0.988 0.987
most likely does not affect the performance of the model in
thecaseof"onlycolours"mode,butmayhaveanimpacton
models trained on magnitudes too. The incompleteness in
M type dwarfs that stems from limitations in the Best et al.
(2018)catalogueintroducesabiasinthedatasettowardsthe
included objects, potentially impacting the generalizability
of the results.
Brown dwarfs are faint astronomical sources with peak
intensitiesthatfallintotheinfra-redpartofthespectrum,so
itishardertocarryoutthemeasurementsinopticalandfar-
red filters, such as 𝑖and𝑧. We imputed missing values with
Iterative Imputer and explored the result. We also imputed
the colour indices regardless of corresponding magnitudes
inordertoreducetheerrorofimputing.Formostofthemag-
nitude features, the imputing error is compatible with the
measurementerror.Forcolourindexfeatures,imputingerror
is usually much lower than the corresponding measurement
error.Thus,theimputingpartofpreprocessingisconsidered
to be successful.
Four models, namely, Random Forest Classifier, SVM
Classifier, XGBoost Classifier and TabNet Classifier were
trained to distinguish brown dwarfs among all objects ac-
cording to their photometric data. The classification results
for all models are consistently high, all models outperform
thebaselines.However,tree-likemodels(RFandXGBoost)
tend to exploit the faintness of the objects, which is less
preferable in terms of robustness of the model. On the
contrary, SVM primarily relies on colour indices, which
reduce the possibility of misclassification of other types of
faint sources.
Examiningthefeaturesofthemodels,wehavefoundthat
the(𝑖−𝑦)𝑃𝑆1colour index is the most important feature in
mostcases.Thiscolourindexcanbeusedsubsequentlyand
independently in brown dwarf classification problems. Wealso confirm that the 𝑧𝑃𝑆1−𝐽,(𝑖−𝑧)𝑃𝑆1and(𝑧−𝑦)𝑃𝑆1
colourindicesareimportantandpowerfulfeaturesinbrown
dwarf classification.
Along with the overall success of the work, there are
severallimitationstotheapplicabilityoftheresults,thatare
yet to be solved. First, potential biases of the dataset were
emphasised,includingabiastowardsbrighterobjects.These
biases can affect the generalizability and reliability of the
findings.Second,itisknown,thathighlyred-shiftedquasars
mightalsobeasourceofcontaminationwhendistinguishing
brown dwarfs from other types of objects (Carnero Rosell
et al., 2019). This issue can be resolved using colour cuts
in most cases, but can still remain for some cases (Reed
et al., 2017). At this point, we haven’t considered quasars.
Third, while the imputing magnitudes and colour indices
independentlyallowustopredictcolourindicesmuchmore
accurately, this leads to the colour index not being always
a linear combination of magnitudes involved. Multi-class
classification will also be in scope of future work.
Acknowledgements
This work was supported by by Non-commercial Foun-
dation for the Advancement of Science and Education IN-
TELLECT.
AuthorisgratefultoIlyaDyugay,KonstantinMalanchev,
Dana Kovaleva and Oleg Malkov for the fruitful discussion
and valuable advice. Author thanks the anonymous referee
forcarefulreadingandsuggestions,whichgreatlyhelpedto
improve the article.
This research has made use of NASA’s Astrophysics
Data System Bibliographic Services.
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 10 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
/uni00000035/uni00000029 /uni0000003b/uni0000002a/uni00000025/uni00000052/uni00000052/uni00000056/uni00000057 /uni00000036/uni00000039/uni00000030 /uni00000037/uni00000044/uni00000045/uni00000031/uni00000048/uni00000057/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000013/uni00000011/uni0000001c/uni00000019/uni00000013/uni00000011/uni0000001c/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000013/uni00000030/uni00000026/uni00000026/uni00000003/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000044/uni00000055/uni00000051/uni00000048/uni00000055/uni00000052/uni00000003/uni00000035/uni00000052/uni00000056/uni00000048/uni0000004f/uni0000004f/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni0000000b/uni00000015/uni00000013/uni00000014/uni0000001c/uni0000000c
/uni00000025/uni00000058/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni0000004b/uni00000044/uni00000050/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni0000000b/uni00000015/uni00000013/uni00000014/uni00000016/uni0000000c/uni00000024/uni0000004f/uni0000004f/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056
/uni00000035/uni00000029 /uni0000003b/uni0000002a/uni00000025/uni00000052/uni00000052/uni00000056/uni00000057 /uni00000036/uni00000039/uni00000030 /uni00000037/uni00000044/uni00000045/uni00000031/uni00000048/uni00000057
/uni0000003a/uni00000012/uni00000052/uni00000003/uni00000033/uni00000036/uni00000003/uni00000050/uni00000044/uni0000004a/uni00000051/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048/uni00000056
/uni00000035/uni00000029 /uni0000003b/uni0000002a/uni00000025/uni00000052/uni00000052/uni00000056/uni00000057 /uni00000036/uni00000039/uni00000030 /uni00000037/uni00000044/uni00000045/uni00000031/uni00000048/uni00000057/uni00000032/uni00000051/uni0000004f/uni0000005c/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000052/uni00000058/uni00000055/uni00000056
Figure 9: Confidence intervals for the scores of models
References
Kumar, S.S., The structure of stars of very low mass., The Astrophysical
Journal 137 (1963) 1121.
Hayashi, C. and Nakano, T., Evolution of stars of small masses in the pre-
main-sequence stages, Progress of Theoretical Physics 30 (1963) 460–
474.
Rebolo,R.,ZapateroOsorio,M.R.,andMartín,E.L., Discoveryofabrown
dwarf in the pleiades star cluster, Nature 377 (1995) 129–131.
Nakajima, T., Oppenheimer, B.R., Kulkarni, S.R., et al., Discovery of a
cool brown dwarf, Nature 378 (1995) 463–465.
Luhman, K.L., Discovery of a binary brown dwarf at 2 pc from the sun,
The Astrophysical Journal Letter 767 (2013) L1.
Burningham,B.,Cardoso,C.V.,Smith,L.,etal.,76tdwarfsfromtheukidss
las: benchmarks, kinematics and an updated space density, Monthly
Notices of the Royal Astronomical Society 433 (2013) 457–497.
CarneroRosell,A.,Santiago,B.,dalPonte,M.,etal., Browndwarfcensus
withthedarkenergysurveyyear3dataandthethindiscscaleheightof
early l types, Monthly Notices of the Royal Astronomical Society 489
(2019) 5301–5325.
Kirkpatrick,J.D.,Reid,I.N.,Liebert,J.,etal.,DwarfsCoolerthan“M“:The
Definition of Spectral Type “L” Using Discoveries from the 2 Micron
All-Sky Survey (2MASS), The Astrophysical Journal 519 (1999) 802–
833.
Skrzypek, N., Warren, S.J., and Faherty, J.K., Vizier online data catalog:
Photometric brown-dwarf classification (skrzypek+, 2016), VizieR
Online Data Catalog (2016) J/A+A/589/A49.
Kirkpatrick, J.D., Gelino, C.R., Faherty, J.K., et al., The field substellar
massfunctionbasedonthefull-sky20pccensusof525l,t,andydwarfs,
Astrophysical Journal Supplement Series 253 (2021) 7.
Mužić, K., Schödel, R., Scholz, A., et al., The low-mass content of the
massive young star cluster rcw 38, Monthly Notices of the Royal
Astronomical Society 471 (2017) 3699–3712.Smith, L., Lucas, P.W., Bunce, R., et al., High proper motion objects
fromtheUKIDSSGalacticplanesurvey, MonthlyNoticesoftheRoyal
Astronomical Society 443 (2014) 2327–2341.
Lodieu, N., Pérez-Garrido, A., Béjar, V.J.S., et al., Binary frequency of
planet-hoststarsatwideseparations.Anewbrowndwarfcompanionto
a planet-host star, Astronomy and Astrophysics 569 (2014) A120.
Artigau, É., Bouchard, S., Doyon, R., et al., Photometric variability of
the t2.5 brown dwarf simp j013656.5+093347: Evidence for evolving
weather patterns, The Astrophysical Journal 701 (2009) 1534 – 1539.
Gillon, M., Triaud, A.H.M.J., Jehin, E., et al., Fast-evolving weather
for the coolest of our two new substellar neighbours, Astronomy &
Astrophysics 555 (2013) L5.
Khandrika,H.G.,Burgasser,A.J.,Melis,C.,etal., Asearchforphotometric
variabilityinl-andt-typebrowndwarfatmospheres, TheAstronomical
Journal 145 (2013).
Marley, M.S. and Robinson, T.D., On the cool side: Modeling the atmo-
spheresofbrowndwarfsandgiantplanets,AnnualReviewofAstronomy
and Astrophysics 53 (2015) 279–323.
Charnay, B., Bézard, B., Baudino, J.L., et al., A self-consistent cloud
model for brown dwarfs and young giant exoplanets: Comparison with
photometricandspectroscopicobservations, TheAstrophysicalJournal
854 (2018) 172.
Tremblin, P., Padioleau, T., Phillips, M.W., et al., Thermo-compositional
diabatic convection in the atmospheres of brown dwarfs and in earth’s
atmosphere and oceans, The Astrophysical Journal 876 (2019) 144.
Tan, X. and Showman, A.P., Atmospheric variability driven by radiative
cloud feedback in brown dwarfs and directly imaged extrasolar giant
planets, The Astrophysical Journal 874 (2019) 111.
Burningham,B.,Marley,M.S.,Line,M.R.,etal., Retrievalofatmospheric
propertiesofcloudyldwarfs, MonthlyNoticesoftheRoyalAstronom-
ical Society 470 (2017) 1177–1197.
Saumon, D. and Marley, M.S., The evolution of l and t dwarfs in color-
magnitudediagrams, TheAstrophysicalJournal689(2008)1327–1344.
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 11 of 12ML for L&T Brown Dwarfs Search in Sky Surveys
0.00 0.02 0.04 0.06 0.08 0.10i
i-y
i-z
z-J
z
y-J
z-y
J
y
H
K
W1
Ks-W1
W1-W2
H-Ks
J-H
W2All featuresRandom Forest
0.00 0.05 0.10 0.15 0.20i
z-J
i-y
i-z
Ks-W1
H-Ks
z-y
K
z
W1-W2
J-H
J
W2
W1
y
H
y-JXGBoost
0.00 0.02 0.04 0.06 0.08i-y
i-z
z-y
z-J
i
y-J
z
W1-W2
H-Ks
J-H
y
J
Ks-W1
H
K
W1
W2SVM
0.00 0.05 0.10 0.15 0.20i-y
i-z
z
z-J
i
J-H
z-y
y-J
Ks-W1
H-Ks
y
W1
H
J
W2
K
W1-W2T abnet
0.000 0.025 0.050 0.075 0.100 0.125i-y
i-z
z-J
J
z-y
H
K
y-J
Ks-W1
W1-W2
W1
W2
H-Ks
J-HW/o PS magnitudes
0.00 0.02 0.04 0.06 0.08 0.10 0.12i-z
z-J
i-y
J
z-y
W1-W2
H-Ks
y-J
H
Ks-W1
W1
J-H
K
W2
0.000 0.025 0.050 0.075 0.100 0.125z-y
i-y
z-J
i-z
J
H
K
Ks-W1
y-J
J-H
H-Ks
W1-W2
W1
W2
0.00 0.05 0.10 0.15 0.20z-J
i-z
J
z-y
y-J
H
i-y
W1
J-H
H-Ks
Ks-W1
W1-W2
K
W2
0.00 0.05 0.10 0.15 0.20 0.25 0.30i-y
z-J
i-z
z-y
H-Ks
J-H
Ks-W1
W1-W2
y-JOnly colours
0.0 0.1 0.2 0.3i-y
z-J
i-z
W1-W2
z-y
y-J
Ks-W1
J-H
H-Ks
0.00 0.02 0.04 0.06 0.08 0.10 0.12i-y
z-y
i-z
z-J
y-J
Ks-W1
H-Ks
J-H
W1-W2
0.00 0.05 0.10 0.15 0.20i-z
z-J
z-y
i-y
H-Ks
y-J
Ks-W1
J-H
W1-W2
Figure 10: Importance of features for all models. For RF, XGBoost and SVM models we calculate the importance of each feature
using SHAPand for TabNet we use built-in capabilities of the model.
Vos, J.M., Allers, K., Apai, D., et al., Astro2020 white paper: The l/t
transition (2019).
Maravelias, G., Bonanos, A.Z., Tramper, F., et al., A machine-learning
photometricclassifierformassivestarsinnearbygalaxiesi.themethod,
arXiv e-prints (2022) arXiv:2203.08125.
Pedregosa, F., Varoquaux, G., Gramfort, A., et al., Scikit-learn: Machine
learning in Python, Journal of Machine Learning Research 12 (2011)
2825–2830.
Mohammadi, M., Mutatiina, J., Saifollahi, T., et al., Detection of extra-
galactic ultra-compact dwarfs and globular clusters using explainable
AI techniques, Astronomy and Computing 39 (2022) 100555.
Best,W.M.J.,Magnier,E.A.,Liu,M.C.,etal., Photometryandpropermo-
tionsofm,l,andtdwarfsfromthepan-starrs13 𝜋survey, Astrophysical
Journal Supplement Series 234 (2018) 1.
Chambers, K.C., Magnier, E.A., Metcalfe, N., et al., The pan-starrs1
surveys, arXiv e-prints (2016) arXiv:1612.05560.
Cutri,R.M.,Skrutskie,M.F.,vanDyk,S.,etal., Vizieronlinedatacatalog:
2mass all-sky catalog of point sources (cutri+ 2003), VizieR Online
Data Catalog (2003) II/246.
Cutri, R.M., Wright, E.L., Conrow, T., et al., Vizier online data catalog:
Allwisedatarelease(cutri+2013), VizieROnlineDataCatalog(2021)
II/328.
Collaboration, G., Prusti, T., de Bruijne, J.H.J., et al., The gaia mission,
Astronomy and Astrophysics 595 (2016) A1.
Collaboration, G., Vallenari, A., Brown, A.G.A., et al., Gaia data release
3:Summaryofthecontentandsurveyproperties, arXive-prints(2022)arXiv:2208.00211.
Akiba, T., Sano, S., Yanase, T., et al., Optuna: A next-generation hyperpa-
rameter optimization framework (2019).
Lundberg, S.M. and Lee, S.I., A unified approach to interpreting model
predictions, in: Guyon, I., Luxburg, U.V., Bengio, S., et al. (Eds.),
Advances in Neural Information Processing Systems 30, Curran Asso-
ciates, Inc., 2017, pp. 4765–4774. URL: http://papers.nips.cc/paper/
7062-a-unified-approach-to-interpreting-model-predictions.pdf .
Prokhorenkova, L., Gusev, G., Vorobev, A., et al., CatBoost: un-
biased boosting with categorical features, arXiv e-prints (2017)
arXiv:1706.09516.
Chen,T.andGuestrin,C.,Xgboost:Ascalabletreeboostingsystem(2016).
Cortes, C. and Vapnik, V., Support-vector networks, Machine learning 20
(1995) 273–297.
Arik, S.O. and Pfister, T., Tabnet: Attentive interpretable tabular learning,
arXiv e-prints (2019) arXiv:1908.07442.
Skrzypek, N., Warren, S.J., Faherty, J.K., et al., Photometric brown-dwarf
classification, Astronomy & Astrophysics 574 (2015) A78.
Reed, S.L., McMahon, R.G., Martini, P., et al., Eight new luminous z ≥6
quasars discovered via sed model fitting of vista, wise and dark energy
surveyyear1observations, MonthlyNoticesoftheRoyalAstronomical
Society 468 (2017) 4702–4718.
Aleksandra Avdeeva et al.: Preprint submitted to Elsevier Page 12 of 12