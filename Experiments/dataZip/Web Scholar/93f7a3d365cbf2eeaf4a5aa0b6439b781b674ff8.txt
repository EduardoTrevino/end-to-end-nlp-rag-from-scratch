Published as a conference paper at ICLR 2023
COUPLED MULTIWAVELET NEURAL OPERATOR LEARN -
ING FOR COUPLED PARTIAL DIFFERENTIAL EQUATIONS
Xiongye Xiao1ËšDefu Cao1ËšRuochen Yang1Gaurav Gupta1
Gengshuo Liu1Chenzhong Yin1Radu Balan2Paul Bogdan1
1University of Southern California, Los Angeles, CA 90089, USA
2University of Maryland, College Park, MD 20742, USA
{xiongyex,defucao,ruocheny,ggaurav,gengshuo,chenzhoy,pbogdan }@usc.edu ,
rvbalan@umd.edu
ABSTRACT
Coupled partial differential equations (PDEs) are key tasks in modeling the complex
dynamics of many physical processes. Recently, neural operators have shown the
ability to solve PDEs by learning the integral kernel directly in Fourier/Wavelet
space, so the difficulty for solving the coupled PDEs depends on dealing with the
coupled mappings between the functions. Towards this end, we propose a coupled
multiwavelets neural operator (CMWNO) learning scheme by decoupling the
coupled integral kernels during the multiwavelet decomposition and reconstruction
procedures in the Wavelet space. The proposed model achieves significantly higher
accuracy compared to previous learning-based solvers in solving the coupled PDEs
including Gray-Scott (GS) equations and the non-local mean field game (MFG)
problem. According to our experimental results, the proposed model exhibits a
2Ë†â€ 4Ë†improvement relative L2 error compared to the best results from the
state-of-the-art models.
1 I NTRODUCTION
Human perception relies on detecting and processing waves. While our eyes detect waves of
electromagnetic radiation, our ears detect waves of compression in the surrounding air. Going
beyond waves, from complex dynamics of blood flow to sustain tissue growth and life, to navigating
underwater, ground and aerial vehicles at high speeds requires discovering, learning and controlling
the partial differential equations (PDEs) governing individual or webs of biological, physical and
chemical phenomena (Lacasse et al., 2007; Henriquez, 1993; Laval & Leclercq, 2013; Ghanavati
et al., 2017; Radmanesh et al., 2020). Within this context, neural operators have been successfully
used to learn and solve various PDEs. By representing the integral kernel termed as Greenâ€™s function
in the Fourier or Wavelet spaces, the fourier neural operator (Li et al., 2020b) and the multiwavelet-
based neural operator (Gupta et al., 2021b;a)) exhibit significant improvements on solving PDEs
compared with previous work. However, when it comes to coupled systems characterized by coupled
differential equations such as mean field games (MFGs) (Lasry & Lions, 2007; Achdou & Capuzzo-
Dolcetta, 2010), analysis of coupled cyber-physical systems (Xue & Bogdan (2017), or analysis of the
surface currents in the tropical Pacific Ocean Bonjean & Lagerloef (2002), the interactions between
the variables/functions need to be considered to decouple the system. Without the knowledge of
underlying PDEs, the complex interactions can be hardly represented in the data-driven model. To
build a data-driven model that can give a general representation of the interactions to efficiently solve
coupled differential equations, we propose the coupled multiwavelets neural operator (CMWNO).
Neural Operators. Neural operators (Li et al., 2020b;c;a; Gupta et al., 2021b; Bhattacharya et al.,
2020; Patel et al., 2021) focus on learning the mapping between infinite-dimensional spaces of
functions. The critical feature for neural operators is to model the integral operator namely the
Greenâ€™s function through various neural network architectures. The graph neural operators (Li et al.,
2020b;c) use the graph kernel to model the integral operator inspired by graph neural networks; the
ËšEqual Contribution.
1arXiv:2303.02304v4  [cs.LG]  8 Dec 2023Published as a conference paper at ICLR 2023
Fourier neural operator (Li et al., 2020b) uses an iterative architecture to learn the integral operator
in Fourier space. The multiwavelet neural operators (Gupta et al., 2021b;a) utilize the non-standard
form of the multiwavelets to represent the integral operator through 4 neural networks in the Wavelet
space. The neural operators are completely data-driven and resolution independent by learning
the mapping between the functions directly, which can achieve the state-of-the-art performance on
solving PDEs and initial value problems (IVPs). To deal with coupled PDEs in the coupled system
and be data-efficient, we aim to decode the various interaction scenarios inside the neural operators.
Multiwavelet Transform. In contrast to wavelets, multiwavalets (refer to Appendix C) use more
than one scaling functions which are orthogonal. The multiwavelets exploit the advantages of
wavelets, such as ( i) the vanishing moments, ( ii) the orthogonality, and ( iii) the compact support.
Along the essence of wavelet transform, a series of wavelet bases are introduced with scaled/shifted
versions in multiwavelets to construct the basis of the coarsest scale polynomial subspace. The
multiwavelet bases have been proved to be successful for representing integral operators as shown
in (Alpert et al., 1993) (the discrete version of multiwavelets) and (Alpert, 1993b). In our proposed
model, to develop compactly supported multiwavelets, we use the Legendre polynomials (Appendix
D) which are non-zero only over a finite interval as the basis. The differential ( B{Bt) and the integral
(Å¥
â„¦) operators can be represented by the first-order multiwavelet coefficients ( sandd) of orthogonal
bases via decomposition in the Wavelet space.
Mean Field Games (MFGs). As a representative problem for coupled systems in the real world,
MFGs gains raising attentions in various areas, including economics (Achdou et al., 2014; 2022),
finance (Gu Â´eant et al., 2011; Huang et al., 2019) and engineering (De Paola et al., 2019; Gomes
et al., 2021), etc. Building on statistical mechanics principles and infusing them into the study of
strategic decision making, MFGs investigate the dynamics of a large population of interacting agents
seen as particles in a thermodynamic gas. Simply speaking, MFGs consist of ( i) a Fokkerâ€“Planck
equation (or related PDE) that describes the dynamics of the aggregate distribution of agents, which
is coupled to ( ii) a Hamiltonâ€“Jacobiâ€“Bellman equation (another PDE) prescribing the optimal control
of an individual (Lasry & Lions, 2006; 2007; Huang et al., 2006; 2007). Among different types of
MFGs, the class of non-potential MFGs system with mixed couplings is particularly important as it is
more reflective of the real world with a continuum of agents in a differential game.
Solutions on MFGs. Previous works either only restrict to systems without non-local coupling,
such as alternating direction method of multipliers (ADMM) (Benamou & Carlier, 2015; Benamou
et al., 2017) and primal-dual hybrid gradient (PDHG) algorithm (Briceno-Arias et al., 2019; 2018)
or use general purpose numerical methods for solving the MFG problems (Achdou et al., 2013a;b;
Achdou & Capuzzo-Dolcetta, 2010), which misses specific information from the target structure. In
addition, the aforementioned works are not parallelizable with linear computational cost under the
coupled MFGs settings. Recently, (Liu & Nurbekyan, 2020) considers dual variables of nonlocal
couplings in Fourier or feature space. Furthermore, (Liu et al., 2021) expands the feature-space in
the kernel-based representations of machine learning methods and uses expansion coefficients to
decouple the mean field interactions. However, both dual variables and expansion coefficients need
to bound the interactions of coupled system in a reasonable interval with prior knowledge. In our
work, we first introduce the neural operator into coupled MFG fields, which can decouple the various
interactions inside the multiwavelet domain.
Novel Contributions. The main novel contributions of our work are summarized as follows:
â€¢For coupled differential equations, we propose a coupled neural operator learning scheme,
named CMWNO. To the best of our knowledge, CMWNO is the first neural operator work
using pure data-driven method to decouple and then solve coupled differential equations.
â€¢Utilizing multiwavelet transform, CMWNO can deal with the interactions between the
kernels of coupled differential equations in the Wavelet space. Specifically, we first yield
the representation of coupled information during the decomposition process of multiwavelet
transform. Then, the decoupled representation can interact separately to help the operatorsâ€™
reconstruction process. In addition, we propose a dice strategy to mimic the information
interaction during the training process.
2Published as a conference paper at ICLR 2023
â€¢The proposed model successfully learns the interaction between the coupled variables
when the couple degree is increasing and thus it could open new directions for studying
complex coupled systems via data-driven methods. Experimentally, the proposed CMWNO
framework offers the state-of-the-art performance on both Gray-Scotts (GS) equations and
non-local MFGs. Specifically, CMWNO outperforms the best baseline (MWT c) by54.0%
on GS equations with various resolutions and outperforms the best baseline (FNO c) by
61.4% on non-local MFGs with different time steps.
2 C OUPLED MULTIWAVELET NEURAL OPERATORS LEARNING
To solve a coupled control system characterized by coupled state equations in control theory, a
popular way is to use the Laplace operator sto represent differential and integral operators (Gilbarg
et al., 1977). Therefore, the coupled high-order differential equations can be transformed into the
first-order differential equations in the Laplace space which will reduce the decoupling difficulty.
Inspired by the use of the Laplace operator and the properties of the multiwavelets, we assume that
the interactions between kernels can be used to approximate the coupled information by reducing the
degree of high-order operators in multiwavelet bases. With this assumption, we are able to build the
coupled multiwavelet neural operators (CMWNO) learning scheme, which utilizes decomposition
representation from the operator and mimic the interaction via a dice strategy.
2.1 C OUPLED DIFFERENTIAL EQUATIONS
To provide a simple example of the coupled kernels, Îº1andÎº2, let us consider a general coupled
system with 2 coupled variables upx, tqandvpx, tqwith the given initial conditions u0pxqandv0pxq.
Given AandUas two Sobolev spaces Hs,pwithsÄ…0, pâ€œ2, letTdenote a generic operator such
thatT:AÃ‘U. Without the knowledge of how these two variables are coupled, to solve for upx, Ï„q
andvpx, Ï„q, we need two operators T1andT2such that T1u0pxqâ€œupx, Ï„qandT2v0pxqâ€œvpx, Ï„q.
The coupled kernels termed as Greenâ€™s function can be written as follows:
T1u0pxqâ€œÅ¼
DÎº1px, y, u 0pxq, u0pyq, v0pxq, v0pyq, Îº2qu0pyqdy,
T2v0pxqâ€œÅ¼
DÎº2px, y, u 0pxq, u0pyq, v0pxq, v0pyq, Îº1qv0pyqdy,
upx,0qâ€œu0pxq;vpx,0qâ€œv0pxq, xPD,(1)
where DÄ‚Rdis a bounded domain. The interacted kernels cannot be directly solved without
considering the interference from the other kernel, and our idea is to simplify the kernels first and
deal with the interactions in the multiwavelet domain.
2.2 M ULTIWAVELET OPERATOR
To briefly introduce the multiwavelet operator, we explain how the neural networks are used to
represent the kernel in this section. The basic concept of multiresolution analysis (MRA) and
multiwavelets (Alpert et al., 1993; Alpert, 1993a;b) are provided in the Appendix C.
Notation ForkPZandnPN, the space of piecewise polynomial functions is defined as:
Vk
nâ€œ tf|the restriction of fto the interval p2Â´nl,2Â´npl`1qqis a polynomial of degree Äƒk,
for all lâ€œ0,1, . . . ,2nÂ´1, and f vanishes elsewhere u.Vk
0consists of the orthogonal scaling
functions Ï†iwithiâ€œ0, . . . , kÂ´1, andVk
ncan be spanned by shifting and scaling these functions
asÏ†n
jlpxqâ€œ2n{2Ï†jp2nxÂ´lq, where jâ€œ0, . . . , kÂ´1andlâ€œ0, . . . ,2nÂ´1. The coefficients of
Ï†n
jlpxqare called scaling coefficients marked as sn
jl. The multiwavelet subspace Wk
nis defined as the
orthogonal complement of Vk
ninVk
n`1such that Vk
nÃ€Wk
nâ€œVk
n`1,Vk
nKWk
n.Wk
0consists of
the orthogonal wavelet functions Ïˆiwithiâ€œ0, . . . , kÂ´1. Similar to Vk
n,Wk
nis composed of the
wavelets functions Ïˆn
jlpxqwith wavelets coefficients dn
jl.
To represent the functions and learn the mapping in multiwavelet space, the nonstandard form is
used to represent the integral operator. According to (Beylkin et al., 1991; Alpert et al., 2002b), an
3Published as a conference paper at ICLR 2023
orthogonal projection operator Pk
n:Hs,2Ã‘Vk
n, and Qk
n:Hs,2Ã‘Wk
nwithQk
nâ€œPk
n`1Â´Pk
n,
then an single operator Tin our coupled system can be represented as:
Tâ€œÂ¯Tk
0`8Ã¿
nâ€œ0pAk
n`Bk
n`Ck
nq, (2)
where Â¯Tk
0â€œPk
0TPk
0, Ak
nâ€œQk
nTQk
n, Bk
nâ€œQk
nTPk
n, Ck
nâ€œPk
nTQk
n,Qk
nis the multi-
wavelet operator. Therefore, the nonstandard forms of the operator is a collection of triplets
tÂ¯Tk
0,pAk
i, Bk
i, Ck
iqnâ€œ0,1,...u. For a given operator T:Tu0pxq â€œuÏ„pxq, the map under wavelet
space can be written as:
Ti
d lâ€œAk
idi
l`Bk
isi
l, Ti
Ë†s lâ€œCk
idi
l, T0
s lâ€œÂ¯Ts0
l, iâ€œ0,1, . . . , n (3)
where,pTi
s l, Ti
d lq{psi
l, di
lqare the scaling/wavelet coefficients of uÏ„pxq{u0pxqin subspace Vk
i`1. In
our model, one kernel is approximated using 4 simiple neural networks A, B, C andÂ¯Tsuch that
Ti
d lÂ«AÎ¸Apdi
lq`BÎ¸Bpsi
lq, Ti
Ë†s lÂ«CÎ¸Cpdi
lq, and T0
s lÂ«Â¯TÎ¸Â¯TpsL
lq.
2.3 C OUPLED MULTIWAVELETS MODEL
This section introduces a coupled multiwavelets model to provide a general solution on coupled
differential equations. First, we make a mild assumption to decouple two coupled operators given in
Section 2.1. To simplify eq. 1, without loss of generality, we assume that we can build two operators
TuandTvto approximate upx, Ï„qandvpx, Ï„q, where TuandTvare decoupled and do not carry any
interference from each other. In other words, we can write Tuu0pxqâ€œu1px, Ï„q;Tvv0pxqâ€œv1px, Ï„q,
where u1px, Ï„qandv1px, Ï„qare the approximations of upx, Ï„qandvpx, Ï„qwithout coupling. The
assumption is mild and easy to get satisfied in the Wavelet space since the operators can be represented
by the first-order multiwavelet coefficients. According to this assumption, we can derive the following
relations:
upx, Ï„qâ€œTuupx,0q`Ïµ1pTvq, xPD
vpx, Ï„qâ€œTvvpx,0q`Ïµ2pTuq, xPD(4)
where Ïµ1pTuqquantifies the interference from operator Tvto solve upx, Ï„qandÏµpTvqrepresents the
measurable interaction from operator Tu. Therefore, the integral operators can be written as:
Tuu0pxqâ€œÅ¼
DÎºupx, yqu0pyqdy;Tvv0pxqâ€œÅ¼
DÎºvpx, yqv0pyqdy, (5)
the kernels ÎºuandÎºvtermed as Greenâ€™s functions can be learned through neural operators, where
Îºucan be learned using the data of uwhile the kernel Îºvis learned from v. To model Ïµ1pTuqand
Ïµ2pTvq, we transform the operators into multiwavelet coefficients in the Wavelet space and embed it
through simple linear combination after the decomposition steps.
Based on the concept of multiwavelets (Appendix Section C), here we simply explain the de-
composition step and reconstruction step of multiwavelets in our coupled system. Since Vk
nâ€œ
Vk
nÂ´1Ã€Wk
nÂ´1according to Section 2.2, the bases of Vk
ncan be written as a linear combi-
nation of the scaling functions Ï†nÂ´1
i and the wavelet functions ÏˆnÂ´1
i. The linear coefficients
pHp0q, Hp1q, Gp0q, Gp1qqare termed as multiwavelet decomposition filters, transforming represen-
tation between subspaces Vk
nÂ´1,Wk
nÂ´1, andVk
n. For a given function fpxq, the scaling/wavelet
coefficients sn
jl/dn
jlof scaling/wavelet functions Ï†n
jl/Ïˆn
jlare computed as:
sn
jlâ€œÅ¼2Â´npl`1q
2Â´nlfpxqÏ†n
jlpxqdx;dn
jlâ€œÅ¼2Â´npl`1q
2Â´nlfpxqÏˆn
jlpxqdx. (6)
Using the multiwavelet decomposition filters, the relations between the coefficients on two consecutive
levels nandn`1are computed as (decomposition step):
sn
lâ€œHp0qsn`1
2l`Hp1qsn`1
2l`1;dn
lâ€œGp0qsn`1
2l`Gp1qsn`1
2l`1. (7)
Therefore, starting with the coefficients sn
l, we repeatedly apply the decomposition step in eq. 7 to
compute the scaling/wavelet coefficients on coarser levels. Similarly, the reconstruction step can be
represented as:
sn`1
2lâ€œHp0qTsn
l`Gp0qTdn
l,sn`1
2l`1â€œHp1qTsn
l`Gp1qTdn
l. (8)
4Published as a conference paper at ICLR 2023
H, Gâ€¦+H, G!ğ‘‡!,#$+Decomposition                                                              â€¦â€¦â€¦Reconstructionğ‘‡!ğ‘‡"ACB+ğ‘‡%,&'ğ‘‡%,Ì‚&$ğ‘‡%,#$ğ‘ˆ',&$ğ‘ˆ',#$ğ‘ˆ',&'ğ‘‡%,#$ğ‘ˆ),&'=!ğ‘‡!,&'+ğ‘‡%,&'!ğ‘‡!,Ì‚&$+ğ‘‡%,Ì‚&$ğ‘ˆ),&$*+
H, Gâ€¦+H, G!ğ‘‡%,#$+â€¦â€¦â€¦ACB+ğ‘‡!,&'ğ‘‡!,Ì‚&$ğ‘‡!,#$ğ‘‰',&$ğ‘‰',#$ğ‘‰',&'ğ‘‡!,#$!ğ‘‡%,Ì‚&$+ğ‘‡!,Ì‚&$ğ‘ˆ),&$
ğ‘‰),&'=!ğ‘‡%,&'+ğ‘‡!,&'ğ‘‰),&$ğ‘‰),&$*+ğ‘ˆ',&$*+
ğ‘‰',&$*+!ğ‘‡
!ğ‘‡
Figure 1: Architecture of CMWNO. Note that there are two coupled operators, TuandTv, in our system, which
aligns the number of coupled variables. The network Â¯Tis only applied for the coarsest scale L(0 in this system).
The dashed arrows correspond to the auxiliary information from the unused operator without gradient during
training process. For the interaction between operators, when we update the operator Tu, the decomposed
ingredients from Tvwill be equipped into the reconstruction module of Tuin the Wavelet domain, vice versa.
Repeatedly applying the reconstruction step, we can compute the coefficients sn
lfrom s0
land
di
l, iâ€œ0, . . . , n . In general, the function can be parameterized as the scaling/wavelet coefficients in
the Wavelet space after the decomposition steps, and the coefficients can be mapped to the function
after reconstruction steps. In our work, to model the interference Ïµ1pTuqandÏµ2pTvq, we obtain the
multiwavelets coefficients of each kernel during the decomposition steps and embed them into the
other kernel in the reconstruction step. Note that we will elaborate the detailed training strategy of
how to mimic interactions inside our system in Section 2.4.
Our idea is to represent the functions and operators in Wavelet space to decouple the system using
simple linear combinations. Considering the example in Section 2.1, according to the eq. 4 and
5, we first build two operators TuandTvsuch that Tuu0pxqâ€œu1
Ï„pxq;Tvv0pxqâ€œv1
Ï„pxq. For the
operators TuandTv, we denote their scaling/wavelet coefficients in wavelet domain as Ti
u,sl;Ti
u,dl
andTi
v,sl;Ti
v,dlrespectively. For the input u0pxq;v0pxqand the output uÏ„pxq;vÏ„pxq, we denote their
coefficients as Ui
0,spdql;Vi
0,spdqlandUi
Ï„,spdql;Vi
Ï„,spdql. According to eqs. 3 and 5, the multiwavelet
coefficients of TuandTvcan be calculated as:
Ti
u,d lâ€œAk
u,iUi
0,d l`Bk
u,iUi
0,s l, Ti
u,Ë†s lâ€œCk
u,iUi
0,d l, T0
u,s lâ€œÂ¯TU0
0,s l;
Ti
v,d lâ€œAk
v,iVi
0,d l`Bk
v,iVi
0,s l, Ti
v,Ë†s lâ€œCk
v,iVi
0,d l, T0
v,s lâ€œÂ¯TV0
0,s l,(9)
where iâ€œ0,1, . . . , n . Considering the interference from the other operators, the coefficients of the
solutions uÏ„pxqandvÏ„pxqin the Wavelet space can be written as:
Ui
Ï„,d lâ€œTi
u,d l`Ë†Ti
v,d l, Ui
Ï„,Ë†s lâ€œTi
u,Ë†s l`Ë†Ti
v,Ë†s l, U0
Ï„,s lâ€œT0
u,s l`Ë†T0
v,s l;
Vi
Ï„,d lâ€œTi
v,d l`Ë†Ti
u,d l, Vi
Ï„,Ë†s lâ€œTi
v,Ë†s l`Ë†Ti
u,Ë†s l, V0
Ï„,s lâ€œT0
v,s l`Ë†T0
u,s l; (10)
where iâ€œ0,1, . . . , n . In the training process, the inputs of the neural networks
tAru,vs, Bru,vs, Cru,vs,Â¯Tru,vsuare the multiwavelet coefficients of u0pxq;v0pxq, and the outputs
are the multiwavelet coefficients of Tu;Tv. When the neural networks tAu, Bu, Cu,Â¯Tuuare trained
forTu, the neural networks tAv, Bv, Cv,Â¯Tvuoutput Ti
v,rsl,dlswithout backpropagation, we use
Ë†Ti
ru,vs,rsl,dlsto mark the coefficients without gradient. Utilizing the orthogonality of the multi-
wavelets, the coefficients embedding the information of the operators Tu;Tvcan be directly added to
Tv;Tuin the same Wavelet space Vk
n, then the neural networks with backpropagation can learn the
5Published as a conference paper at ICLR 2023
information from the other operator. In that way, the complex coupled equations can be solved via
reducing the order of the functions and directly approximate decoupled functions at each iteration.
The architecture of the CMWNO is shown in Fig. 1, which illustrates the mapping process inside the
wavelet space of layer n. The operations inside the wavelet space can be matched by the order of
layers in the models, which means the decomposition operations for different resolutions are done
independently. After decomposing snvia eq. 7, we can get the transferred information of input where
each component will be used to reconstruct the original input at the layer n.
2.4 D ICE STRATEGY
â€¦â€¦â€¦â€¦Dice = 1RecDecRecDecRecDecRecDecğ‘ˆ!,#$%&ğ‘‰!,#$%&ğ‘‰!,#$%&ğ‘ˆ!,#$%&RecDecRecDecRecDecRecDec(ğ‘ˆ!,#$%&,ğ‘‰!,#$%&)
(ğ‘‰!,#$%&,ğ‘ˆ!,#$%&)
Figure 2: Dice strategy. For each sample, one only needs
to go through a specific path (round diagonal corner rect-
angle). Inside each path, the order of updating is from
left to right, where the darker block indicates the oper-
ator we want to update and the lighter blocks provide
decomposition information from the fixed operator.Inspired by scheduled sampling (Bengio et al.,
2015) , which is designed to gently bridge the
discrepancy between training and inference sam-
ples, we propose rolling the dice to randomly
decide the interaction order between each neural
operators, which is named dice strategy. Specifi-
cally, we roll the dice for every sample to decide
which path to use, which can effectively mitigate
the imbalance update problem for each kernel
caused by the fixed training order. As illustrated
in Fig. 2, when the dice tells the model to use
path 1 (upper path) , we will update operator Tu
by equipping the coupled information from the
other operator Tvfirst. Note that, Tvis learned
by previous samples and have not updated yet.
Then we use the updated operator Tuto decom-
pose the initial state u0, which can be used to
update Tv. Inside the Wavelet space with well-defined basis, where we are able to utilize vary
orthogonal information from each initial state jointly. Note that this strategy is scalable to more
operators referring to Fig. 6 in Supplementary and we left the design of this strategy for future work.
3 E XPERIMENTS
In this section, we empirically evaluate the proposed model on famous coupled PDEs such as the
Gray-Scott (GS) equations and the non-local mean field game (MFG) problem characterized by
coupled PDEs. Note that we compare against the state-of-the-art data-driven models which fits for
our research goal to build efficient coupled operators for general downstream data-driven applications
without sufficient expert knowledge. The experiments show that CMWNO not only achieves the
lowest L2relative errors when solving coupled PDEs, but also works consistently great under
different input conditions. For the data structure, since our datasets are functions, we apply point-wise
evaluations on the input and output data. For example, for the function fpxq, xPD, we discretize
the domain as x1, . . . , x sPD, where xiare s-point discretization of the domain. Unless stated
otherwise, we train on 1000 samples and test on 200samples.
Model architecture. In our proposed model, for each operator, the neural networks A, B andC
use a single-layered convolutional neural networks while Â¯Tuses a single linear layer. Our model is
extensible and each kernel constructed by 4 neural networks tA, B, C, Â¯Tulearning the mapping in
wavelet space. The number of the kernels can be chosen based on the number of coupled variables or
the number of explicit operators.
Benchmark models. We compare our model with the state-of-the-art neural operators including
Fourier neural operator ( FNO ), Multiwavelet-based neural operator ( MWT ), and Pad Â´e exponential
model ( PadÂ´e), which show the best performance on solving PDEs according to the experiment results
in (Li et al., 2020b;a; Gupta et al., 2021b;a). For the benchmark neural operator models, since we
have the coupled functions as input and output (e.g., uandv), we concatenate uandvfor the models
and marked the models as FNO c, MWT c, and Pad Â´ec. We also use two single multiwavelet-based
6Published as a conference paper at ICLR 2023
neural operators to learn uÏ„pxq;vÏ„pxqfrom vÏ„pxq;uÏ„pxqindependently and mark the model as
MWT s.
Similar to the coupling structure of our CMWNO, by creating the multiple kernels learned in Fourier
space and applying the dice strategy during the Fourier transform, we build the coupled Fourier neural
operator and mark it as CFNO.
Training parameters. The neural operators are trained using Adam optimizer with a learning rate
of0.001and decay of 0.95after every 100steps. The models are trained for a total of 500epochs
which is the same with training CMWNO for fair comparison. All experiments are done on an Nvidia
A100 40 GB GPUs.
3.1 G RAY-SCOTT (GS) E QUATIONS
The GS equations are coupled differential equations which model the underlying reaction and
diffusion patterns of chemical species. It is also able to generate a wide range of patterns which
exist in nature, such as bacteria, spirals and coral patterns. Each variable (i.e., uandv) diffuses
independently with a linear growth or decay term, while coupled together by Ë˜uv2(Driscoll et al.,
2014). For a given field upx, tq;vpx, tq, the GS equations take the form:
Btupx, tqâ€œÏµ1Bxxupx, tq`Fp1Â´upx, tqqÂ´Î»upx, tqv2px, tq, xPp0,10q, tPp0,1s
Btvpx, tqâ€œÏµ2Bxxvpx, tqÂ´pK`Fqvpx, tq`Î»upx, tqv2px, tq, xPp0,10q, tPp0,1s
upx,0qâ€œu0pxq;vpx,0qâ€œv0pxq, xPp0,10q(11)
where Ïµ1â€œ1, Ïµ2â€œ10Â´2, Kâ€œ6.62Ë†10Â´2, Fâ€œ2Ë†10Â´2. We use the coupling coefficient
Î»Pp0,1sto control the degree of coupling of uandv. We aim to learn the operators (i) mapping
the initial condition upx,0qto the solution upx, tâ€œ1qwith the interference of vpx, tq; (ii) mapping
the initial condition vpx,0qto the solution vpx, tâ€œ1qconsidering the interference of upx, tq. The
initial conditions are generated in Gaussian random fields (GRF) according to u0pxq, v0pxq â€
Np0,74pÂ´âˆ†`72IqÂ´2.5qwith periodic boundary conditions. We also use a different scheme
to generate u0pxqby using the smooth random functions (Rand) in chebfun package (Driscoll
et al., 2014) which returns a band-limited function defined by a Fourier series with independent
random coefficients; the parameter Î³specifies the minimal wavelength and here we choose Î³â€œ0.5.
Therefore, generating the initial conditions by different schemes, we have two combinations of
the initial conditions (i.e., u0pxqandv0pxq) and we mark them as (U-GRF, V-GRF) and (U-Rand,
V-GRF) respectively according to the generating schemes. Given the initial conditions, we solve the
equations using a fourth-order stiff time-stepping scheme named as ETDRK4 (Cox & Matthews, 2002)
with a resolution of 210, and sub-sample this data to obtain the datasets with the lower resolutions.
Modelss=256 s=512 s=1024
u v u v u v
CMWNO 0.00468 0.00464 0.00492 0.00434 0.00471 0.00450
CFNO 0.01371 0.00654 0.01345 0.00643 0.01421 0.00619
MWT s 0.08075 0.07308 0.08041 0.07382 0.07996 0.07213
MWT c 0.01445 0.00742 0.01408 0.00744 0.01334 0.00779
FNO c 0.01431 0.00812 0.01542 0.00819 0.01545 0.00885
PadÂ´ec 0.01904 0.00964 0.02070 0.01022 0.02233 0.01055
Table 1: Grayâ€“Scott (GS) equation benchmarks for different input resolution sat initial condition (U-GRF,V-
GRF). The relative L2errors are shown for each model. Bolded values are the best results of all the models, and
underlined values are the best results of the existing models. Set the same below.
Varying resolution. The results of our experiments on GS equations with different resolutions
(i.e.,sâ€œ256,512,1024 ) are shown in Table 1. As shown in the results, all the models exhibit the
resolution independence. The model MWT cwith concatenated data performs better than model
MWT susing two independent single MWT models to train uandvseparately, which indicates
the information from v0pxq;u0pxqbenefits the model predicting for uÏ„pxq;vÏ„pxq. For solving
uÏ„pxq;vÏ„pxq, our proposed CMWNO outperforms 2XÂ´3Ximprovements compared with the best
benchmark with respect to relative L2error. CFNO also outperforms FNO c, however, compared
7Published as a conference paper at ICLR 2023
with the improvement of CMWNO on MWT, the improvement of CFNO on FNO is not significant,
indicating that decoupling in the Fourier space is not as efficient as decoupling in the multiwavelets
domain. The learning curve of the neural operators solving uÏ„pxqat resolution sâ€œ1024 is shown in
Fig. 3.
/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000026/uni00000030/uni0000003a/uni00000031/uni00000032
/uni00000030/uni0000003a/uni00000037s
/uni00000030/uni0000003a/uni00000037c
/uni00000029/uni00000031/uni00000032c
/uni00000033/uni00000044/uni00000047/uni00000048c
Figure 3: Learning curve - Relative L2errorvsepochs
for neural operators.Varying coupling coefficient By varying
the coupling coefficient Î»in the GS equa-
tions, we can get different degree of cou-
pling between uandvaccording to eq. 11.
The higher value of Î»means higher de-
gree of coupling between uandv. Given
the same initial conditions u0pxqandv0pxq,
the outputs with different Î»(i.e., Î»â€œ
0.2,0.4,0.6,0.8,1) are shown in Fig. 4.
It shows that as Î»increases, all the mod-
els perform worse. For solving uÏ„pxq,
compared with at Î»â€œ0.2, the relative
L2errors at Î»â€œ1of the models in-
crease by 22.0% (CMWNO) ; 459.6% (MWT s);
105.9% (MWT c); 107.4% (FNO c); 146.7% (Pad Â´es). In terms of vpx,Ï„q, the numbers are
11.6% (CMWNO) ; 326.8% (MWT s); 34.5% (MWT c); 44.8% (FNO c); 44.3% (Pad Â´es). As we can
see, the MWT sworks the worst since the model cannot learn the interaction between uandv. The
models learning coupled operators through concatenated data works better than the single model but
still do not perform well on high coupling data. On the contrary, our CMWNO outperforms well
consistently with both low / high coupling coefficient, which indicates that our architecture is able to
decouple the coupled kernels.
/uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055
/uni00000026/uni00000030/uni0000003a/uni00000031/uni00000032
/uni00000030/uni0000003a/uni00000037s
/uni00000030/uni0000003a/uni00000037c
/uni00000029/uni00000031/uni00000032c
/uni00000033/uni00000044/uni00000047/uni00000048c
/uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055
/uni00000026/uni00000030/uni0000003a/uni00000031/uni00000032
/uni00000030/uni0000003a/uni00000037s
/uni00000030/uni0000003a/uni00000037c
/uni00000029/uni00000031/uni00000032c
/uni00000033/uni00000044/uni00000047/uni00000048c
Figure 4: Comparing the models by varying the coupling coefficient Î»at the initial condition (U-GRF, V-GRF)
with resolution sâ€œ1024 .
Varying initial conditions In addition to experimenting with both the initial conditions u0pxqand
v0pxqgenerated in the GRF as marked (U-GRF, V-GRF), we also perform the models on (U-Rand,
V-GRF). The numerical results are shown in Table 3 (see Appendix F). Our CMWNO achieves the
lowest relative L2error on both uandvwith3Xand2Ximprovements respectively. We provide
a sample of initial conditions in Fig. 7 (see Appendix E), and Fig. 5 shows its predicted outputs
from models CMWNO, MWT sand MWT c. It shows that our proposed CMWNO can give a precise
prediction in a smooth way while MWT sand MWT ccan only fit the true curve roughly.
Modelst=0.2 t=0.4 t=0.6 t=0.8
Ï Ï• Ï Ï• Ï Ï• Ï Ï•
CMWNO 0.00083 0.00073 0.00154 0.00252 0.00543 0.00467 0.02417 0.00305
CFNO 0.00767 0.00162 0.00890 0.00638 0.02011 0.01029 0.04780 0.00744
MWT c 0.00328 0.00646 0.00916 0.02244 0.02245 0.02768 0.06011 0.01622
FNO c 0.00241 0.00278 0.00473 0.00667 0.01329 0.01096 0.04950 0.00818
PadÂ´ec 0.00213 0.00320 0.00473 0.01307 0.01189 0.02466 0.03676 0.01171
Table 2: The relative L2errors for predicting Ïpx, tq{Ï†px, tqwithtâ€œ0.2,0.4,0.6,and0.8.
8Published as a conference paper at ICLR 2023
0 200 400 600 800 1000
Samples0.075
0.050
0.025
0.0000.0250.0500.075u (x,)
MWTs
MWTcCMWNO
True
0 200 400 600 800 1000
Samples0.3
0.2
0.1
0.00.10.20.3v (x,)
MWTs
MWTcCMWNO
True
Figure 5: The output of GS couple equations at the initial condition (U-Rand, V-GRF). (Left) The predicted
output of the models to upx, Ï„â€œ1q. (Right) The predicted output of the models to vpx, Ï„â€œ1q.
3.2 M EAN FIELD GAME PROBLEM
For local interactions, directly discretizing interaction terms is economical. However, non-local MFG
requires each player in making decisions to take into account the global information rather than local
information, which will increase the amount of computation in the process of calculation. In other
words, we need matrix multiplication on a full grid to calculate the interaction terms by evaluating
the expressionsÅŸ
Ï‰Kpx, yqÏpy, tqdy. In this work, we propose a more general framework, CMWNO,
to model the interactions in the Wavelet space and the results show that our model can be used to deal
with the coupled systems. Here we solve the non-local MFG which can be characterized as:
BtÏpx, tq`âˆ‡Â¨pÏpx, tqâˆ‡Ï†px, tqqâ€œ0, xPr0,1s, tPp0,1q
BtÏ†px, tqÂ´1
2}Ï†px, tq}2`Å¼
DKpx, yqÏpy, tqdtâ€œ0, xPr0,1s, tPp0,1q(12)
where Ïpx, tqis the density distribution of the players, and Ï†px, tqis the cost function. In a forward-
forward MFG setting (Gomes & Sedjro, 2017), we can obtain the value of Ïpx,0qandÏ†px,0q. We
aim to learn the operators: (i) mapping the initial condition Ïpx,0qto the solution Ïpx, tâ€œÏ„qwith
the interference of Ï†px, tq; (ii) mapping the initial condition Ï†px,0qto the solution Ï†px, tâ€œÏ„q
considering the interference of Ïpx, tq. To obtain the datasets, we generate Ïpx,0q;Ïpx, tâ€œ1q
by using the random functions in chebfun package with the wavelength parameter Î³â€œ0.3; 0.1,
respectively. The coupled equations are numerically solved by the primal-dual hybrid gradient
(PDHG) algorithm (Briceno-Arias et al., 2019; 2018) with the resolution sâ€œ256. The initial
conditions of Ïpx,0qandÏ†px,0qare used as the input while Ïpx, tqandÏ†px, tq(tâ€œ0.2,0.4,0.6,0.8)
are taken as the output.
We perform all the models working for coupled datasets mentioned above to solve this MFG coupled
PDEs, and the results with different tare shown in Table 2. Compared to the existing model with
the best results, our proposed CMWNO yields 34.2%â€67.4%improvements in terms of Ïand
57.4%â€73.7%in terms of Ï†with respect to the relative L2error. It is worth noting that MWT c
performs the worst in most cases which indicates that the interactions between ÏandÏ†can not be
learned through a single multiwavelet kernel. By interacting two kernels in the Wavelet space after
decomposition steps, our proposed CMWNO can better decouple the interactions between ÏandÏ†to
solve the MFG PDEs.
4 C ONCLUSION
In this work, we propose a coupled multiwavelets neural operator using multiwavelet discretization
of the spatial domain. Solving for coupled equations requires an information entanglement across
operators for individual process. We found that combining operators in the projected domain of
multiwavelets is effective. Numerical experiments using representative coupled PDEs including
Gray-Scott and mean field game problem show that our coupling mechanism effectively learns the
two processes in comparison with standalone operators.
9Published as a conference paper at ICLR 2023
ACKNOWLEDGEMENT
We are thankful to the anonymous reviewers for providing their valuable feedback which improved
our manuscript. We would also like to thank Dr. Justinian Rosca and Lisang Ding for their valuable
feedback. We gratefully acknowledge the support by the National Science Foundation Career award
under Grant No. Cyber-Physical Systems / CNS-1453860, the NSF award under Grant CCF-1837131,
MCB-1936775, CNS-1932620, the U.S. Army Research Office (ARO), the Defense Advanced
Research Projects Agency (DARPA) Young Faculty Award and DARPA Director Award under Grant
No. N66001-17-1-4044, an Intel faculty award, the Okawa Foundation award, a Northrop Grumman
grant, and Google cloud program. A part of this work used the Extreme Science and Engineering
Discovery Environment (XSEDE), which is supported by National Science Foundation grant number
ACI-1548562. R.B. has been supported in part by a NSF award under grant DMS-2108900 and
by the Simons Foundation. The views, opinions, and/or findings contained in this article are those
of the authors and should not be interpreted as representing the official views or policies, either
expressed or implied by the Defense Advanced Research Projects Agency, the Army Research Office,
the Department of Defense, or the National Science Foundation.
10Published as a conference paper at ICLR 2023
REFERENCES
Yves Achdou and Italo Capuzzo-Dolcetta. Mean field games: numerical methods. SIAM Journal on
Numerical Analysis, 48(3):1136â€“1162, 2010.
Yves Achdou, Guy Barles, Hitoshi Ishii, and Grigorii Lazarevich Litvinov. Hamilton-jacobi equations:
approximations, numerical analysis and applications. 2013a.
Yves Achdou, Fabio Camilli, and Italo Capuzzo-Dolcetta. Mean field games: convergence of a finite
difference method. SIAM Journal onNumerical Analysis, 51(5):2585â€“2612, 2013b.
Yves Achdou, Francisco J Buera, Jean-Michel Lasry, Pierre-Louis Lions, and Benjamin Moll. Partial
differential equation models in macroeconomics. Philosophical Transactions oftheRoyal Society
A:Mathematical, Physical andEngineering Sciences, 372(2028):20130397, 2014.
Yves Achdou, Jiequn Han, Jean-Michel Lasry, Pierre-Louis Lions, and Benjamin Moll. Income and
wealth distribution in macroeconomics: A continuous-time approach. Thereview ofeconomic
studies, 89(1):45â€“86, 2022.
B. Alpert, G. Beylkin, D. Gines, and L. V ozovoi. Adaptive solution of partial differential equations in
multiwavelet bases. Journal ofComputational Physics , 182(1):149â€“190, 2002a. ISSN 0021-9991.
Beylkin Alpert, Gregory Beylkin, David Gines, and Lev V ozovoi. Adaptive solution of partial
differential equations in multiwavelet bases. Journal ofComputational Physics , 182(1):149â€“190,
2002b.
Bradley Alpert, Gregory Beylkin, Ronald Coifman, and Vladimir Rokhlin. Wavelet-like bases for
the fast solution of second-kind integral equations. SIAM journal onScientific Computing , 14(1):
159â€“184, 1993.
Bradley K. Alpert. A class of bases in L2for the sparse representation of integral operators. SIAM
Journal onMathematical Analysis, 24(1):246â€“262, 1993a. doi: 10.1137/0524016.
Bradley K Alpert. A class of bases in lË†2 for the sparse representation of integral operators. SIAM
journal onMathematical Analysis, 24(1):246â€“262, 1993b.
Jean-David Benamou and Guillaume Carlier. Augmented lagrangian methods for transport opti-
mization, mean field games and degenerate elliptic equations. Journal ofOptimization Theory and
Applications, 167(1):1â€“26, 2015.
Jean-David Benamou, Guillaume Carlier, and Filippo Santambrogio. Variational mean field games.
InActive Particles, V olume 1, pp. 141â€“171. Springer, 2017.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. Advances inneural information processing systems ,
28, 2015.
Gregory Beylkin, Ronald Coifman, and Vladimir Rokhlin. Fast wavelet transforms and numerical
algorithms i. Communications onpure andapplied mathematics, 44(2):141â€“183, 1991.
Kaushik Bhattacharya, Bamdad Hosseini, Nikola B. Kovachki, and Andrew M. Stuart. Model
reduction and neural networks for parametric pdes, 2020.
Fabrice Bonjean and Gary SE Lagerloef. Diagnostic model and analysis of the surface currents in the
tropical pacific ocean. Journal ofPhysical Oceanography, 32(10):2938â€“2954, 2002.
Luis Briceno-Arias, Dante Kalise, Ziad Kobeissi, Mathieu Lauriere, A Mateos Gonz Â´alez, and
Francisco J Silva. On the implementation of a primal-dual algorithm for second order time-
dependent mean field games with local couplings. ESAIM: Proceedings andSurveys , 65:330â€“348,
2019.
Luis M Briceno-Arias, Dante Kalise, and Francisco J Silva. Proximal methods for stationary mean
field games with local couplings. SIAM Journal onControl andOptimization , 56(2):801â€“836,
2018.
11Published as a conference paper at ICLR 2023
Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong
Xu, Jing Bai, Jie Tong, et al. Spectral temporal graph neural network for multivariate time-series
forecasting. Advances inneural information processing systems, 33:17766â€“17778, 2020.
Defu Cao, Jiachen Li, Hengbo Ma, and Masayoshi Tomizuka. Spectral temporal graph neural network
for trajectory prediction. In 2021 IEEE International Conference onRobotics andAutomation
(ICRA), pp. 1839â€“1845. IEEE, 2021.
Defu Cao, Yousef El-Laham, Loc Trinh, Svitlana Vyetrenko, and Yan Liu. A synthetic limit order
book dataset for benchmarking forecasting algorithms under distributional shift. In NeurIPS 2022
Workshop onDistribution Shifts: Connecting Methods andApplications , 2022. URL https:
//openreview.net/forum?id=_u-1--wBV1 .
Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks
with arbitrary activation functions and its application to dynamical systems. IEEE Transactions on
Neural Networks, 6(4):911â€“917, 1995.
Jeffrey M. Connors, Jason S. Howell, and William J. Layton. Partitioned time stepping for a
parabolic two domain problem. SIAM Journal onNumerical Analysis , 47(5):3526â€“3549, 2009.
doi: 10.1137/080740891. URL https://doi.org/10.1137/080740891 .
Steven M Cox and Paul C Matthews. Exponential time differencing for stiff systems. Journal of
Computational Physics, 176(2):430â€“455, 2002.
Antonio De Paola, Vincenzo Trovato, David Angeli, and Goran Strbac. A mean field game approach
for distributed control of thermostatic loads acting in simultaneous energy-frequency response
markets. IEEE Transactions onSmart Grid, 10(6):5987â€“5999, 2019.
T. A Driscoll, N. Hale, and L. N. Trefethen. Chebfun Guide. Pafnuty Publications, 2014.
Meysam Ghanavati, Animesh Chakravarthy, and Prathyush P Menon. Analysis of automotive cyber-
attacks on highways using partial differential equation models. IEEE Transactions onControl of
Network Systems, 5(4):1775â€“1786, 2017.
David Gilbarg, Neil S Trudinger, David Gilbarg, and NS Trudinger. Elliptic partial differential
equations ofsecond order, volume 224. Springer, 1977.
Diogo Gomes and Marc Sedjro. One-dimensional, forward-forward mean-field games with congestion.
arXiv preprint arXiv:1703.10029, 2017.
Diogo A Gomes et al. A mean-field game approach to price formation. Dynamic Games and
Applications, 11(1):29â€“53, 2021.
Somdatta Goswami, Aniruddha Bora, Yue Yu, and George Em Karniadakis. Physics-informed neural
operators. arXiv preprint arXiv:2207.05748, 2022.
Olivier Gu Â´eant, Jean-Michel Lasry, and Pierre-Louis Lions. Mean field games and applications. In
Paris-Princeton lectures onmathematical finance 2010, pp. 205â€“266. Springer, 2011.
Gaurav Gupta, Xiongye Xiao, Radu Balan, and Paul Bogdan. Non-linear operator approximations
for initial value problems. In International Conference onLearning Representations, 2021a.
Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differential
equations, 2021b.
Craig S Henriquez. Simulating the electrical behavior of cardiac tissue using the bidomain model.
Critical reviews inbiomedical engineering, 21(1):1â€“77, 1993.
Minyi Huang, Roland P Malham Â´e, and Peter E Caines. Large population stochastic dynamic games:
closed-loop mckean-vlasov systems and the nash certainty equivalence principle. Communications
inInformation &Systems, 6(3):221â€“252, 2006.
Minyi Huang, Peter E. Caines, and Roland P. Malhame. Large-population cost-coupled lqg problems
with nonuniform agents: Individual-mass behavior and decentralized Îµ-nash equilibria. IEEE
Transactions onAutomatic Control, 52(9):1560â€“1571, 2007. doi: 10.1109/TAC.2007.904450.
12Published as a conference paper at ICLR 2023
Xuancheng Huang, Sebastian Jaimungal, and Mojtaba Nourian. Mean-field game strategies for
optimal execution. Applied Mathematical Finance, 26(2):153â€“185, 2019.
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. arXiv
preprint arXiv:2108.08481, 2021.
David Lacasse, Andr Â´e Garon, and Dominique Pelletier. Mechanical hemolysis in blood flow: user-
independent predictions with the solution of a partial differential equation. Computer Methods in
Biomechanics andBiomedical Engineering, 10(1):1â€“12, 2007.
Jean-Michel Lasry and Pierre-Louis Lions. Jeux `a champ moyen. iâ€“le cas stationnaire. Comptes
Rendus Math Â´ematique, 343(9):619â€“625, 2006.
Jean-Michel Lasry and Pierre-Louis Lions. Mean field games. Japanese journal ofmathematics , 2
(1):229â€“260, 2007.
Jorge A Laval and Ludovic Leclercq. The hamiltonâ€“jacobi partial differential equation and the three
representations of traffic flow. Transportation Research PartB:Methodological, 52:17â€“30, 2013.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations,
2020a.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential
equations, 2020b.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik
Bhattacharya, and Anima Anandkumar. Multipole graph neural operator for parametric partial
differential equations. In Advances inNeural Information Processing Systems , volume 33, pp.
6755â€“6766, 2020c.
Siting Liu and Levon Nurbekyan. Splitting methods for a class of non-potential mean field games.
arXiv preprint arXiv:2007.00099, 2020.
Siting Liu, Matthew Jacobs, Wuchen Li, Levon Nurbekyan, and Stanley J Osher. Computational
methods for first-order nonlocal mean field games with applications. SIAM Journal onNumerical
Analysis, 59(5):2639â€“2668, 2021.
Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning
nonlinear operators via deeponet based on the universal approximation theorem of operators.
Nature Machine Intelligence, 3(3):218â€“229, 2021.
Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and
George Em Karniadakis. A comprehensive and fair comparison of two neural operators (with prac-
tical extensions) based on fair data. Computer Methods inApplied Mechanics andEngineering ,
393:114778, 2022.
Chuizheng Meng, Sungyong Seo, Defu Cao, Sam Griesemer, and Yan Liu. When physics meets ma-
chine learning: A survey of physics-informed machine learning. arXiv preprint arXiv:2203.16797 ,
2022.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Icml, 2010.
Ravi G. Patel, Nathaniel A. Trask, Mitchell A. Wood, and Eric C. Cyr. A physics-informed operator
regression framework for extracting data-driven continuum models. Computer Methods inApplied
Mechanics andEngineering , 373:113500, 2021. ISSN 0045-7825. doi: https://doi.org/10.1016/j.
cma.2020.113500.
Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay,
Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcast-
net: A global data-driven high-resolution weather model using adaptive fourier neural operators.
arXiv preprint arXiv:2202.11214, 2022.
13Published as a conference paper at ICLR 2023
Mohammadreza Radmanesh, Manish Kumar, and Donald French. Partial differential equation-based
trajectory planning for multiple unmanned air vehicles in dynamic and uncertain environments.
Journal ofDynamic Systems, Measurement, andControl, 142(4), 2020.
Kimberly Stachenfeld, Drummond B Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff,
Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez. Learned
coarse models for efficient turbulence simulation. arXiv preprint arXiv:2112.15275, 2021.
Chen Tang, Lin Han, Hongwei Ren, Tao Gao, Zhifang Wang, and Ke Tang. The oriented-couple partial
differential equations for filtering in wrapped phase patterns. Optics Express , 17(7):5606â€“5617,
2009.
Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson.
U-fnoâ€”an enhanced fourier neural operator-based deep-learning model for multiphase flow.
Advances inWater Resources, 163:104180, 2022.
Xiongye Xiao, Hanlong Chen, and Paul Bogdan. Deciphering the generating rules and functionalities
of complex networks. Scientific reports, 11(1):22964, 2021.
Yuankun Xue and Paul Bogdan. Constructing compact causal mathematical models for complex
dynamics. In 2017 ACM/IEEE 8thInternational Conference onCyber-Physical Systems (ICCPS) ,
pp. 97â€“108, April 2017.
Chenzhong Yin, Xiongye Xiao, Valeriu Balaban, Mikhail E Kandel, Young Jae Lee, Gabriel Popescu,
and Paul Bogdan. Network science characteristics of brain-derived neuronal cultures deciphered
from quantitative phase imaging data. Scientific reports, 10(1):15078, 2020.
Yizhou Zhang, Defu Cao, and Yan Liu. Counterfactual neural temporal point process for estimating
causal influence of misinformation on social media. In Advances inNeural Information Processing
Systems, 2022.
14Published as a conference paper at ICLR 2023
â€¦â€¦â€¦Dice = 1RecDecRDRecDecRDğ‘ˆ!,#$%&ğ‘‰!,#$%&(ğ‘ˆ!,#$%&,ğ‘‰!,#$%&,â€¦,ğ‘Š!,#$%&)
(ğ‘‰!,#$%&,ğ‘ˆ!,#$%&,â€¦,ğ‘Š!,#$%&)RDâ€¦RDâ€¦â€¦RecDecRDğ‘Š!,#$%&RDâ€¦
â€¦
RecDecRDğ‘‰!,#$%&RDâ€¦RecDecRDğ‘ˆ!,#$%&RDâ€¦â€¦RecDecRDğ‘Š!,#$%&RDâ€¦â€¦â€¦RecDDâ€¦RecDDâ€¦â€¦RecDDâ€¦â€¦Path 1Path iPath mâ€¦
Figure 6: The scalable dice strategy with multiple coupled kernels. For each sample of coupled variables,
one only needs to go through a specific path (round diagonal corner rectangle). For example, we get dice equal
to1in this case so that this specific sample will go through Path 1. Inside each path, the order of updating is
from left to right, where the darker block indicates the operator we want to update and the lighter blocks provide
decomposition information from the fixed operator. Note that the updated operator can help to update other
operators at the same stage that occurs after the specific operator. Thus, for nkernels, we have noperators that
need to be updated and mpaths that can be selected, where mâ€œAn
nandAis the function of all permutations.
A R ELATED WORK
The operator network can be made up of several neural networks, which aims to approximate any
function defined as an input in one network and evaluated at the locations specified in the target
network (Lu et al., 2022). Chen & Chen propose the universal approximation theory of operators
for a single layer, which has led to several research works recently. Among them, DeepONet (Lu
et al., 2021) first applies deep neural network on the universal approximation theorem to learn
nonlinear operators. After that, Fourier Neural Operator (FNO) formulates the operator regression
by parameterizing the integral kernel directly in Fourier space (Li et al., 2020a). Several works
take advantage of FNOâ€™s ability to efficiently solve PDEs to design models for applications in
practical chaotic systems such as turbulence simulation (Stachenfeld et al., 2021), multiphase flow
simulation (Wen et al., 2022), and weather forecasting (Pathak et al., 2022). In addition, we would
like to highlight that neural operators are able to tackle the more fundamental problems in time
series analysis (Cao et al., 2021; Zhang et al., 2022; Cao et al., 2020) and easily to finds many
application in transportation, healthcare, manufacturing, finance (Cao et al., 2022), etc. Gupta et al.
introduce a multiwavelet-based neural operator that compresses the associated operatorâ€™s kernel using
fine-grained wavelets and the same group further proposes non-linear operator approximation for
initial value problems. Besides, physics-informed neural network and machine learning methods
provide a new research direction to equip specific physics information into neural operators Goswami
et al. (2022); Meng et al. (2022). For more information of neural operators, please refer to the
new survey paper: (Kovachki et al., 2021). Another related research line of our work is coupled
PDEs Tang et al. (2009), which is usually discussed in the form of MFGs (Benamou & Carlier, 2015;
Benamou et al., 2017; Briceno-Arias et al., 2019; 2018; Liu & Nurbekyan, 2020; Liu et al., 2021).
In addition, the interaction between terms in complex systems Xue & Bogdan (2017); Xiao et al.
(2021); Yin et al. (2020) can be characterized by coupled PDEs. Combining those two research lines,
this is the first work which proposes a decoupled multiwavelet-based neural operator learning schema
to solve coupled PDE problems.
15Published as a conference paper at ICLR 2023
B R EPRODUCIBILITY & C ODE AVAILABILITY
Architecture description in detail : The CMWNO model in Figure 1 is presented in the form of a
recurrent cell. In the decomposition stage, the input data at each iteration are Upi`1q
0,s (Vpi`1q
0,s) which
then gets transformed into Ui
0,s, Ui
0,d(Vi
0,s, Vi
0,d) using the filters H, G . At the same iteration, we also
obtain the corresponding outputs Ti
u,dandTi
u,Ë†s(Ti
v,dandTi
v,Ë†s). The same process is repeated in the
next iteration but now with using Ui
0,s(Vi
0,s) (obtained from the previous step) as the input, thus this
makes a recurrent chain of operations and is a kind of ladder-down operation. The loop is repeated
till we reach the L-th scale (coarsest scale) at which the final operation of Â¯Tis applied according
to eqs 9. The trainable neural networks layer in this stage is composed of tAu, Bu, Cu,Â¯Tuuand
tAv, Bv, Cv,Â¯Tvu. We use two cascading neural network layers with normal Xavier initialization to
handle 1D coupled equations in our experiments. Moreover, tAËš, BËš, CËšuare all one-layer CNNs
with the ReLU Nair & Hinton (2010) activation function followed by a linear layer, where CNNâ€™s
kernel size equals 3, stride equals 1, and padding size equals 1. The input channel of CNN equals the
feature number and the output channel is set to be 128 in all the experiments. In addition, tÂ¯Tvuis a
single kË†klinear layer with kâ€œ4suggested by Gupta et al. (2021b). In the reconstruction stage
(which is ladder-up), iteratively, the outputs of decomposition part Ti
u,dandTi
u,Ë†s(Ti
v,dandTi
v,Ë†s) are
first combined by the dice strategy in section 2.4, and then we use a reconstruction filter H, G . to
obtain the finer scales Upi`1q
Ï„,s (Vpi`1q
Ï„,s ), and finally, Un
Ï„,s(Vn
Ï„,s) is the finest scale of the output.
Our code to run the experiments can be found at https://github.com/joshuaxiao98/
CMWNO/ .
C M ULTIWAVELET BASES
C.1 M ULTIRESOLUTION ANALYSIS
The basic idea of MRA is to establish a preliminary basis in a subspace V0ofL2pRq, and then use
simple scaling and translation transformations to expand the basis of the subspace V0intoL2pRq
for analysis on multiscales. Multiwavelets further this operation by using a class of orthogonal
polynomials (OPs), in our case, we use Legendre polynomials for an efficient representation over a
finite interval (Alpert et al., 2002a).
ForkPZandnPN, the space of piecewise polynomial functions is defined as: Vk
nâ€œ tf|the
restriction of fto the interval p2Â´nl,2Â´npl`1qqis a polynomial of degree Äƒk, for all lâ€œ
0,1, . . . ,2nÂ´1, and f vanishes elsewhere u. Therefore, the space Vk
nhas dimension 2nk, and
each subspace Vk
iis contained in Vk
i`1shown as Vk
0Ä‚Vk
1Ä‚. . .Vk
nÄ‚. . . . Given a basis
Ï†0, Ï†1, . . . , Ï† kÂ´1ofVk
0, the space Vk
nis spanned by 2nkfunctions obtained from Ï†0, Ï†1, . . . , Ï† kÂ´1
ofVk
0by shifts and scales as
Ï†n
jlpxqâ€œ2n{2Ï†jp2nxÂ´lq, jâ€œ0,1, . . . , kÂ´1, lâ€œ0,1, . . . ,2nÂ´1. (13)
The functions Ï†0, Ï†1, . . . , Ï† kÂ´1are also called scaling functions which can project a function to the
approach space Vk
0.
C.2 M ULTIWAVELETS
The multiwavelet subspace Wk
nis defined as the orthogonal complement of Vk
ninVk
n`1, such that
Vk
nÃ 
Wk
nâ€œVk
n`1,Vk
nKWk
n; (14)
andWk
nhas dimension 2nk. Therefore, the decomposition can be obtained as
Vk
nâ€œVk
0Ã 
Wk
0Ã 
Wk
1. . .Ã 
Wk
nÂ´1. (15)
To form the orthogonal bases for Wk
n, a class of bases is constructed for L2pRq. Each basis consists
of translates and dilates of a finite set of functions Ïˆ1, . . . Ïˆ kshown as follows:
Ïˆn
jlpxqâ€œ2n{2Ïˆjp2nxÂ´lq, jâ€œ0,1, . . . , kÂ´1, lâ€œ0,1, . . . ,2nÂ´1. (16)
16Published as a conference paper at ICLR 2023
where the wavelet functions Ïˆ1, . . . Ïˆ kare piecewise polynomial and orthogonal to low-order poly-
nomials (vanishing moments):
Å¼1
0xiÏˆjpxqdxâ€œ0, iâ€œ0,1, . . . , kÂ´1. (17)
Here we restrict our attention to the interval r0,1sPR; however, the transformation to any finite
intervalrp, qscould be directly obtained by the appropriate translates and dilates.
D L EGENDRE POLYNOMIALS
The Legendre polynomials are defined with respect to (w.r.t.) a uniform weight function wLpxqâ€œ1
forÂ´1ÄxÄ1orwLpxqâ€œ1rÂ´1,1spxqsuch that
1Å¼
Â´1PipxqPjpxqdxâ€œ#
2
2i`1iâ€œj,
0 iâ€°j.(18)
For our work, we shift and scale the Legendre polynomials so they are defined over r0,1sas
Pip2xÂ´1q, and the corresponding weight function as wLp2xÂ´1q. The Legendre polynomials satisfy
the following recurrence relationships
iPipxqâ€œp2iÂ´1qxPiÂ´1pxqÂ´piÂ´1qPiÂ´2pxq,p2i`1qPipxq â€œ P1
i`1pxqÂ´P1
iÂ´1pxq,
which allows the expression of derivatives as a linear combination of lower-degree polynomials itself
as follows:
P1
ipxqâ€œp2iÂ´1qPiÂ´1pxq`p2iÂ´3qPiÂ´1pxq`. . . , (19)
where the summation ends at either P0pxqorP1pxq, with P0pxqâ€œ1andP1pxqâ€œx.
A set of orthonormal basis of the space of polynomials with degree Äƒddefined over the interval
r0,1sis obtained using shifted Legendre polynomials such that
Ï•iâ€œ?
2i`1Pip2xÂ´1q,
w.r.t. weight function wpxqâ€œwLp2xÂ´1q, such that
xÏ•i, Ï•jyÂµâ€œÅ¼1
0Ï•ipxqÏ•jpxqdxâ€œÎ´ij.
The basis for Vk
0are chosen as normalized shifted Legendre polynomials of degree upto kw.r.t.
weight function wLp2xÂ´1qâ€œ1r0,1spxqfrom Section D. For example, the first three bases are
Ï•0pxqâ€œ1,
Ï•1pxqâ€œ?
3p2xÂ´1q,
Ï•2pxqâ€œ?
5p6x2Â´6x`1q,0ÄxÄ1.(20)
For deriving a set of basis ÏˆiofWk
0using GSO, we need to evaluate the integrals efficiently, which
could be achieved using the Gaussian quadrature.
17Published as a conference paper at ICLR 2023
0 200 400 600 800 1000
Samples1.0
0.5
0.00.51.01.5
u0(x)
0 200 400 600 800 1000
Samples0.2
0.00.2
v0(x)
Figure 7: The sample of the initial conditions at (U-Rand, V-GRF).
/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001a/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni0000001c/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni00000017
/uni00000013/uni00000011/uni00000016
/uni00000013/uni00000011/uni00000015
/uni00000013/uni00000011/uni00000014
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000058/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni0000000c/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001a/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni0000001c/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni0000001b
/uni00000013/uni00000011/uni00000019
/uni00000013/uni00000011/uni00000017
/uni00000013/uni00000011/uni00000015
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000059/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001a/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni0000001c/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni00000015/uni00000013
/uni00000013/uni00000011/uni00000014/uni00000018
/uni00000013/uni00000011/uni00000014/uni00000013
/uni00000013/uni00000011/uni00000013/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni00000058/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000014/uni0000000c/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001a/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni0000001c/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni0000001b
/uni00000013/uni00000011/uni00000019
/uni00000013/uni00000011/uni00000017
/uni00000013/uni00000011/uni00000015
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000059/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000014/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
Figure 8: The input/output samples at the initial conditions (U-GRF, V-GRF).
E I NITIAL STATES SAMPLES
In this section, we use Fig. 7 to illustrate the initial states (u-rand and v-grf) for Gray-scott equations;
use Fig. 8 to exhibit the different initial states (u-grf and v-grf) and solutions for Gray-scott equations;
use Fig. 9 to show the samples of Ïpx, tqandÏ•px, tqat different time tin our non-local MFG case.
F A DDITIONAL RESULTS
F.1 A DDITIONAL RESULTS FOR GRAY-SCOTT (GS) E QUATIONS
In this section, we provide more results on the L2errors with different initial conditions on Table 3.
F.2 B ELOUSOV -ZHABOTINSKY (BZ) E QUATIONS
Adapted from the Belousovâ€“Zhabotinsky dynamic system, the coupled Belousov-Zhabotinsky (BZ)
equations in (21) describe a reaction-diffusion process with three species Driscoll et al. (2014). For a
18Published as a conference paper at ICLR 2023
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni00000016/uni00000015/uni00000013
/uni00000013/uni00000011/uni00000016/uni00000014/uni00000018
/uni00000013/uni00000011/uni00000016/uni00000014/uni00000013
/uni00000013/uni00000011/uni00000016/uni00000013/uni00000018
/uni00000013/uni00000011/uni00000016/uni00000013/uni00000013
/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000014/uni00000011/uni00000014/uni00000018/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni00000011/uni00000015/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni00000014/uni0000001c/uni00000015
/uni00000013/uni00000011/uni00000014/uni0000001c/uni00000013
/uni00000013/uni00000011/uni00000014/uni0000001b/uni0000001b
/uni00000013/uni00000011/uni00000014/uni0000001b/uni00000019
/uni00000013/uni00000011/uni00000014/uni0000001b/uni00000017
/uni00000013/uni00000011/uni00000014/uni0000001b/uni00000015
/uni00000013/uni00000011/uni00000014/uni0000001b/uni00000013
/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni00000011/uni00000015/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000013/uni00000011/uni0000001c/uni00000019/uni00000013/uni00000011/uni0000001c/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000013/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni00000011/uni00000017/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni00000013/uni00000019/uni00000016
/uni00000013/uni00000011/uni00000013/uni00000019/uni00000015
/uni00000013/uni00000011/uni00000013/uni00000019/uni00000014
/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013
/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001c
/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001b
/uni00000013/uni00000011/uni00000013/uni00000018/uni0000001a
/uni00000013/uni00000011/uni00000013/uni00000018/uni00000019
/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni00000011/uni00000017/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001c/uni00000016/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000019/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni00000011/uni00000019/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni00000013/uni00000017/uni00000015/uni00000013/uni00000011/uni00000013/uni00000017/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000013/uni00000011/uni00000013/uni00000017/uni00000019/uni00000013/uni00000011/uni00000013/uni00000017/uni0000001a/uni00000013/uni00000011/uni00000013/uni00000017/uni0000001b/uni00000013/uni00000011/uni00000013/uni00000017/uni0000001c/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni00000011/uni00000019/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001c/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000015/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000013/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni00000011/uni0000001b/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
/uni00000014 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni00000014/uni00000019/uni00000019/uni00000013/uni00000011/uni00000014/uni00000019/uni0000001b/uni00000013/uni00000011/uni00000014/uni0000001a/uni00000013/uni00000013/uni00000011/uni00000014/uni0000001a/uni00000015/uni00000013/uni00000011/uni00000014/uni0000001a/uni00000017/uni0000000b/uni0000005b/uni0000000f/uni00000057/uni00000020/uni00000013/uni00000011/uni0000001b/uni0000000c
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000014
/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000015
Figure 9: The sample input/output for ÏandÏ•. The top two figures are the initial conditions of ÏandÏ•as the
input. The other figures are the output of ÏandÏ„with different time t.
19Published as a conference paper at ICLR 2023
Modelss=256 s=512 s=1024
u v u v u v
CMWNO 0.00937 0.00856 0.00866 0.00726 0.00819 0.00769
MWT s 0.09353 0.05203 0.09137 0.05121 0.09381 0.05161
MWT c 0.02997 0.01804 0.02799 0.01832 0.02619 0.01866
FNO c 0.02496 0.01511 0.02433 0.01539 0.02335 0.01516
PadÂ´ec 0.04605 0.01449 0.04605 0.01479 0.04747 0.01509
Table 3: Grayâ€“Scott (GS) equation benchmarks for different input resolution sat initial condition (U-Rand,V-
GRF). The relative L2errors are shown for each model.
given field upx, tq;vpx, tq;wpx, tq, the BZ coupled equations take the form:
Btupx, tqâ€œÏµ1Bxxupx, tq`u`vÂ´uvÂ´u2, xPp0,1q, tPp0,0.2s
Btvpx, tqâ€œÏµ2Bxxvpx, tq`wÂ´vÂ´uv, xPp0,1q, tPp0,0.2s
Btwpx, tqâ€œÏµ3Bxxwpx, tq`uÂ´2, xPp0,1q, tPp0,0.2s(21)
where Ïµ1â€œ5Ë†10Â´2, Ïµ2â€œ5Ë†10Â´2, Ïµ3â€œ2Ë†10Â´2. Our goal is to learn the operators mapping
the initial condition of each variable to the solution with the interference of other variables. The
initial conditions are generated by using the smooth random functions (Rand) in chebfun package
(Driscoll et al., 2014). For the initial conditions upx,0q;vpx,0q;wpx,0q, we set the parameters
Î³â€œ0.3; 0.2; 0.1respectively. Given the initial conditions, we solve the equations using the fourth-
order stiff time-stepping scheme named as ETDRK4 (Cox & Matthews, 2002) with a resolution of
210, and sub-sample this data to obtain the datasets with the a resolution of 28. The results of the
experiments on BZ coupled equations with different resolutions (i.e., s=256,1024) are shown in Table
4.
Modelss=256 s=1024
u v w u v w
CMWNO 0.02306 0.02727 0.02412 0.01994 0.02505 0.02481
CFNO 0.04294 0.05738 0.05856 0.04087 0.06056 0.06148
MWT s 0.28703 0.34383 0.51297 0.27376 0.33410 0.53644
MWT c 0.04654 0.06489 0.07371 0.04723 0.06537 0.07287
FNO c 0.03575 0.09662 0.09302 0.03611 0.09371 0.09119
PadÂ´ec 0.04361 0.06766 0.08508 0.04479 0.06960 0.08486
Table 4: Belousovâ€“Zhabotinsky (BZ) equation benchmarks for different input resolution s. The relative L2
errors are shown for each model.
To handle multiple coupled variables, we apply the dice strategy referring to Fig. 6 to mimic the
interaction between u;v;w. As we can see in the results, our CMWNO still achieves the new
state-of-the-art.
20Published as a conference paper at ICLR 2023
MWT sMWTW cPade c FNO c CMWNO
Parameters 508754 254377 143957 287425 508754
Table 5: Comparison of modelâ€™s parameters.
G M ODEL COMPARISON
Table 5 compares in detail the data of parameters from CMWNO and baselines. Although the number
of parameters of our model is about twice as many as the second best model (FNO), the performance
of our model is improved by 57.68% and 61.41%, separately. In solving the coupled PDE problem,
our model is the optimal choice in terms of performance and power balance. One of our future efforts
is to improve model efficiency, which we leave as the further work.
H D ISCUSSION ON PINN
We note that neural operators and PINN are two recent deep learning approaches that have gathered the
tractions in the community. The PINN combines the advantages of data-driven machine learning and
physical modeling to train a model that automatically satisfies physical constraints with insufficient
training data and has comparable generalization performance to predict important physical parameters
of the model while ensuring accuracy. One can incorporate the differential form constraints from
PDEs into the design of the loss function of the neural network with automatic differentiation
techniques in deep neural networks. In addition, PINN cannot be used directly in a complete data-
driven scenario without an exact PDE structure, and the PDE function is hard to be decided in the
wild applications, However, one can take a compromise approach by relying on a specific PDE (such
as (Connors et al., 2009)) to design its loss function and using it on different input functions, which
should be undesirable.
I D EFAULT NOTATION
a A scalar (integer or real)
Ti Generic operator
A, B, C, Â¯T neural networks
A A set
R The set of real numbers
Îºi The kernel in the opeartor
ra, bs The real interval including aandb
pa, bs The real interval excluding abut including b
Vk
n tf|fare polynomials of degree Äƒkdefined over interval
p2Â´nl,2Â´npl`1qqfor all lâ€œ0,1, . . . ,2nÂ´1, and assumes
0elsewhereu
Wk
n Orthogonal space to Vk
nsuch that Wk
nÃ€Vk
nâ€œVk
n`1
TËš
Ëš,Ëšl Coefficients according to operator
UËš
Ëš,Ëšl, VËš
Ëš,Ëšl Coefficients according to input uËšandvËš
Pn Projection operator such that Pn:Hs,2Ã‘Vk
n
Qn Projection operator such that Qn:Hs,2Ã‘Wk
n
L Coarsest scale of the multiwavelet transform
21