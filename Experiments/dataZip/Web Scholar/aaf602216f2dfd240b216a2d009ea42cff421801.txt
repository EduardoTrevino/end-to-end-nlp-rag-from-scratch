Levin Tree Search with Context Models
Laurent Orseau1,Marcus Hutter1,Levi H. S. Lelis2
1Google DeepMind
2Department of Computing Science, University of Alberta, Canada
and Alberta Machine Intelligence Institute (Amii), Canada
{lorseau,mhutter }@google.com, levi.lelis@ualberta.ca
Abstract
Levin Tree Search (LTS) is a search algorithm that
makes use of a policy (a probability distribution
over actions) and comes with a theoretical guar-
antee on the number of expansions before reach-
ing a goal node, depending on the quality of the
policy. This guarantee can be used as a loss func-
tion, which we call the LTS loss, to optimize neu-
ral networks representing the policy (LTS+NN). In
this work we show that the neural network can
be substituted with parameterized context models
originating from the online compression literature
(LTS+CM). We show that the LTS loss is convex
under this new model, which allows for using stan-
dard convex optimization tools, and obtain conver-
gence guarantees to the optimal parameters in an
online setting for a given set of solution trajecto-
ries — guarantees that cannot be provided for neu-
ral networks. The new LTS+CM algorithm com-
pares favorably against LTS+NN on several bench-
marks: Sokoban (Boxoban), The Witness, and the
24-Sliding Tile puzzle (STP). The difference is par-
ticularly large on STP, where LTS+NN fails to
solve most of the test instances while LTS+CM
solves each test instance in a fraction of a sec-
ond. Furthermore, we show that LTS+CM is able to
learn a policy that solves the Rubik’s cube in only
a few hundred expansions, which considerably im-
proves upon previous machine learning techniques.
1 Introduction
We1consider the problem of solving a set of deterministic
single-agent search problems of a given domain, by starting
with little prior domain-specific knowledge. We focus on al-
gorithms that learn from previously solved instances to help
solve the remaining ones. We consider the satisficing setting
where solvers should (learn to) quickly find a solution, rather
than to minimize the cost of the returned solutions.
Levin Tree Search (LevinTS, LTS) is a tree search algo-
rithm for this setup that uses a policy, i.e., a probability distri-
1Extended version of the IJCAI 2023 paper. Source code at:
https://github.com/deepmind/levintreesearch cm.bution over actions, to guide the search [Orseau et al. , 2018 ].
LTS has a guarantee on the number of search steps required
before finding a solution, which depends on the probability of
the corresponding sequence of actions as assigned by the pol-
icy. Orseau and Lelis [2021 ]showed that this guarantee can
be used as a loss function. This LTS loss is used to optimize
a neural-network (NN) policy in the context of the Bootstrap
search-and-learn process [Jabbari Arfaee et al. , 2011 ]: The
NN policy is used in LTS (LTS+NN) to iteratively solve an
increasing number of problems from a given set, optimizing
the parameters of the NN when new problems are solved to
improve the policy by minimizing the LTS loss.
One constant outstanding issue with NNs is that the loss
function (whether quadratic, log loss, LTS loss, etc.) is al-
most never convex in the NN’s parameters. Still, most of the
time NNs are trained using online convex optimization algo-
rithms, such as stochastic gradient descent, Adagrad [Duchi
et al. , 2011 ], and its descendants. Such algorithms often
come with strong convergence or regret guarantees that only
hold under convexity assumptions, and can help to under-
stand the effect of various quantities (number of parameters,
etc.) on the learning speed [Zinkevich, 2003; Hazan, 2016;
Boyd and Vandenberghe, 2004 ]. In this paper we present pa-
rameterized context models for policies that are convex with
respect to the model’s parameters for the LTS loss. Such mod-
els guarantee that we obtain an optimal policy in terms of LTS
loss for a given set of training trajectories — a guarantee NNs
do not have.
The context models we introduce for learning policies are
based on the models from the online data compression litera-
ture[Rissanen, 1983; Willems et al. , 1995 ]. Our context mod-
els are composed of a set of contexts, where each context is
associated with a probability distribution over actions. These
distributions are combined using product-of-experts [Hinton,
2002 ]to produce the policy used during the LTS search. The
expressive power of product-of-experts comes mainly from
the ability of each expert to (possibly softly) veto a particu-
lar option by assigning it a low probability. A similar com-
bination using geometric mixing [Mattern, 2013; Matthew,
2005 ](a geometrically-parameterized variant of product-of-
experts) in a multi-layer architecture has already proved com-
petitive with NNs in classification, regression and density
modelling tasks [Veness et al. , 2017; Veness et al. , 2021;
Budden et al. , 2020 ]. In our work the context distributionsarXiv:2305.16945v2  [cs.LG]  27 Jun 2023are fully parameterized and we show that the LTS loss is con-
vex for this parameterization.
In their experiments, Orseau and Lelis [2021 ]showed that
LTS+NN performs well on two of the three evaluated do-
mains (Sokoban and The Witness), but fails to learn a pol-
icy for the 24-Sliding Tile Puzzle (STP). We show that LTS
with context models optimized with the LTS loss within the
Bootstrap process is able to learn a strong policy for all three
domains evaluated, including the STP. We also show that LTS
using context models is able to learn a policy that allows it to
find solutions to random instances of the Rubik’s Cube with
only a few hundred expansions. In the context of satisficing
planning, this is a major improvement over previous machine-
learning-based approaches, which require hundreds of thou-
sands expansions to solve instances of the Rubik’s Cube.
We start with giving some notation and the problem defi-
nition (Section 2), before describing the LTS algorithm, for
which we also provide a new lower bound on the number of
node expansions (Section 3). Then, we describe parameter-
ized context models and explain why we can expect them to
work well when using product-of-experts (Section 4), before
showing that the LTS loss function is convex for this param-
eterization (Section 5) and considering theoretical implica-
tions. Finally we present the experimental results (Section 6)
before concluding (Section 7).
2 Notation and Problem Definition
A table of notation can be found in Appendix I. We write
[t] ={1,2, . . . t}for a natural number t. The set of nodes
isNand is a forest, where each tree in the forest represents
a search problem with the root being the initial configuration
of the problem. The set of children of a node n∈ N is
C(n)and its parent is par (n); if a node has no parent it is
a root node. The set of ancestors of a node is anc (n)and
is the transitive closure of par (·); we also define anc +(n) =
anc(n)∪{n}. Similarly, desc (n)is the set of the descendants
ofn, and desc +(n) = desc(n)∪ {n}. The depth of a node
isd(n) =|anc(n)|, and so the depth of a root node is 0. The
rootroot( n)of a node nis the single node n0∈anc+(n)
such that n0is a root. A set of nodes N′is a tree in the
forestNif and only if there is a node n0∈ N′such thatS
n∈N′root( n) ={n0}. LetN0=S
n∈Nroot( n)be the
set of all root nodes. We write n[j]for the node at depth
j∈[d(n)]on the path from root( n) =n[0]ton=n[d(n)].
LetN∗⊆ N be the set of all solution nodes, and we write
N∗(n) =N∗∩desc +(n)for the set of solution nodes under
n. A policy πis such that for all n∈ N and for all n′∈
C(n) :π(n′|n)≥0andP
n′∈C(n)π(n′|n)≤1. The
policy is called proper if the latter holds as an equality. We
define, for all n′∈ C(n),π(n′) =π(n)π(n′|n)recursively
andπ(n) = 1 ifnis a root node.
Edges between nodes are labeled with actions and the chil-
dren of any node all have different labels, but different nodes
can have overlapping sets of actions. The set of all edge la-
bels is A. Let a(n)be the label of the edge from par (n)ton,
and let A(n)be the set of edge labels for the edges from node
nto its children. Then n̸=n′∧par(n) = par(n′)implies
a(n)̸=a(n′).Starting at a given root node n0, a tree search algorithm
expands a set N′⊆desc +(n0)until it finds a solution node
inN∗(n0). In this paper, given a set of root nodes, we are
interested in parameterized algorithms that attempt to mini-
mize the cumulative number of nodes that are expanded be-
fore finding a solution node for each root node, by improv-
ing the parameters of the algorithm from found solutions, and
with only little prior domain-specific knowledge.
3 Levin Tree Search
Levin Tree Search (LevinTS, which we abbreviate to LTS
here) is a tree/graph search algorithm based on best-first
search [Pearl, 1984 ]that uses the cost function2n7→
d(n)/π(n)[Orseau et al. , 2018 ], which, for convenience, we
abbreviate asd
π(n). That is, sinced
π(·)is monotonically in-
creasing from parent to child, LTS expands all nodes by in-
creasing order ofd
π(·)(Theorem 2, Orseau et al. [2018 ]).
Theorem 1 (LTS upper bound, adapted from Orseau et
al.[2018 ], Theorem 3) .Letπbe a policy. For any node
n∗∈ N , letN(n∗) ={n∈ N : root( n) = root( n∗)∧
d
π(n)≤d
π(n∗)}be the set of nodes within the same tree with
cost at most that of n∗. Then
|N(n∗)| ≤1 +d(n∗)
π(n∗).
Proof. LetLbe the set of leaves of N(n∗), then
|N(n∗)| ≤1 +X
n∈Ld(n) = 1 +X
n∈Lπ(n)d
π(n)
≤1 +X
n∈Lπ(n)d
π(n∗)≤1 +d
π(n∗),
where we used Lemma 10 (in Appendix) on the last inequal-
ity.
The consequence is that LTS started at root( n∗)expands
at most 1 +d
π(n∗)nodes before reaching n∗.
Orseau and Lelis [2021 ]also provides a related lower
bound showing that, for any policy, there are sets of prob-
lems where any algorithm needs to expand Ω(d
π(n∗))nodes
before reaching some node n∗in the worst case. They also
turn the guarantee of Theorem 1 into a loss function, used to
optimize the parameters of a neural network. Let N′be a set
of solution nodes whose roots are all different, define the LTS
loss function :
L(N′) =X
n∈N′d
π(n) (1)
which upper bounds the total search time of LTS to reach
all nodes in N′. Equation (1) is the loss function used in
Algorithm 2 (Appendix A) to optimize the policy — but a
more precise definition for context models will be given later.
To further justify the use of this loss function, we provide
a lower bound on the number of expansions that LTS must
perform before reaching an (unknown) target node.
2Orseau et al. [2018 ]actually use the cost function (d(n) +
1)/π(n). Here we use d(n)/π(n)instead which is actually (very)
slightly better and makes the notation simpler. All original results
can be straightforwardly adapted.Theorem 2 (Informal lower bound) .For a proper policy π
and any node n∗, the number of nodes whosed
πcost is at
most that of n∗is at least [1
¯dd
π(n∗)−1]/(|A| − 1), where
¯d−1is the average depth of the leaves of those nodes.
A more formal theorem is given in Appendix B.
Example 3. For a binary tree with a uniform policy, since
¯d=d(n∗) + 1 , the lower bound gives 2dd/(d+ 1)−1nodes
for a node n∗at depth dand of probability 2−d, which is quite
tight since the tree has 2d−1nodes. The upper bound 1+d2d
is slightly looser.
Remark 4. Even though pruning (such as state-equivalence
pruning) can make the policy improper, in which case the
lower bound does not hold and the upper bound can be loose,
optimizing the parameters of the policy for the upper bound
still makes sense, since pruning can be seen as a feature
placed on top of the policy — that is, the policy is optimized
as if pruning is not used. It must be noted that for optimiza-
tion Orseau and Lelis [2021 ](Section 4) use the log gradient
trick to replace the upper bound loss with the actual number
of expansions in an attempt to account for pruning; as the re-
sults of this paper suggest, it is not clear whether one should
account for the actual number of expansions while optimizing
the model.
4 Context Models
Now we consider that the policy πhas some parameters β∈
B(where B ⊆ Rkfor some k, which will be made more
precise later) and we write π(·;β)when the parameters are
relevant to the discussion. As mentioned in the introduction,
we want the LTS loss function of Eq. (1) to be convex in the
policy’s parameters, which means that we cannot use just any
policy — in particular this rules out deep neural networks.
Instead, we use context models, which have been widely used
in online prediction and compression ( e.g.,[Rissanen, 1983;
Willems et al. , 1995; Matthew, 2005; Veness et al. , 2021 ]).
The set of contexts is Q. A context is either active or in-
active at a given node in the tree. At each node n, the set
of active contexts is Q(n), and the policy’s prediction at n
depends only on these active contexts.
Similarly to patterns in pattern databases [Culberson and
Schaeffer, 1998 ], we organize contexts in sets of mutually ex-
clusive contexts, called mutex sets , and each context belongs
to exactly one mutex set. The set of mutex sets is M. For ev-
ery mutex set M∈ M , for every node n, at most one context
is active per mutex set. In this paper we are in the case where
exactly one context is active per mutex set, which is what hap-
pens when searching with multiple pattern databases, where
each pattern database provides a single pattern for a given
node in the tree. When designing contexts, it is often more
natural to directly design mutex sets. See Figure 1 for an ex-
ample, omitting the bottom parts of (b) and (d) for now.
To each context c∈ Q we associate a predictor pc:A →
[0,1]which is a (parameterized) categorical probability dis-
tribution over edge labels that will be optimized from training
data — the learning part will be explained in Section 5.1.
To combine the predictions of the active contexts at some
node n, we take their renormalized product, as an instance ofproduct-of-experts [Hinton, 2002 ]:
∀a∈ A(n) :p×(n, a) =Q
c∈Q(n)pc(a)
P
a′∈A(n)Q
c∈Q(n)pc(a′)(2)
We refer to the operation of Eq. (2) as product mixing , by re-
lation to geometric mixing [Mattern, 2013 ], a closely related
operation. Then, one can use p×(n, a)to define the policy
π(n′|n) =p×(n, a(n′))to be used with LTS.
The choice of this particular aggregation of the individual
predictions is best explained by the following example.
Example 5 (Wisdom of the product-of-experts crowd) .Fig-
ure 1 (a) and (b) displays a simple maze environment where
the agent is coming from the left. The only sensible action
is to go Up (toward the exit), but no single context sees the
whole picture. Instead, they see only individual cells around
the agent, and one context also sees (only) the previous action
(which is Right). The first two contexts only see empty cells to
the left and top of the agent, and are uninformative (uniform
probability distributions) about which action to take. But the
next three contexts, while not knowing what to do, know what
notto do. When aggregating these predictions with product
mixing, only one action remains with high probability: Up.
Example 6 (Generalization and specialisation) .Another ad-
vantage of product mixing is its ability to make use of both
general predictors and specialized predictors. Consider a
mutex set composed of mcontexts, and assume we have a
total of Mdata points (nodes on solution trajectories). Due
to the mutual exclusion nature of mutex sets, these Mdata
points must be partitioned among the mcontexts. Assum-
ing for simplicity a mostly uniform partitioning, then each
context receives approximately M/m data points to learn
from. Consider the mutex sets in Fig. 1 (b): The first 4 mu-
tex sets have size 3 (each context can see a wall, an empty
cell or the goal) and the last one has size 4. These are very
small sizes and thus the parameters of the contexts predictors
should quickly see enough data to learn an accurate distribu-
tion. However, while accurate, the distribution can hardly be
specific , and each predictor alone is not sufficient to obtain
a nearly-deterministic policy — though fortunately product
mixing helps with that. Now compare with the 2-cross mu-
tex set in Fig. 1 (d), and assume that cells outside the grid are
walls. A quick calculation, assuming only one goal cell, gives
that it should contain a little less than 1280 different contexts.
Each of these contexts thus receives less data to learn from on
average than the contexts in (b), but also sees more informa-
tion from the environment which may lead to more specific
(less entropic) distributions, as is the case in situation (c).
Remark 7. A predictor that has a uniform distribution has no
effect within a product mixture. Hence, adding new predictors
initialized with uniform predictions does not change the pol-
icy, and similarly, if a context does not happen to be useful to
learn a good policy, optimization will push its weights toward
the uniform distribution, implicitly discarding it.
Hence, product mixing is able to take advantage of both
general contexts that occur in many situations and specialised
contexts tailored to specific situations — and anything in-
between.(a) (b)Left
Up
Right
Downproduct
mixing
0.01
0.97
0.01
0.01
(c) (d)Left
Up
Right
Downproduct
mixing
0.01
0.01
0.96
0.02
Figure 1: (a) A simple maze environment. The dark gray cells are walls, the green circle is a goal. The blue arrow symbolizes the fact that
the agent (red triangle) is coming from the left. (b) A simple context model with five mutex sets: One mutex set for each of the four cells
around the triangle, and one mutex set for the last chosen action. Each of the first four mutex sets contains three contexts (wall, empty cell,
goal), and the last mutex set contains four contexts (one for each action). The 5 active contexts (one per mutex set) for the situation shown
in (a) are depicted at the top, while their individual probability predictions are the horizontal blue bars for each of the four actions. The last
column is the resulting product mixing prediction of the 5 predictions. No individual context prediction exceeds 1/3 for any action, yet the
product mixing prediction is close to 1 for the action Up. (c) Another situation. (d) A different set of mutex sets for the situation in (c): A
1-cross around the agent, a 2-cross around the agent, and the last action. The specialized 2-cross context is certain that the correct action is
Right, despite the two other contexts together giving more weight to action Down. The resulting product mixing gives high probability to
Right, showing that, in product mixing, specialized contexts can take precedence over less-certain more-general contexts.
Our LTS with context models algorithm is given in Algo-
rithm 1, building upon the one by Orseau and Lelis [2021 ]
with a few differences. As mentioned earlier, it is a best-
first search algorithm and uses a priority queue to main-
tain the nodes to be expanded next. It is also budgeted
and returns "budget_reached" if too many nodes have
been expanded. It returns "no_solution" if all nodes
have been expanded without reaching a solution node —
assuming safe pruning or no pruning. Safe pruning (us-
ingvisited_states ) can be performed if the policy is
Markovian [Orseau et al. , 2018 ], which is the case in par-
ticular when the set of active contexts Q(n)depends only
onstate (n). The algorithm assumes the existence of
application-specific state and state_transition
functions, such that state (n′) =state_transition
(state (n), a(n′))for all n′∈ C(n). Note that with context
models the prediction π(n′|n)depends on the active con-
textsQ(n)butnoton the state of a child node. This allows
us to delay the state transition until the child is extracted from
the queue, saving up to a branching factor of state transitions
(see also [Agostinelli et al. , 2021 ]).
Remark 8. In practice, usually a mutex set can be imple-
mented as a hashtable as for pattern databases: the active
context is read from the current state of the environment, and
the corresponding predictor is retrieved from the hashtable.
This allows for a computational cost of O(log|M|)per mu-
tex set M, or even O(1)with perfect hash functions, and thus
O(P
M∈Mlog|M|)which is much smaller than |Q|. Using
an imperfect hashtable, only the contexts that appear on the
paths to the found solution nodes need to be stored.
5 Convexity
Because the LTS loss in Eq. (1) is different from the log
loss [Cesa-Bianchi and Lugosi, 2006 ](due to the sum in-
between the products), optimization does notreduce to max-
imum likelihood estimation. However, we show that convex-
ity in the log loss implies convexity in the LTS loss. Thismeans, in particular, that if a probability distribution is log-
concave (such as all the members of the exponential family),
that is, the log loss for such models is convex, then the LTS
loss is convex in these parameters, too.
First we show that every sequence of functions with a con-
vex log loss also have convex inverse loss and LTS loss.
Theorem 9 (Log loss to inverse loss convexity) .Let
f1, f2, . . . f sbe a sequence of positive functions with fi:
Rn→(0,∞)for all i∈[s]and such that β7→ − logfi(β)is
convex for each i∈[s], then L(β) =P
k1Q
tfk,t(β)is convex,
where each (k, t)corresponds to a unique index in [s].
The proof is in Appendix E.1. For a policy π(·;β)pa-
rameterized by β, the LTS loss in Eq. (1) is LN′(β) =P
k∈N′d(nk)/π(nk;β), and its convexity follows from The-
orem 9 by taking fk,0(·) = 1 /d(nk), and fk,t(β) =
π(nk
[t]|nk
[t−1];β)such thatQd(nk)
t=1fk,t(β) =π(nk;β).
Theorem 9 means that many tools of compression and on-
line prediction in the log loss can be transferred to the LTS
loss case. In particular, when there is only one mutex set
(|M|= 1), the fiare simple categorical distributions, that is,
fi(β) =βjtfor some index jt, and thus −logfiis a convex
function, so the corresponding LTS loss is convex too. Unfor-
tunately, the LTS loss function for such a model is convex in β
only when there is only one mutex set, |M|= 1. Fortunately,
it becomes convex for |M| ≥ 1when we reparameterize the
context predictors with β⇝expβ.
Letβc,a∈[lnεlow,0]be the value of the parameter of the
predictor for context cfor the edge label a. Then the predic-
tion of a context cis defined as
∀a∈ A(n) :pc(a;β) =exp(βc,a)P
a′∈A(n)exp(βc,a′).(3)
We can also now make precise the definition of B:B=
[lnεlow,0]|Q|×A, and note that pc(a;β)≥εlow/|A(n)|. Sim-
ilarly to geometric mixing [Mattern, 2013; Mattern, 2016 ],
it can be proven that context models have a convex log loss,Algorithm 1 Budgeted LTS with context models. Returns a
solution node if any is found, or "budget_reached" or
"no_solution" .
#n0: root node
#B: node expansion budget
#β: parameters of the context models
def LTS+CM( n0,B,β):
q = priority_queue ()
# tuple: {d
π, d, π n,node , state , action}
tup = {0, 0, 1, n0, state( n0), False}
q.insert(tup) # insert root node/state
visited_states = {} # dict: state( n) -> π(n)
repeat forever:
if q is empty: return "no_solution"
# Extract the tuple with minimum costd
π
d
πn, d, πn, n, s_parent , a = q.extract_min ()
ifn∈ N∗: return n # solution found
s = state_transition(s_parent , a) if a else
s_parent
πs= visited_states.get(s, default =0)
# Note: BFS ensuresd
π(ns)≤d
π(n);s=state (ns)
# Optional: Prune the search if s is better
ifπs≥πn: continue
else: visited_states.set(s, πn)
# Node expansion
expanded += 1
if expanded == B: return "budget_reached"
Z =P
a∈A(n)Q
c∈Q(n)pc(a;β)# normalizer
forn′∈ C(n):
a =a(n′)# action
# Product mixing of the active contexts ’
predictions
p×,a=1
ZQ
c∈Q(n)pc(a;β)# See Eq. (3)
# Action probability , εmixensures πn′>0
πn′=πn((1−εmix)p×,a+εmix
|A(n)|)
q.insert ({(d+1)/ πn′, d+1, πn′,n′, s, a})
and thus their LTS loss is also convex by Theorem 9. In Ap-
pendix E.2 we provide a more direct proof, and a generaliza-
tion to the exponential family for finite sets of actions.
Plugging (3) into Eq. (2) and pushing the probabilities
away from 0 with εmix>0[Orseau et al. , 2018 ]we obtain
the policy’s probability for a child n′ofn(i.e., for the action
a(n′)at node n) with parameters β:
p×(n, a;β) =exp(P
c∈Q(n)βc,a)
P
a′∈A(n)expP
c∈Q(n)βc,a′, (4)
π(n′|n;β) = (1 −εmix)p×(n, a(n′);β) +εmix
|A(n)|.(5)
5.1 Optimization
We can now give a more explicit form of the LTS loss func-
tion of Eq. (1) for context models with a dependency on theparameters β, for a set of solution nodes N′assumed to all
have different roots:
L(N′, β) =X
n∈N′ℓ(n, β), (6)
ℓ(n, β) =d(n)
π(n;β)=d(n)
Qd(n)−1
j=0π(n[j+1]|n[j];β)(7)
=d(n)d(n)−1Y
j=0X
a′∈A(n[j])exp
X
c∈Q(n[j])βc,a′−βc,a(n[j+1])

where a(n[j+1])should be read as the action chosen at step
j, and the last equality follows from Eqs. (4) and (5) where
we take εmix= 0 during optimization. Recall that this loss
function Lgives an upper bound on the total search time (in
node expansions) required for LTS to find all the solutions N′
for their corresponding problems (root nodes), and thus opti-
mizing the parameters corresponds to optimizing the search
time.
5.2 Online Search-and-Learn Guarantees
Suppose that at each time step t= 1,2. . ., the learner re-
ceives a problem n0
t(a root node) and uses LTS with param-
etersβt∈ B until it finds a solution node nt∈ N∗(n0
t). The
parameters are then updated using nt(and previous nodes)
and the next step t+ 1begins.
LetNt= (n1, . . . , n t)be the sequence of found solution
nodes. For the loss function of Eq. (6), after tfound solu-
tion nodes, the optimal parameters in hindsight areβ∗
t=
argminβ∈BL(Nt, β). We want to know how the learner fares
against β∗
t— which is a moving target as tincreases. The
regret [Hazan, 2016 ]at step tis the cumulative difference be-
tween the loss incurred by the learner with its time varying
parameters βi, i= 1,2, . . . , t , and the loss when using the
optimum parameters in hindsight β∗
t:
R(Nt) =X
i∈[t]ℓ(ni, βi)−L(Nt, β∗
t).
A straightforward implication of the convexity of Eq. (7) is
that we can use Online Gradient Descent (OGD) [Zinkevich,
2003 ]or some of its many variants such as Adagrad [Duchi
et al. , 2011 ]and ensure that the algorithm incurs a regret of
R(Nt) =O(|A||Q| G√
tln1
εlow), where Gis the largest ob-
served gradient in infinite norm3and when using quadratic
regularization. Regret bounds are related to the learning
speed (the smaller the bound, the faster the learning), that is,
roughly speaking, how fast the parameters converge to their
optimal values for the same sequence of solution nodes. Such
a regret bound (assuming it is tight enough) also allows to ob-
serve the impact of the different quantities on the regret, such
as the number of contexts |Q|, orεlow.
OGD and its many variants are computationally efficient
as they take O(d(n)|A||M| )computation time per solu-
tion node n, but they are not very data efficient, due to the
linearization of the loss function — the so-called ‘gradient
3The dependency on the largest gradient can be softened signifi-
cantly, e.g., with Adagrad and sporadic resets of the learning rates.trick’ [Cesa-Bianchi and Lugosi, 2006 ]. To make the most of
the data, we avoid linearization by sequentially minimizing
the full regularized loss function L(Nt,·)+R(·)where R(β)
is a convex regularization function. That is, at each step, we
set:
βt+1= argmin
β∈BL(Nt, β) +R(β) (8)
which can be solved using standard convex optimization tech-
niques (see Appendix C) [Boyd and Vandenberghe, 2004 ].
This update is known as (non-linearized) Follow the Leader
(FTL) which automatically adapts to local strong convex-
ity and has a fast O(logT)regret without tuning a learning
rate[Shalev-Shwartz, 2007 ], except that we add regulariza-
tion to avoid overfitting which FTL suffers from. Unfortu-
nately, solving Eq. (8) even approximately at each step is too
computational costly, so we amortize this cost by delaying
updates (see below), which of course incurs a learning cost,
e.g.,[Joulani et al. , 2013 ].
6 Experiments
As with previous work, in the experiments we use the LTS al-
gorithm with context models (Algorithm 1) within the search-
and-learn loop of the Bootstrap process [Jabbari Arfaee et al. ,
2011 ]to solve a dataset of problems, then test the learned
model on a separate test set. See Appendix A for more de-
tails. Note that the Bootstrap process is a little different from
the online learning setting, so the theoretical guarantees men-
tioned above may not carry over strictly — this analysis is left
for future work.,
This allows us to compare LTS with context models
(LTS+CM) in particular with previous results using LTS with
neural networks (LTS+NN) [Guez et al. , 2019; Orseau and
Lelis, 2021 ]on three domains. We also train LTS+CM to
solve the Rubik’s cube and compare with other approaches.
LTS+NN’s domains. We foremost compare LTS with con-
text models (LTS+CM) with LTS with a convolutional neu-
ral network [Orseau and Lelis, 2021 ](LTS+NN) on the
three domains where the latter was tested: (a) Sokoban
(Boxoban) [Guez et al. , 2018 ]on the standard 1000 test prob-
lems, a PSPACE-hard puzzle [Culberson, 1999 ]where the
player must push boxes onto goal positions while avoiding
deadlocks, (b) The Witness, a color partitioning problem that
is NP-hard in general [Abel et al. , 2020 ], and (c) the 24 ( 5×5)
sliding-tile puzzle (STP), a sorting problem on a grid, for
which finding short solutions is also NP-hard [Ratner and
Warmuth, 1986 ]. As in previous work, we train LTS+CM
on the same datasets of 50 000 problems each, with the same
initial budget ( 2000 node expansions for Sokoban and The
Witness, 7000 for STP) and stop as soon as the training set
is entirely solved. Training LTS+CM for these domains took
less than 2 hours each.
Harder Sokoban. Additionally, we compare algorithms
on the Boxoban ‘hard’ set of 3332 problems. Guez et
al.[2019 ]trained a convLSTM network on the medium-
difficulty dataset (450k problems) with a standard actor-critic
setup — not the LTS loss — and used LTS (hence LTS+NN)
at test time. The more recent ExPoSe algorithm [Mittal etal., 2022 ]updates the parameters of a policy neural network
4during the search, and is trained on both the medium set
(450k problems) and the ‘unfiltered’ Boxoban set (900k prob-
lems) with solution trajectories obtained from an A* search.
Rubik’s Cube. We also use LTS+CM to learn a fast policy
for the Rubik’s cube, with an initial budget of B1= 21000 .
We use a sequence of datasets containing 100k problems
each, generated with a random walk of between mandm′=
m+ 5moves from the solution, where mincreases by steps
of 5 from 0 to 50, after which we set m′=m= 50 for each
new generated set. DeepCubeA [Agostinelli et al. , 2019 ]uses
a fairly large neural network to learn in a supervised fash-
ion from trajectories generated with a backward model of the
environment, and Weighted A* is used to solve random test
cubes. Their goal is to learn a policy that returns solutions
of near-optimal length. By contrast, our goal is to learn a
fast-solving policy. Allen et al. [2021 ]takes a completely
different approach (no neural network) by learning a set of
‘focused macro actions’ which are meant to change the state
as little as possible so as to mimic the so-called ‘algorithms’
that human experts use to solve the Rubik’s cube. They use
a rather small budget of 2 million actions to learn the macro
actions, but also use the more informative goal-count scor-
ing function (how many variables of the state have the cor-
rect value), while we only assume access to the more basic
solved/unsolved function. As with previous work, we report
solution lengths in the quarter-turn metric. Our test set con-
tains 1000 cubes scrambled 100 times each — this is likely
more than enough to generate random cubes [Korf, 1997 ]—
and we expect the difficulty to match that of previous work.
Machine description. We used a single EPYC 7B12 (64
cores, 128 threads) server with 512GB of RAM without GPU.
During training and testing, 64 problems are attempted con-
currently — one problem per CPU core. Optimization uses
128 threads to calculate the loss, Jacobian and updates.
Hyperparameters. For all experiments we use εlow=
10−4,εmix= 10−3, a quadratic regularization R(β) =
5∥β−β0∥2where β0= (1−1/A) lnεlow(see Appendix F).
The convex optimization algorithm we use to solve Eq. (8) is
detailed in Appendix C.
Figure 2: Example of a relative tiling of row span 2, column span
3, at maximum row distance 1 and maximum column distance 3
around the agent (red triangle). Each orange rectangle is a mutex
set of at most 46different contexts. A padding value can be chosen
arbitrarily (such as the wall value) for cells outside the grid.
4The architecture of the neural network was not specified.Domain Algorithm %solved Length Expansions Time (ms)
Boxoban LTS+CM (this work) 100.00 41.7 2 132.3 124
LTS+NN [Orseau and Lelis, 2021 ] 100.00 40.1 2 640.4 19 500
The Witness LTS+CM (this work) 100.00 15.5 102.8 9
LTS+NN [Orseau and Lelis, 2021 ] 100.00 14.8 520.2 3 200
STP (24-puzzle) LTS+CM (this work) 100.00 211.2 5 667.4 236
LTS+NN [Orseau and Lelis, 2021 ] 0.90 145.1 39 005.6 31 100
Boxoban hard LTS+CM (this work) 100.00 67.8 48 058.6 3 275
LTS+NN [Guez et al. , 2019 ] 94.00 n/a n/a 3 600
ExPoSe [Mittal et al. , 2022 ] 97.30 n/a n/a n/a
Rubik’s cube LTS+CM (this work) 100.00 81.7 498.0 16
DeepCubeA [Agostinelli et al. , 2019 ]100.00 21.5 ∼600 000.0 24 220
GBFS(A+M) [Allen et al. , 2021 ] 100.00 378.0 †171 300.0 n/a
Table 1: Results on the test sets. The last 3 columns are the averages over the test instances. The first three domains allow for a fair comparison
between LTS with context models and LTS with neural networks [Orseau and Lelis, 2021 ]using the same 50k training instances and initial
budget. For the last two domains, comparison to prior work is more cursory and is provided for information only, in particular because the
objective of DeepCubeA is to provide near-optimal-length solutions rather than fast solutions. The values for LTS+ {CM,NN }all use a single
CPU, no GPU (except for LTS+NN [Guez et al. , 2019 ]). DeepCubeA uses four high-end GPU cards. More results can be found in Table 2 in
Appendix H. †Does not account for the cost of macro-actions.
Mutex sets. For Sokoban, STP, and The Witness we use
several mutex sets of rectangular shapes at various distances
around the agent (the player in Sokoban, the tip of the ‘snake’
in The Witness, the blank in STP), which we call relative
tilings . An example of relative tiling is given in Fig. 2, and a
more information can be found in Appendix G. For the Ru-
bik’s cube, each mutex set {i, j}corresponds to the ordered
colors of the two cubies (the small cubes that make up the
Rubik’s cube) at location iandj(such as the up-front-right
corner and the back-right edge). There are 20 locations, hence
190 different mutex sets, and each of them contains at most
242contexts (there are 8 corner cubies, each with 3 possible
orientations, and 12 side cubies, each with 2 possible orienta-
tions). For all domains, to these mutex sets we add one mu-
tex set for the last action, indicating the action the agent per-
formed to reach the node; for Sokoban this includes whether
the last action was a push. The first 3 domains all have 4
actions (up, down, left, right), and the Rubik’s cube has 12
actions (a rotation of each face, in either direction).
Results. The algorithms are tested on test sets that are sep-
arate from the training sets, see Table 1. For the first three
domains, LTS+CM performs better than LTS+NN, even solv-
ing all test instances of the STP while LTS+NN solves less
than 1% of them. On The Witness, LTS+CM learns a policy
that allows it to expand 5 times fewer nodes than LTS+NN.
LTS+CM also solves all instances of the Boxoban hard set, by
contrast to previous published work, and despite being trained
only on 50k problems. On the Rubik’s cube, LTS+CM learns
a policy that is hundreds of times faster than previous work —
though recall that DeepCubeA’s objective of finding short so-
lutions differs from ours. This may be surprising given how
simple the contexts are — each context ‘sees’ only two cu-
bies — and is a clear sign that product mixing is taking full
advantage of the learned individual context predictions.7 Conclusion
We have devised a parameterized policy for the Levin Tree
Search (LTS) algorithm using product-of-experts of context
models that ensures that the LTS loss function is convex.
While neural networks — where convexity is almost certainly
lost — have achieved impressive results recently, we show
that our algorithm is competitive with published results, if
not better.
Convexity allows us in particular to use convex optimiza-
tion algorithms and to provide regret guarantees in the online
learning setting. While this provides a good basis to work
with, this notion of regret holds against any competitor that
learns from the same set of solution nodes. The next ques-
tion is how we can obtain an online search-and-learn regret
guarantee against a competitor for the same set of problems
(root nodes), for which the cumulative LTS loss is minimum
across all sets of solution nodes for the same problems. And,
if this happens to be unachievable, what intermediate regret
setting could be considered? We believe these are important
open research questions to tackle.
We have tried to design mutex sets that use only ba-
sic domain-specific knowledge (the input representation of
agent-centered grid-worlds, or the cubie representation of the
Rubik’s cube), but in the future it would be interesting to also
learn to search the space of possible context models — this
would likely require more training data.
LTS with context models, as presented here, cannot di-
rectly make use of a value function or a heuristic function,
however they could either be binarized into multiple mutex
sets, or be used as in PHS* [Orseau and Lelis, 2021 ]to esti-
mate the LTS cost at the solution, or be used as features since
the loss function would still be convex (see Appendix C).Acknowledgments
We would like to thank the following people for their use-
ful help and feedback: Csaba Szepesvari, Pooria Joulani, Tor
Lattimore, Joel Veness, Stephen McAleer.
The following people also helped with Racket-specific
questions: Matthew Flatt, Sam Tobin-Hochstadt, Bog-
dan Popa, Jeffrey Massung, Jens Axel Søgaard, Sorawee
Porncharoenwase, Jack Firth, Stephen De Gabrielle, Alex
Hars ´anyi, Shu-Hung You, and the rest of the quite helpful
and reactive Racket community.
This research was supported by Canada’s NSERC and the
CIFAR AI Chairs program.
References
[Abel et al. , 2020 ]Zachary Abel, Jeffrey Bosboom,
Michael J. Coulombe, Erik D. Demaine, Linus Hamil-
ton, Adam Hesterberg, Justin Kopinsky, Jayson Lynch,
Mikhail Rudoy, and Clemens Thielen. Who witnesses
the witness? finding witnesses in the witness is hard and
sometimes impossible. Theor. Comput. Sci. , 839:41–102,
2020.
[Agostinelli et al. , 2019 ]Forest Agostinelli, Stephen
McAleer, Alexander Shmakov, and Pierre Baldi. Solving
the rubik’s cube with deep reinforcement learning and
search. Nature Machine Intelligence , 1, 07 2019.
[Agostinelli et al. , 2021 ]Forest Agostinelli, Alexander
Shmakov, Stephen McAleer, Roy Fox, and Pierre Baldi.
A* search without expansions: Learning heuristic
functions with deep q-networks, 2021. arXiv 2102.04518.
[Allen et al. , 2021 ]Cameron Allen, Michael Katz, Tim
Klinger, George Konidaris, Matthew Riemer, and Gerald
Tesauro. Efficient black-box planning using macro-actions
with focused effects. In Proceedings of the Thirtieth Inter-
national Joint Conference on Artificial Intelligence , pages
4024–4031, 2021.
[Boyd and Vandenberghe, 2004 ]Stephen Boyd and Lieven
Vandenberghe. Convex Optimization . Cambridge Univer-
sity Press, Cambridge, England, 2004.
[B¨uchner et al. , 2022 ]Clemens B ¨uchner, Patrick Ferber,
Jendrik Seipp, and Malte Helmert. A comparison of ab-
straction heuristics for rubik’s cube. In ICAPS 2022 Work-
shop on Heuristics and Search for Domain-independent
Planning , 2022.
[Budden et al. , 2020 ]David Budden, Adam Marblestone,
Eren Sezener, Tor Lattimore, Gregory Wayne, and Joel
Veness. Gaussian gated linear networks. In Advances in
Neural Information Processing Systems , volume 33, pages
16508–16519. Curran Associates, Inc., 2020.
[Cesa-Bianchi and Lugosi, 2006 ]Nicolo Cesa-Bianchi and
Gabor Lugosi. Prediction, Learning, and Games . Cam-
bridge University Press, New York, NY , USA, 2006.
[Cort´es, 2006 ]Jorge Cort ´es. Finite-time convergent gradient
flows with applications to network consensus. Automatica ,
42(11):1993–2000, 2006.[Culberson and Schaeffer, 1998 ]Joseph C. Culberson and
Jonathan Schaeffer. Pattern databases. Computational In-
telligence , 14(3):318–334, 1998.
[Culberson, 1999 ]Joseph C. Culberson. Sokoban is
PSPACE-Complete. In Fun With Algorithms , pages 65–
76, 1999.
[Duchi et al. , 2011 ]John Duchi, Elad Hazan, and Yoram
Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning
Research , 12(61):2121–2159, 2011.
[Ebendt and Drechsler, 2009 ]R¨udiger Ebendt and Rolf
Drechsler. Weighted a* search – unifying view and
application. Artificial Intelligence , 173(14):1310 – 1342,
2009.
[Frank and Wolfe, 1956 ]Marguerite Frank and Philip Wolfe.
An algorithm for quadratic programming. Naval Research
Logistics Quarterly , 3(1-2):95–110, 1956.
[Guez et al. , 2018 ]Arthur Guez, Mehdi Mirza, Karol Gre-
gor, Rishabh Kabra, Sebastien Racaniere, Theophane
Weber, David Raposo, Adam Santoro, Laurent Orseau,
Tom Eccles, Greg Wayne, David Silver, Timothy Lilli-
crap, and Victor Valdes. An investigation of model-free
planning: boxoban levels. https://github.com/deepmind/
boxoban-levels/, 2018. Accessed: 2023-05-01.
[Guez et al. , 2019 ]Arthur Guez, Mehdi Mirza, Karol Gre-
gor, Rishabh Kabra, Sebastien Racaniere, Theophane We-
ber, David Raposo, Adam Santoro, Laurent Orseau, Tom
Eccles, Greg Wayne, David Silver, and Timothy Lillicrap.
An investigation of model-free planning. In Proceedings
of the 36th International Conference on Machine Learn-
ing, volume 97 of Proceedings of Machine Learning Re-
search , pages 2464–2473. PMLR, 2019.
[Hazan, 2016 ]Elad Hazan. Introduction to online convex
optimization. Foundations and Trends® in Optimization ,
2(3-4):157–325, 2016.
[Hinton, 2002 ]Geoffrey E. Hinton. Training Products of Ex-
perts by Minimizing Contrastive Divergence. Neural Com-
putation , 14(8):1771–1800, 08 2002.
[Jabbari Arfaee et al. , 2011 ]S. Jabbari Arfaee, S. Zilles, and
R. C. Holte. Learning heuristic functions for large state
spaces. Artificial Intelligence , 175(16-17):2075–2098,
2011.
[Jaggi, 2013 ]Martin Jaggi. Revisiting Frank-Wolfe:
Projection-free sparse convex optimization. In Proceed-
ings of the 30th International Conference on Machine
Learning , volume 28(1) of Proceedings of Machine Learn-
ing Research , pages 427–435, Atlanta, Georgia, USA, 17–
19 Jun 2013. PMLR.
[Joulani et al. , 2013 ]Pooria Joulani, Andras Gyorgy, and
Csaba Szepesvari. Online learning under delayed feed-
back. In Proceedings of the 30th International Confer-
ence on Machine Learning , volume 28(3) of Proceedings
of Machine Learning Research , pages 1453–1461. PMLR,
2013.[Korf, 1997 ]Richard E. Korf. Finding optimal solutions to
rubik’s cube using pattern databases. In Proceedings of
the Fourteenth National Conference on Artificial Intelli-
gence and Ninth Conference on Innovative Applications of
Artificial Intelligence , AAAI’97/IAAI’97, page 700–705.
AAAI Press, 1997.
[Mattern, 2013 ]Christopher Mattern. Linear and geometric
mixtures - analysis. Proceedings of the Data Compression
Conference , pages 301–310, 02 2013.
[Mattern, 2016 ]Christopher Mattern. On Statistical Data
Compression . PhD thesis, Technische Universit ¨at Ilmenau,
Fakult ¨at f¨ur Informatik und Automatisierung, Feb 2016.
[Matthew, 2005 ]V Mahoney Matthew. Adaptive weighing
of context models for lossless data compression. Florida
Institute of Technology CS Dept, Technical Report CS-
2005-16, https://www. cs. fit. edu/Projects/tech reports/cs-
2005-16. pdf , 2005.
[Mittal et al. , 2022 ]Dixant Mittal, Siddharth Aravindan, and
Wee Sun Lee. Expose: Combining state-based explo-
ration with gradient-based online search, 2022. arXiv
2202.01461.
[Nesterov, 1983 ]Yurii Nesterov. A method for solving
the convex programming problem with convergence rate
o(1/k2).Proceedings of the USSR Academy of Sciences ,
269:543–547, 1983.
[Orabona and P ´al, 2018 ]Francesco Orabona and D ´avid P ´al.
Scale-free online learning. Theoretical Computer Science ,
716:50–69, 2018.
[Orseau and Hutter, 2021 ]Laurent Orseau and Marcus Hut-
ter. Isotuning with applications to scale-free online learn-
ing, 2021.
[Orseau and Lelis, 2021 ]Laurent Orseau and Levi H. S.
Lelis. Policy-guided heuristic search with guarantees. Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
35(14):12382–12390, May 2021.
[Orseau et al. , 2018 ]Laurent Orseau, Levi Lelis, Tor Latti-
more, and Theophane Weber. Single-agent policy tree
search with guarantees. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing
Systems , volume 31. Curran Associates, Inc., 2018.
[Pearl, 1984 ]Judea Pearl. Heuristics: Intelligent Search
Strategies for Computer Problem Solving . Addison-
Wesley Longman Publishing Co., Inc., USA, 1984.
[Pohl, 1970 ]Ira Pohl. Heuristic search viewed as path find-
ing in a graph. Artificial Intelligence , 1(3):193 – 204,
1970.
[Ratner and Warmuth, 1986 ]Daniel Ratner and Manfred
Warmuth. Finding a shortest solution for the nxn exten-
sion of the 15-puzzle is intractable. In Proceedings of the
Fifth AAAI National Conference on Artificial Intelligence ,
AAAI’86, page 168–172. AAAI Press, 1986.
[Rissanen, 1983 ]J. Rissanen. A universal data compres-
sion system. IEEE Transactions on Information Theory ,
29(5):656–664, 1983.[Shalev-Shwartz, 2007 ]Shai Shalev-Shwartz. Online learn-
ing: Theory, algorithms, and applications . PhD thesis,
Hebrew University, Jerusalem, 2007.
[Sutton and Barto, 1998 ]Richard S. Sutton and Andrew G.
Barto. Reinforcement Learning : An Introduction . MIT
Press, 1998.
[Truong and Nguyen, 2021 ]Tuyen Truong and Hang-Tuan
Nguyen. Backtracking gradient descent method and some
applications in large scale optimisation. part 2: Algorithms
and experiments. Applied Mathematics & Optimization ,
84:1–30, 12 2021.
[Veness et al. , 2017 ]Joel Veness, Tor Lattimore, Avishkar
Bhoopchand, Agnieszka Grabska-Barwinska, Christopher
Mattern, and Peter Toth. Online learning with gated linear
networks, 2017. arXiv 1712.01897.
[Veness et al. , 2021 ]Joel Veness, Tor Lattimore, David Bud-
den, Avishkar Bhoopchand, Christopher Mattern, Ag-
nieszka Grabska-Barwinska, Eren Sezener, Jianan Wang,
Peter Toth, Simon Schmitt, and Marcus Hutter. Gated lin-
ear networks. Proceedings of the AAAI Conference on Ar-
tificial Intelligence , 35(11):10015–10023, May 2021.
[Willems et al. , 1995 ]Frans M. J. Willems, Yuri M.
Shtarkov, and Tjalling J. Tjalkens. The context tree
weighting method: Basic properties. IEEE Transactions
on Information Theory , 41:653–664, 1995.
[Zinkevich, 2003 ]Martin Zinkevich. Online convex pro-
gramming and generalized infinitesimal gradient ascent. In
Proceedings of the Twentieth International Conference on
Machine Learning , pages 928–935, 2003.A Bootstrap Algorithm Details
Algorithm 2 Bootstrap using LTS_CM (given in Algo-
rithm 1), which returns "budget_reached" when the
number of nodes expanded reaches Bt, or returns a solu-
tion node n∗∈ N∗(n0)if it reaches n∗, or returns "
no_solution" if all nodes have been expanded without
exhausting the budget and without reaching a solution node,
which means that the problem has no solution.
#N0: set of root nodes = problems
#B1: initial budget
#π1: initial policy
# Returns the set of solution nodes
def Bootstrap_with_LTS( N0,B1,π1):
solns = {} # dictionary of problem -> solution
for t = 1, 2, ...:
for each n0∈ N0:
result = LTS_CM( n0,Bt,βt)# search
if result is "no_solution": N0← N0\ {n0}
if result is a node n∗: soln[ n0] =n∗
if len(soln) = |N0|: return soln
# Update the parameters of the model
βt+1≈argminβ∈BL(soln.values () , β) +R(β)
choose budget Bt+1
Orseau and Lelis [2021 ]use a variant of the Bootstrap pro-
cess [Jabbari Arfaee et al. , 2011 ]to iteratively solve a set of
problems while improving the policy based on the solutions
for the already solved problems. See Algorithm 2.
At each Bootstrap iteration t, LTS is run on each problem
(even those already solved) with a budget of Btnode expan-
sions with the context-model policy with current parameters
βt. After collecting the set of solutions Nt, the parameters
βt+1are obtained from Eq. (8) to some approximation (see
Appendix C), and the next bootstrap iteration t+ 1is started
with budget Bt+1.
Adjusting the budget is non trivial. Keep in mind that com-
putation time during search is proportional to the number of
expansions. Solving previously solved problems usually is
fast, because the policy has been optimized for them. Each
problem for which a solution is newly found usually takes
a large fraction of the budget (since they couldn’t be solved
for the previous budget), and every problem that remains un-
solved consumes the whole budget. While a larger budget
means that more problems can be solved, for a fixed set of
parameters it is usual to see this number grow only logarith-
mically with the budget (since we are tackling hard prob-
lems). Hence when only few problems have already been
solved, a large budget will make the algorithm spend a lot of
time in yet-unsolvable problems, wasting computation time.
By contrast, a too small budget will prevent finding new so-
lutions and improving the policy, requiring more Bootstrap
iterations.
Jabbari Arfaee et al. [2011 ]double the budget at each new
Bootstrap iteration. This can become wasteful in computa-
tion time if learning works well, but a larger budget does nothelp much, in which case it may be better to use a constant
budget. It may also be not fast enough during the last iter-
ations: suppose 95% of the problems are solved, but the re-
maining 5% ones require to double the budget kmore times:
then the 95% will be resolved kmore times (possibly finding
different solutions), and, if the found solutions change, opti-
mization is also performed ktimes before any new problem
can be solved. By contrast, Orseau and Lelis [2021 ]use a
fixed budget and double the budget only if no new problem
is solved, which can also be wasteful in computation time if
learning does not manage to work well enough and just one
more problem is solved at each step.
To alleviate these issues, first, if more than a factor (1 +b)
(sayb= 1/4) of problems are solved at iteration tcom-
pared to the previous iteration — formally, |Nt| ≥ (1 +
b)S
t′<tNt′— we reduce the next budget in case the cur-
rent budget is too high:
Bt+1= max {B1, Bt/2}.
Otherwise, we increase Bt+1so as to approximately dou-
ble the number of total expansions (in the worst case of no
new problem solved), rather than merely doubling the budget.
More precisely, at Bootstrap iteration t, say the total number
of expansions of solved problems is T+
t, and the number of
remaining unsolved problems is s−
t=|N∗\S
t′≤tNt′|, then
we set the next budget
Bt+1= 2Bt+T+
t/s−
t,
which ensures that T+
t+s−
tBt+1= 2(T+
t+s−
tBt), where
T+
t+s−
tBtis the actual number of expansions used dur-
ing iteration t, and T+
t+s−
tBt+1is the probable number of
expansions in case no new problem is solved, and assuming
that previous problems take about the same time to be solved
again (which is likely to be an overestimate due to learning).
B Formal Statement of the Lower Bound
In this section we provide a formal version of the informal
lower bound of Theorem 2 on the number of node expansions
required before reaching a target node n∗. This number is
within a factor (A−1)¯dof the upper bound, showing that the
upper bound is quite tight and can be meaningfully used as a
loss function.
The lower bound in Theorem 11 below requires the fol-
lowing lemma, which shows that the probability mass at the
root behaves like a liter of water that is distributed recursively
(but unevenly) along all the branches, and that if we collect
the water at all the leaves (assuming a finite tree) then it still
amounts to one liter, as long as the policy is proper. This
lemma can be found in a compact form in the proof of Theo-
rem 3 [Orseau et al. , 2018 ].
A tree N′⊆ N is said to be fullif every node of the tree
either has all its children in the tree, or none of them.
Lemma 10. LetN′⊆ N be a finite tree with root n0, and
letL′⊆ N′be its leaves. Let πbe a policy with π(n0) = 1 .
ThenP
n∈L′π(n)≤1. Furthermore, if the policy is proper
andN′is a full tree, thenP
n∈L′π(n) = 1 .Proof. We start with the equality case. Using the fact that the
policy is proper on the second line, and the fact that the tree
N′is full on the fourth line, we have
X
n∈L′π(n) =X
n∈N′π(n)−X
n∈N′\L′π(n)
=X
n∈N′π(n)−X
n∈N′\L′π(n)X
n′∈C(n)π(n′|n)
=X
n∈N′π(n)−X
n∈N′\L′X
n′∈C(n)π(n′)
=X
n∈N′π(n)−X
n∈N′\{n0}π(n′)
=π(n0) = 1 .
If the tree N′is not full, it suffices to assign probability 0
to children outside of N′, which reduces to an improper pol-
icy. If the policy is not proper, it can be made proper on
N′by renormalization of πto˜π. More precisely, if the
treeN′is not full or the policy is not proper, define, for
alln∈ N′\ L′, for all n′∈ C(n)∩ N′: ˜π(n′|n) =
π(n′|n)/P
n′′∈C(n)∩N′π(n′′|n), which ensures that
˜π(n)≥π(n)for all nodes n∈ N′, and thusP
n∈L′π(n)≤P
n∈L′˜π(n) = 1 .
For a node n∗, define N(n∗) ={n∈ N : root( n) =
root( n∗)∧d
π(n)≤d
π(n∗)}, which is the set of nodes of
the same tree of cost at most that of n∗, and L′(n∗) =
{n∈ N :d
π(n)>d
π(n∗)∧par(n)∈N(n∗)}, which is
the set of children right outside N(n∗)—L′(n∗)would be
the ‘frontier’ or the contents of the priority queue in Algo-
rithm 1, disregarding tie breaking. Let A≥2be the maxi-
mal branching factor of the search tree N(n∗), that is, for all
n∈ N :|C(n)| ≤A. Observe that N(n∗)may not be a full
tree, but that N(n∗)∪ L′(n∗)is a full tree.
Theorem 11 (Lower bound) .Letπbe a proper policy. Then,
for a node n∗, the number of nodes with cost at most that of
n∗is at least
|N(n∗)| ≥1
(A−1)1
¯dd
π(n∗)−1
.
where ¯d= 1/P
n∈L′(n∗)π(n)
d(n)is the harmonic average of the
depth at the leaves L′(n∗).
Also observe that by the harmonic-mean – arithmetic mean
inequality, ¯d≤P
n∈L′(n∗)π(n)d(n), the average depth at
the leaves of the search tree N(n∗).
Proof. First, using the fact that |L′(n∗)∪N(n∗)|is a full
tree,
|L′(n∗)|+|N(n∗)|=|L′(n∗)∪N(n∗)|
= 1 +X
n∈N(n∗)X
n′∈C(n)1≤1 +A|N(n∗)|,
and by rearranging we obtain
|N(n∗)| ≥(|L′(n∗)| −1)/(A−1).n0
n1
n1,11
A
n1,21
A
. . . n1,A1
A1−2
A
n2
n2,11
2
n2,21
22
A
Figure 3: A tree showing the necessity of the factor A−1for the
lower bound, with A≥3. Nodes that have fewer than Achildren
can be completed with children of probability 0.
Now,
|L′(n∗)|=X
n∈L′(n∗)d
π(n)π(n)
d(n)
>d
π(n∗)X
n∈L′(n∗)π(n)
d(n)=1
¯dd
π(n∗),
SinceL′(n∗)are the leaves of a full tree,P
n∈L′(n∗)π(n) =
1by Lemma 10 and thus ¯dis indeed an harmonic mean of the
depths of the leaves. Therefore,
|N(n∗)| ≥1
(A−1)1
¯dd
π(n∗)−1
.
Remark 12. It appears that the factor 1/(A−1)is neces-
sary. Consider the tree in Fig. 3, and take A≥3. Then
d
π(n2,1) = 2 Awhiled
π(n1,·) = 2 A2/(A−2)>d
π(n2,1).
Then|N(n2,1)|= 5 = 5d
π(n2,1)/(2A) =O(d
π(n2,1)/(A−
1)).Also note that replacing 2/Awith1/Ain the tree leads
tod
π(n2,1) = 4 Aandd
π(n1,·) = 2 A2/(A−1)<d
π(n2,1),
which means that |N(n2,1)| ≥5 +A= Ω(d
π(n2,1))instead.
C Convex optimization algorithm
To minimize Eq. (6), many convex optimization algorithms
can be considered. For a first simple implementation, we
would recommend using Frank-Wolfe [Frank and Wolfe,
1956; Jaggi, 2013 ]while being mindful of numerical stability
(see Appendix D).
In the following, we describe the convex optimization rou-
tine we use for the experiments, however we suspect better
and possibly more principled algorithms might be applicable
too.
For the optimizer, we use isoGD [Orseau and Hutter,
2021 ]projection onto B(see also SOLO-FTRL [Orabona and
P´al, 2018 ]), a scale-free variant of Adagrad [Duchi et al. ,
2011 ], which is adapted to use a line search. We have ob-
served empirically that these algorithms tend to close the du-
ality gap [Jaggi, 2013 ]faster than other algorithms (Frank-
Wolfe [Frank and Wolfe, 1956 ], normalized gradient de-
scent [Cort´es, 2006 ], accelerated gradient descent [Nesterov,
1983 ]), but admittedly we did not try all variants of all algo-
rithms). However, the well-known difficulty with Adagrad-
style algorithms is that the learning rate is always smaller than1/Gwhere Gis the amplitude of largest observed gradient.
But in function optimization, almost always the largest gra-
dients are observed early and decrease significantly near the
optimum — in our case, the initial gradients can be exponen-
tially large. Hence, to reduce this dependency, we reset the
learning rates on steps that are powers of 2 — this is known
as the doubling trick [Cesa-Bianchi and Lugosi, 2006 ]and
we fully expect that a regret bound can be proven with resets
too, paying only a constant factor in the regret bound for a
significantly milder dependency on the usually-larger initial
gradients. We use one learning rate per context.
Optimization is stopped after 200 iterations: if this happens
to not be enough to improve the policy significantly to solve
more problems, 200 more iterations will be triggered anyway
after the next Bootstrap iteration.
Optimization is also stopped early if the duality gap [Jaggi,
2013 ]guarantees that the loss is within a factor 2 of the op-
timum. Recall that this roughly means that the bound on the
search time is a factor 2 away from the bound on the search
time for the optimal parameters, which can hardly be much
better. The duality gap is calculated every 20 iterations to
amortize the computation cost. See also Appendix F.
For the line search, we use some ideas from Truong and
Nguyen [2021 ]: A line search is triggered at each update iter-
ation twhere 1≤tmod 20 ≤3, and the learning rate found
by the line search is re-used for the next optimization steps as
long as there is improvement — otherwise a line search is
triggered too — and also as the first middle query of the line
search in [0, 1]. We use a (quasi-)exact line search rather
than a backtracking line search, as it can make a significant
difference on the first iterations, but less so afterwards.
See also Appendix D on numerical stability.
D Numerical Stability Considerations
One should use a numerically stable ‘softmax’ function for
product mixing, and a stable ‘log-sum-exp’ (LSE) to calcu-
late the logarithm of the LTS loss — the LTS loss can be
exponentially large for untuned parameters.
LSE( X) =C+ logX
x∈Xexp(x−C), C = max X .
For a set N′of nodes, We can rewrite Eqs. (6) and (7), and
the scaled gradient as:
logℓ(n, β) = log d(n)−d(n)−1X
j=0logp×(n[j], a(n[j+1]), β),
logL(N′, β) = LSE( {logℓ(n, β)|n∈ N′})
log(L(N′, β) +R(β)) = LSE(log L(N′, β),logR(β)).
Recall that εmix= 0 during optimization. During the line
search, one can use log(L(N′, β)+R(β))but note that, while
still unimodal (quasiconvex), it may not be convex anymore
(despite Theorem 14) with a quadratic regularizer.
To calculate the gradients, similar caution should be used,for example for some constant C:
βt+1= argmin
βe−CR(β) +X
τ∈Texp(log ℓ(τ, β)−C),
∇exp(log ℓ(τ, β)−C) = exp(log ℓ(τ, β)−C)∇logℓ(τ, β).
E Convexity
E.1 Log loss to LTS loss
Proof of Theorem 9. We can write
L(x) =X
kexp X
t−logfk,t(x)!
,
By assumption, −logfk,t(x)is convex for all k, t, and
convexity is preserved by both summation and exponentia-
tion (convex and non-decreasing) [Boyd and Vandenberghe,
2004 ], hence L(x)is convex.
Remark 13. Convexity in LTS loss does not imply convex-
ity in log loss. For example, take f(x) = 1 /x2forx > 0,
then1/fis (strongly) convex, but −log 1/x2= 2 log |x|is
not only concave on (−∞,0)and on (0,∞)but also has a
singularity at 0. In a sense, the LTS loss is ‘nicer’ for convex
optimization than the log loss.
Theorem 14. The logarithm of loss function L(·)defined in
Theorem 9 is convex.
Proof. Following the proof of Theorem 9 we can write
logL(x) = logX
kexp X
t−logfk,t(x)!
,
and the result follows by observing that log-sum-exp and
summation preserve convexity [Boyd and Vandenberghe,
2004 ].
E.2 LTS loss convexity of product mixing of
context predictors
Theorem 15. The function L(Nt, β)defined in Eq. (6)is con-
vex in β.
Proof. This function is of the formPexpPlogPexpf(β)where fis linear. The result
follows by observing that summation, exponentiation, and
log-sum-exp are all preserving convexity, since they all
are convex and non-decreasing [Boyd and Vandenberghe,
2004 ].
We now provide a more general result that applies if the
context predictors are members of the exponential family,
rather than just categorical distributions.
Lemma 16. LetAbe a finite set of actions and canonical
parameters β∈RQ×A, and let Qbe a set of predictors. Let
pc(n, a;β) = exp[ β·Tc(n, a)−Ac(n, β) +Bc(n, a)]
and all node n∈ N and all actions a∈ A, with Ac:N ×
B →RandBc:N × A → RandTc:N × A → RQ×Aforall predictors c∈ Q, be a set of members of the exponential
family in canonical form, with a dependency on the current
node n∈ N . Then the product mixing p×(n, a;β)(Eq. (2))
of the{pc}c∈Qis also a member of the exponential family in
canonical form:
p×(n, a;β) = exp[ β·T(n, a)−A(n, β) +B(n, a)],
with
T(n, a) =X
c∈QTc(n, a),
B(n, a) =X
c∈QBc(n, a),
A(n, β) = lnX
a′∈Aexp [β·T(n, a′) +B(n, a′)].
Proof. The result is a straightforward application of the defi-
nition of product mixing:
p×(n, a;β)
=Q
c∈Qpc(n, a;β)P
a′∈AQ
c∈Qpc(n, a′;β)
=exp
β·P
c∈QTc(n, a) +P
c∈QBc(n, a)
P
a′∈Aexp
β·P
c∈QTc(n, a′) +P
c∈QBc(n, a′)
= exp[ β·T(n, a)−A(n, β) +B(n, a)].
Categorical context models can be expressed as mem-
bers of the exponential family in canonical form, by setting
Tc(n, a)to a zero vector, with just a single 1 at index (c, a)
for context cand action a, but only if the context cis active at
node n,i.e.,c∈ Q(n), that is
Tc(n, a)c′,a′= [ [c′=c] ]·[ [a′=a] ]·[ [c∈ Q(n)] ],
where [ [test] ] = 1 iftest is true, 0 otherwise. This implies
that the vector T(n, a)also has a 1 at index (c, a)for each
active context c, for each action a∈ A . To select only the
valid actions at node n, also set Bc(n, a) = 0 ifa∈ A(n)
andBc(n, a) =−∞ otherwise. Then
pc(n, a;β) = exp( β·Tc(n, a)−Ac(n, β) +Bc(n, a))
= exp( βc,a[ [c∈ Q(n)] ]−Ac(n, β)) [ [a∈ A(n)] ],
Ac(n, β) = lnX
a∈A(n)exp(βc,a[ [c∈ Q(n)] ]),
that is,
pc(n, a;β) =

expβc,aP
a′∈A(n)expβc,a′ifc∈ Q(n), a∈ A(n),
0 ifa /∈ A(n),
1
|A(n)|otherwise.
Recall that uniform distributions have no effects in the prod-
uct mixing, and thus a context that is not active is in effect
removed from the product mixing, as in Eq. (4).
It is interesting to note that the Ac(n, β)do not appear di-
rectly in the resulting form of the product mixing and thus do
not need to be calculated.Beyond simple context models, since the vector Tc(n, a)
can depend on the current node n, it can make use in particu-
lar of features of the corresponding state of the environment,
such as a heuristic distance to the goal.
Finally, members of the exponential family in canonical
form are well-known to be log-concave in their natural pa-
rameters β, that is, their log loss is convex in their natural pa-
rameters: −logpc(·,·;β)is of the form h(β)+lnPexpg(β)
where handgare linear, and since log-sum-exp is convex
the result follows. Therefore, by Theorem 9, the LTS loss of
the product mixing of members of the canonical exponential
family is convex in their natural parameters.
F Beta-Simplex
This section describes a small but computationally help-
ful improvement regarding the calculation of the duality
gap[Jaggi, 2013 ], which is used to terminate the optimiza-
tion procedure.
The domain of the parameters βis defined in the main
paper as B= [ln εlow,0]|Q|×A, and wrote that pc(a;β)≥
εlow/|C(n)|.
While the duality gap can be calculated on this set, it can
also be calculated for a subset of B, which more closely re-
lates to the probability distributions of the predictors. Fur-
thermore, the regret can still be meaningfully compared to
the best probability distributions for the context predictors,
rather than the optimal parameters β∗
t∈ B.
The highest-entropy probability distribution is the uniform
distribution and can be expressed with pcby setting all com-
ponents βc,·to the same value.
The lowest-entropy probability distribution that can be ex-
pressed with pcis such that βc,a= 0for some chosen a∈ A
andβc,a′= lnεlowfora′̸=a, giving
pc(a;β) = 1 /(1 + ( A−1)εlow)≥1−(A−1)εlow,
pc(a′;β) =εlow/(1 + ( A−1)εlow)≤εlow.
Consider the constrained simplex ∆εlowsuch that if p∈
∆εlow, then for all a∈ A:p(a)≥εlow. Hence the probabil-
ity distributions expressed by Bcan express at least as much
as∆εlowby convex combinations. Unfortunately, enforcingP
aexpβ·,a= 1onBdoes not lead to a convex set.
Instead, we define the β-simplex as a (convex) subset of
Bby constrainingP
a∈Aβ·,a= (A−1) lnεlow. Note that
theβ-simplex still contains the highest and lowest entropy
distributions of ∆εlow. The ‘center’ of the β-simplex is at
β.,a= ((A−1) lnεlow)/A, which we define as β0, and ex-
plains why we use the regularization ∥β−β0∥2.
Hence, instead of calculating the regret compared to β∗∈
B, we can also consider calculating the regret to the best dis-
tribution in ∆εlowor the best point in the β-simplex.
More importantly, we calculate the duality gap [Jaggi,
2013 ]for the β-simplex, which experimentally is easier to
reduce than the duality gap for the β-hypercube B.
G Mutex Sets: Relative Tilings
We now give a general and formal definition of the rectan-
gular tiling example in Figure Figure 2. It is closely relatedto tile coding [Sutton and Barto, 1998 ]. On a grid of dimen-
sionR×C, relative to some position (r0, c0)on the grid,
we call a relative tile T(sr, sc, dr, dc)a particular mutex set
with row span sr, column span sc, row offset drand column
offset dc. The ordered values of the grid rectangle between
(r0+dr, c0+dc)and(r0+dr+sr−1, c0+dc+sc−1)
identify a unique context within the relative tile. A padding
value can be chosen arbitrarily for coordinates that are outside
the grid.
Arelative tiling RT(sr, sc, Dr, Dc)of row span sr, col-
umn span sc, row distance Drand column distance Dc, rela-
tive to some position (r0, c0)is a set of relative tile mutex sets
{T(sr, sc, dr, dc)}dr,dcfordr∈[−Dr, . . . , D r−sr+1] and
dc∈[−Dc, . . . , D c−sc+1]— this ensures that the last posi-
tion of the last relative tile is at (r0+Dr, c0+Dc), while the
first position of the first relative tile is at (r0−Dr, c0−Dc).
There are (2Dr+ 2−sr)(2Dc+ 2−sc)mutex sets in the
relative tiling.
In particular the mutex sets used in the experiments for
Sokoban, The Witness, and the Sliding-Tile Puzzle are as fol-
lows: The position (r0, c0)is taken to be the position of the
agent: the avatar in Sokoban, the blank tile in the sliding tile
puzzle, and the tip of the ‘snake’ in The Witness.
We used the following relative tilings. For Sokoban:
RT(3,3,4,4),RT(2,4,2,3),RT(4,2,3,2)RT(2,2,2,2),
RT(1,2,1,1),RT(2,1,1,1), the number of mutex sets is
|M| = 125 , and walls are used as padding value; For
The Witness: RT(3,3,4,4),RT(2,2,4,4),RT(2,1,1,1),
RT(1,2,1,1),|M| = 125 , with one additional padding
color, and the goal location is not encoded. For the STP:
RT(2,2,3,3),RT(2,1,2,2),RT(1,2,2,2),RT(1,1,2,2),
|M|= 102 , with an additional padding value.
For The Witness, our implementation uses two grids: one
for the (fixed) colors and one for the snake (the trajectory of
the player). We perform the same relativing tiling on each
grid in parallel, merging each pair of contexts into a single
context.
H Extended Table of Results
While the message of the main paper is to compare LTS+CM
with LTS+NN, it is also interesting to compare LTS+CM with
other algorithms for the same domains. See Table 2.
In particular, Orseau and Lelis [2021 ]introduce the PHS*
algorithm, which is based on LTS, but also uses a heuris-
tic function to speed up the search, and they also compare
with Weighted A* (WA*) [Pohl, 1970; Ebendt and Drechsler,
2009 ]which uses only a heuristic function, both with similar
neural networks as LTS+NN. We can see LTS+CM is com-
petitive on all three domains, while being fast (recall that tests
use only one CPU, and no GPU). Moreover, LTS+CM could
also be extended with a value function, either to PHS*+CM,
or by using the value function as input features to each con-
text, or by binarizing the values into multiple (possibly over-
lapping) contexts.
For the STP, while the test set used for DeepCubeA is
different from the one used by the other algorithms,5we
5The training set is also different, and although the number of
problems is not clearly specified, it appears to be much larger thanstill expect the results to be comparable since Orseau and
Lelis [2021 ]’s test sets are composed of random instances
rather than scrambled from the solution. The heavy cost in
expansions paid by DeepCubeA shows the price of finding
near-optimal solutions — which it is reported to find 97% of
the time. WA* appears to give the best compromise on this
problem. LTS+CM expands more nodes but, given that it is
still fast in milliseconds, we can hope for better results possi-
bly by adding more mutex sets.
DeepCubeA has also been trained on the 900k unfiltered
Boxoban set [Guez et al. , 2018 ], and finds short solutions in
few expansions. But because it is trained differently from
the other algorithms, its results are not directly comparable.
We trained LTS+CM on the 450k medium Boxoban set and
obtained a slightly better average number of expansions, with
a much faster algorithm in milliseconds — at the expense of
solution length. The same LTS+CM also expands almost 4x
fewer nodes on the Boxoban hard set than the one trained with
only 50k problems.
On the Rubik’s cube, we report results for LTS+CM at var-
ious stages of its training. After just 300k cubes (2 hours 10
minutes), scrambled at most 15 times (these are much eas-
ier than fully scrambled cubes) it already finds a policy that
solves the whole random test set — scrambled 100 times,
which is usually considered more than sufficient for gener-
ating random cubes [Korf, 1997 ]. At 400k cubes, the policy
is already substantially faster than previous work. We trained
the network for up to 9M cubes, but the average number of
expansions stabilizes at around 380. The training curve is
shown in Fig. 4.
We also compared LTS+CM on the same 100 hardest Ru-
bik’s Cube problems of B ¨uchner et al. [2022 ]as used by
[Allen et al. , 2021 ]. This set appears slightly simpler than the
test set we used because we are able to find shorter solutions
with fewer expansions in this set than in our set of problems.
Note also that Allen et al. [2021 ]do not account for the length
of the macro actions in the number of expansions, because
they use a logical representation of the problem to ‘compress’
the results of macro actions — this assumes access to more
information about the environment than just a simulator. The
two approaches, focused macro-actions and learning a policy,
are very much composable, and it would be interesting to see
whether such macro actions could help make LTS+CM more
efficient in training time or converge to a faster policy. As
far as we are aware, LTS+CM is the first machine-learning
algorithm to learn a fast policy for the Rubik’s cube — while
using only little domain-specific knowledge.
Finally, it must be noted that the timings reported in the ta-
ble should be read with a grain of salt, as different algorithms
have been tested on different machines and implemented us-
ing different libraries. LTS+CM has been implemented in
Racket6and we report running times as a means of showing
that the CM models are very fast in practice.
50k problems.
6https://racket-lang.org/Wall-clock training timeWall-clock training timeWall-clock training timeWall-clock training timeWall-clock training timeWall-clock training timeWall-clock training timeWall-clock training timeWall-clock training timeAverage expansions on test setAverage expansions on test setAverage expansions on test setAverage expansions on test setAverage expansions on test setAverage expansions on test setAverage expansions on test setAverage expansions on test setAverage expansions on test set
0d0d0d0d0d0d0d0d0d 1d1d1d1d1d1d1d1d1d 2d2d2d2d2d2d2d2d2d 3d3d3d3d3d3d3d3d3d10²10²10²10²10²10²10²10²10²10³10³10³10³10³10³10³10³10³10⁴10⁴10⁴10⁴10⁴10⁴10⁴10⁴10⁴10⁵10⁵10⁵10⁵10⁵10⁵10⁵10⁵10⁵10⁶10⁶10⁶10⁶10⁶10⁶10⁶10⁶10⁶10⁷10⁷10⁷10⁷10⁷10⁷10⁷10⁷10⁷
 3d 12h 50m (90) — 327 (87.3)  3d 12h 50m (90) — 327 (87.3)  3d 12h 50m (90) — 327 (87.3)  3d 12h 50m (90) — 327 (87.3)  3d 12h 50m (90) — 327 (87.3)  3d 12h 50m (90) — 327 (87.3)  3d 12h 50m (90) — 327 (87.3)  3d 12h 50m (90) — 327 (87.3)  3d 12h 50m (90) — 327 (87.3)  1h 48m (3) — 247 863 (39.8)  1h 48m (3) — 247 863 (39.8)  1h 48m (3) — 247 863 (39.8)  1h 48m (3) — 247 863 (39.8)  1h 48m (3) — 247 863 (39.8)  1h 48m (3) — 247 863 (39.8)  1h 48m (3) — 247 863 (39.8)  1h 48m (3) — 247 863 (39.8)  1h 48m (3) — 247 863 (39.8) 
 3h 55m (4) — 19 037 (53.4)  3h 55m (4) — 19 037 (53.4)  3h 55m (4) — 19 037 (53.4)  3h 55m (4) — 19 037 (53.4)  3h 55m (4) — 19 037 (53.4)  3h 55m (4) — 19 037 (53.4)  3h 55m (4) — 19 037 (53.4)  3h 55m (4) — 19 037 (53.4)  3h 55m (4) — 19 037 (53.4) 
 8h 35m (6) — 8 354 (59.1)  8h 35m (6) — 8 354 (59.1)  8h 35m (6) — 8 354 (59.1)  8h 35m (6) — 8 354 (59.1)  8h 35m (6) — 8 354 (59.1)  8h 35m (6) — 8 354 (59.1)  8h 35m (6) — 8 354 (59.1)  8h 35m (6) — 8 354 (59.1)  8h 35m (6) — 8 354 (59.1) 
 16h 33m (10) — 3 856 (64.5)  16h 33m (10) — 3 856 (64.5)  16h 33m (10) — 3 856 (64.5)  16h 33m (10) — 3 856 (64.5)  16h 33m (10) — 3 856 (64.5)  16h 33m (10) — 3 856 (64.5)  16h 33m (10) — 3 856 (64.5)  16h 33m (10) — 3 856 (64.5)  16h 33m (10) — 3 856 (64.5) 
 1d 5h 18m (20) — 1 428 (74.3)  1d 5h 18m (20) — 1 428 (74.3)  1d 5h 18m (20) — 1 428 (74.3)  1d 5h 18m (20) — 1 428 (74.3)  1d 5h 18m (20) — 1 428 (74.3)  1d 5h 18m (20) — 1 428 (74.3)  1d 5h 18m (20) — 1 428 (74.3)  1d 5h 18m (20) — 1 428 (74.3)  1d 5h 18m (20) — 1 428 (74.3) 
 2d 4h 8m (50) — 485 (83.3)  2d 4h 8m (50) — 485 (83.3)  2d 4h 8m (50) — 485 (83.3)  2d 4h 8m (50) — 485 (83.3)  2d 4h 8m (50) — 485 (83.3)  2d 4h 8m (50) — 485 (83.3)  2d 4h 8m (50) — 485 (83.3)  2d 4h 8m (50) — 485 (83.3)  2d 4h 8m (50) — 485 (83.3) Figure 4: Rubik’s cube: Average number of expansions on the test set as a function of training time. Each point label should be read: “training
time (iteration) — expansions (length)”, where “training time” is the wall-clock time used for both solving and optimizing since the start,
“iteration” is the training set iteration (each set contains 100k cubes), “expansions” is the average number of expansions on the test set, and
“length” is the average length of the solutions found on the test set.Domain Algorithm %solved Length Expansions Time (ms)
Boxoban LTS+CM (this work) 100.00 41.7 2 132.3 124
LTS+NN [Orseau and Lelis, 2021 ] 100.00 40.1 2 640.4 19 500
PHS* [Orseau and Lelis, 2021 ] 100.00 37.6 1 522.1 11 300
WA*, w=1.5 [Orseau and Lelis, 2021 ]100.00 34.5 3 729.1 25 500
LTS+CM (this work) @500k 100.00 48.5 858.1 55
DeepCubeA [Agostinelli et al. , 2019 ] 100.00 32.9 1 050.0 2 350
The Witness LTS+CM (this work) 100.00 15.5 102.8 9
LTS+NN [Orseau and Lelis, 2021 ] 100.00 14.8 520.2 3 200
PHS* [Orseau and Lelis, 2021 ] 100.00 15.0 408.1 3 000
WA*, w=1.5 [Orseau and Lelis, 2021 ] 99.90 14.6 18 345.2 71500
STP (24-puzzle) LTS+CM (this work) 100.00 211.2 5 667.4 236
LTS+NN [Orseau and Lelis, 2021 ] 0.90 145.1 39 005.6 31 100
PHS* [Orseau and Lelis, 2021 ] 100.00 224.0 2 867.2 2 800
WA*, w=1.5 [Orseau and Lelis, 2021 ]100.00 129.8 1 989.8 1 600
DeepCubeA [Agostinelli et al. , 2019 ] 100.00 89.5 6 440 000.0 19 330
Boxoban hard LTS+CM (this work) 100.00 67.8 48 058.6 3 275
LTS+CM (this work) @500k 100.00 72.5 12 166.2 761
LTS+NN [Guez et al. , 2019 ] 94.00 n/a n/a 3 600
ExPoSe [Mittal et al. , 2022 ] 97.30 n/a n/a n/a
Rubik’s cube LTS+CM (this work) @300k 100.00 39.8 247 862.4 18 949
LTS+CM (this work) @400k 100.00 53.3 20 647.9 950
LTS+CM (this work) @5M 100.00 81.7 498.0 16
DeepCubeA [Agostinelli et al. , 2019 ] 100.00 21.5 ∼600 000.0 24 220
LTS+CM (this work) @5M 100.00 78.6 431.7 16
GBFS(A+M) [Allen et al. , 2021 ] 100.00 378.0 †171 300.0 n/a
Table 2: More test results. See Table 1 and the text in Appendix H for more information. The line splits in Boxoban and STP are because the
second group uses different training sets from the rest. The test set used by DeepCubeA for STP is different from that of LTS+ {CM,NN }, but
we expect the comparison to be meaningful anyway. †Does not account for the cost of macro-actions.I Table of Notation
N Set of all nodes, may contain several root nodes
n A node in N
d(n) Depth of the node n
C(n) Children of n
par(n) Single parent of n, may not exist
root( n) Topmost ancestor of n, has no parent
anc(n) Set of ancestors of n
anc+(n) anc(n)∪ {n}
desc(n) Descendants of n
desc +(n)desc(n)∪ {n}
n[j] Node at depth jon the path from
root( n) =n[0]ton=n[d(n)]
N(n) Set of nodes of cost d(·)/π(·)at most that of n
Nt Set of nodes after tproblems
N∗Set of solution nodes n∗
N∗(n) = N∗∩desc +(n)et of solution nodes below n
N0Set of root nodes (problems)
L(n) Leaves of the tree N(n)
π Policy
π(n) Probability of the node naccording to
the policy π
π(n′|n)π(n′)/π(n), assuming n′∈ C(n).
d
π(n) d(n)/π(n)
ℓ(n, β) Loss function for a single node =d
π(n)for
parameters β
L(N′, β)Cumulative loss over a set of nodes and
parameters β
Q Set of contexts
Q(n) Set of contexts active at node n
M Set of mutex sets
M A mutex set
pc(a) Probability of the action label aaccording to a
context predictor pc
p×(n, a) Product mixing of context predictors at node n
for action (edge label) a
β Parameters of the context predictors
B = [εlow,0]|Q|×A, set of all possible parameter
values for β
Bt Budget used at Bootstrap iteration t
A Set of actions (edge labels)
A(n) Set of edge labels at node n, possible actions at n
a An action, edge label
a(n) Edge label (action) from par (n)ton
R(Nt) Regret of the learner compared to the optimal
parameters for a set of solution nodes Nt
[ [test] ] =1 iftest is true, 0 otherwise