IAES International Journal of Artificial Intelligence (IJ -AI) 
Vol. 12, No.  3, September  2023, pp. 1329~1341 
ISSN: 2252 -8938 , DOI: 10.11591/ijai.v 12.i3.pp1329-1341 ÔÅ≤     1329  
 
Journal homepage : http://ijai.iaescore.com  Facial  recognition  using multi edge detection and distance 
measure  
 
 
Indo Intan1, Nurdin1, Fitriaty Pangerang2 
1Department of Informatics Engineering , Universitas Dipa Makassar, Makassar, Indonesia  
2Department of Electronics Engineering , Polytechnic  Negeri Ujung Pandang , Makassar, Indonesia  
 
 
Article Info   ABSTRAC T 
Article history:  
Received Dec 24, 2021  
Revised Apr 26, 2022  
Accepted May 19, 2022  
  Face recognition provides broad access to several public devices, so it is 
essential in cutting -edge technology . Human face recognizing has challenge 
in using uncomplicated and straightforward algorithms quickly, using 
memory specifications are not too high, otherwise the results are quality and 
accurate. Face recognition using combination edge detection and Canberra 
distance can be recommended for applications that require fast and precise 
access. The application of several edge detections singly has lo w 
performance, so it requires a combination technique to obtain better results. 
The proposed method combined several edge detections such are Robert, 
Prewitt, Sobel, and Canny to recognize a face image by identification and 
verification. As a feature extra ctor, the combination edge detection forms a 
more robust and more specific facial pattern on the contour lines. The results 
show that the combination accuracy outperforms other extractor features 
significantly. Canberra distance produces the best performan ce compared to 
Euclidean distance and Mahalanobis distance . Keyword s: 
Canberra distance  
Combination edge detection  
Distance measure  
Edge detection  
Face recognition  
This is an open access article under the CC BY -SA license.  
 
Corresponding Author:  
Indo Intan  
Department of Informatics Engineering, Universitas Dipa Makassar  
Jl. Perintis Kemerdekaan KM. 09, Talamanrea, Makassar, Indonesia  
Email: indo.intan@undip a.ac.id  
 
 
1. INTRODUCTION  
A valid identification and verification process need face detection and recognition are essential. 
Facial biometrics have the exact resemblance from one person to another even though they have faces that 
are not twins or identical, but the error calculation is still often high. The solution is based on previous 
research, namely, highly accurate feature extraction and classification processes.  Face recognition  systems 
have b een widely implemented in various accesses, making it easier for humans to carry  out their activities. 
The challenge for face detection is to detect faces quickly and efficiently using a simple, too -complex 
algorithm to obtain good quality and effective solutions  [1]. Edge detection is one of the techniques used in 
the feature extraction section, as the first step that aims to retrieve information from the image  [2]. 
The previous researchers have carried out var ious approaches and techniques, including several 
previous studies on edge detection, including edge detection through a single Sobel technique [3]; Canny 
technique [4]. Likewise, research on  the fusion or merging of several feature extractions of Sobel and Canny  
[5], [6]; Sobel and Gaussian  [7]; edge detection fusion and combination of Sobel, support vector machine 
(SVM ), and convolutional neural network ( CNN ) [8]; Canny and Prewitt  [9]; Sobel, Prewitt, Roberts, Canny, 
Laplacian of Gaussian ( LoG), expectation -maximization (EM), Otsu , and genetics  [10]. They have obtained 
include complementing the shortcomings of every single method to produce a better image; better accuracy, 
so that information retrieval is valid  [2]; can overcome noise in the image due to the presenc e of fine 
      ÔÅ≤          ISSN : 2252 -8938  
Int J Artif  Intell , Vol. 12, No. 3, September  2023: 1329-1341 1330  
threshold wavelet de -noising [7]; able to provide memory operation performance by reducing memory usage 
by 40% and processor operation by 50% in reading logic of field programmable gate array ( FPGA ) 
architecture  [3]; classifying facial expressions, which increased system recognition by 3. 71% after the 
addition of Sobel; produce better input segmentation  [9]; the use of  faster computing time because the image 
resolution used is low so the results are significant  [5]; and the segmentation results are more stable  [10]. 
The basic steps to develop a robust face recognition system [11] are: i) the face detection as the first 
step to localiz e the human faces in a particular image. Its purpose is to determine human face or not;  and ii ) 
feature extraction as the next step if the human face is detected to extract the features of the face images 
segmented. This step represents a face as features vector called a signature . It describes the face segment ed 
such as mouth, nose, and eyes with their geometry distribution [11], [12] ; iii) face recognition as the last step 
to consider the feature extracted an d compares it faces image stored in a faces database (faces gallery) [11]. 
The other word, a face recognition is a classification process to recognize probe face as one of the number 
faces in  the stored face gallery [13]. Previous research Table 1 has not completed the integration of edge 
detection in the accumulation of the pixel value of the image, only comparing singly or combining two or 
three edge detections. The classification method used is also more complicated in t heir algorithm s. They used 
the combination algoritms too, two or more, but they had not used combining of edge detection. They had 
used the other feature extraction algorithms, the more complicated feature extraction. This research focuses 
on the optimizat ion of edge detection through  a summation of the four.  It is a simple algorithm to achiev e 
better performance.  
 
 
Table 1. The approach and results of previous research  
No Author  Feature extractor  Classifier  Result  Limitation  
1 Intan [13]  Component dependent 
analysis  Euclidean 
distance, threshold 
distance  - The accuracy=96.36%  - Dataset is still less.  
- Inrespective of various pose 
and rotation  
2 Boyko et al. 
[14]  DLib, OpenCV Lib, 
HOG  SVM, deep CNN 
(DCNN ) Recognition time OpenCVLib 
<dlib  - It is still needing other 
feature extractors like Haar 
for exploration.  
3 Lakshmi and 
Patilkulakarn
i [9]  Canny and Prewitt  City block 
measure  - False rejection rate is 
improvement  false acceptance 
rate (FAR ) is lower.  - Robust classification is 
lower.  
4 Hashim and 
Shalan [15]  - Hybrid between Gabor 
filter and singular 
value decomposition  
- Image size 64x64  The statistical 
measure  
 - The accuracy of AT&T dataset: 
77.7-100%  
- The accuracy of fan energy 
index (FEI):84-100%  - The statistical measure is 
tricky, and it has random 
behavior  
- The fluctuation of changes in 
accuracy increase is affected 
by the number of training 
sets 
5 Veni et al. 
[16] - Optimalization of 
Sobel edge 
detection +ACO  Matching 
algorithm  Precision rate=1 and error rate=0  
- Efficient and better recognition 
efficiency and recall rate.  - It has limitations on rotation 
invari ant, occluded images, 
head poses.  
6 Faraji and 
Qi [17] AdaS, Gradientface, 
Weberface , local binary 
pattern (LBP), local 
direction pattern ( LDP ), 
enhanced LDP ( EnLDP ), 
local directional number 
pattern ( LDN ). Eigth local 
directional pattern 
(ELDP)  - The accuracy is 98.29%  (CMU -
PIE face database)  
- The accuracy is 100% (Yale B 
face database)  - The sample of each subject 
in the training data is still 
lacking.  
 
7 Ali et al. 
[18] LBP, LBP variance 
(LBPV ), principal 
component analysis  
(PCA ), kernel  
discriminative common 
vectors ( KDCV ). Neural network  
 - The accuracy of FG -NET face 
aging database: 93%  - The number of samples and 
the age ranges are still 
small.  
8 Kihal et al. 
[19]  Gabor filtering, phase 
encoding, linear 
discriminant analysis  
(LDA ) Hamming 
distance measure  - False rejection rate (FRR ) =% 
at 0.1% FAR; equal error rate 
(EER )=0.0916%  
- FRR=0.18% at 0.1% FAR; 
EER=0.18%%  - Database limitations.  
- No ocular feature yet.  
9 Manju and 
Radha [20]  Histogram (HOG ), 
weighted -LBP( WLBP ), 
histogram of face 
orientation ( HFO ), 
histogram of face 
direction ( HFD ) Orthogonal 
locality preserving 
projection 
(OLPP ), POLPP  Spatio -temporal frequent object 
mining -Viola -Jones based face 
detection -pose invariant OLPP 
(STFOM -FD-PIOLPP ) are better 
than STFOM -FD-OLPP and 
STOFM -FD-LPP. The significant 
difference is 0 -52-6.8%.  - It has limitations on rotation 
invariant, occluded images, 
head poses.  
 Int J Artif  Intell   ISSN:  2252 -8938  ÔÅ≤ 
 
 Facial recognition using multi edge detection and distance measure  (Indo Intan ) 1331  
The purpose  of this study is to show the significance of selecting multiple feature extractions and 
classifiers in face recognition  so that the system  performance is better than using only one or two extractors 
and classifiers. Does using a combination of edge detect ion have better accuracy than single or double edge 
detection? How does the distance measure compare to the classifier? Which detection outperforms the 
others?  
 
 
2. PROPOSED  METHOD  
The method used in this research is as showed  in Figure 1.  These are the step of the research 
methods: preprocessing, feature extraction process, classification process, and recognition. The figure 1 is 
described on section 2.1  to 2.4. 
 
 
  
Figure 1. The flowchart of the multi edge detection and distance measure  models and experimental methods 
applied  
 
 
2.1.  Data acquisition  
This study uses 300 datasets sourced from Olivetti Research Laboratorium ( ORL ) and direct 
sampling. The source of this face sample comes from 30 respondents. Each person has ten sampl es, so there 
are 300 samples in total.  The samples are divided into 270 training image samples and 30 testing face 
samples. Then the multi -edge detection  feature extraction process is carried out to produce 5 sample classes. 
Each edge detection produces 27 0 images so that the total image samples are 5 √ó270 samples=1 ,350 train 
image samples. The sample image is stored in the database, while the test image has 30 sample images to test 
system performance. All samples were taken from each person as many as ten poses with various positions, 
including front, left side, righ t side, smile, flat expression, and smiling expression.  
The cropping process was carried out of the image dataset measuring 1 ,100√ó1,500 pixels to 
100√ó100 pixels. Its small size reduce d the use of computing memory. The greyscale process change d the 
original  image to gray with a gray level of 0 -255. Next, the gray image passe d through the threshold process 
      ÔÅ≤          ISSN : 2252 -8938  
Int J Artif  Intell , Vol. 12, No. 3, September  2023: 1329-1341 1332  
as a binary image process (Figure 1).  Images cropped as frontal faces with variation in facial expressions, 
pose variation and illumination conditions.  
2.2.  Face extracture using multi edge detection  
2.2.1 . Robert edge detection  
Chen and Cheng  [21], Razzok et al. [22], Robert has explained that using a gradient  operator with 
two 2 √ó2 pixel matrices in (1). The process  is to convolute grayscale images with Robert kernel matrix in 
horizontal ( H) and vertical ( V) directions, G is gradient pixels value; Gx is Robert horizontal gradient value; 
Gy is Robert  vertical gradie nt value. It is shown in (1)  [5]. 
 
 H=[10
0-1] dan V=[0-1
10] (1) 
 
Convolution is carried out as in (2) and (3)  
 
Gx=‚àÇf(x,y)
‚àÇx=f(x,y)-f(x-1,y)  (2) 
 
Gy=‚àÇf(x,y)
‚àÇy=f(x,y)-f(x,y -1   (3) 
 
2.2.2. Prewitt edge detection  
Compare to Roberts results , Prewitt used 3 √ó3 size  gradient matrix in (9). The Prewitt method 
involves convoluting grayscale images with Its matrix in horizontal ( H) and vertical ( V) directions. G is 
gradient value or pixel value (x,  y); Gx is horizontal Prewitt gradient value; Gy is vertical Prewitt gradient 
value  (4) [5], [21], [22] . 
 
H=[-101
-101
-101] dan V=[-1-1-1
000
111]  (4) 
 
Matrix c onvolution is carried out as in ( 5) and  (6). 
 
Gx=(a2+a3+a4)-(a0+a7+a6)  (5) 
 
And 
 
Gy=(a0+a1+a2)-(a6+a2+ a4) (6) 
 
Next, the gradient magnitude is calculated using ( 7): 
 
G = ‚àö(‚àÇf
‚àÇx)2
+(‚àÇf
‚àÇy)2
= ‚àöGx2+Gy2  (7) 
 
2.2.3. Sobel edge detection  
Sobel has more improvement result than Roberts‚Äôs , which uses a gradient operator, but the matrix 
used is 3 √ó3 pixels, as shown in (5). The Sobel method is done by convoluting grayscale images with a Sobel 
matrix in H and V directions [21]. It performs a two dimensional ( 2-D) spatial gradient measurement on the 
image. T ypically, it is used to find the approximate absolute gradient magnitude at each point in the input 
grayscale image [5]. As edge detector, it uses a pair of convolution masks, one estimating the gradient in the 
x-direction (columns) and the other estimating the gradient in the y -direction (rows) such as equation  (10)-
(12) [5]. 
A convolution  mask is usually much smaller than the actual image. As a result, the mask is shifted 
over the image, manipulating a square of pixels at a time.  
 
H=[-101
-202
-101] dan V=[-1-2-1
000
121]  (8) 
 
Pixels are converted into a matrix as in (9):  Int J Artif  Intell   ISSN:  2252 -8938  ÔÅ≤ 
 
 Facial recognition using multi edge detection and distance measure  (Indo Intan ) 1333  
f(x,y)=[a0 a1 a2
a7(x,y)a3
a6 a5 a4]  (9) 
 
f(x,y) is pixel size of the image in the matrix; a n is pixel value in the matrix; (x,y) is new pixel value from the 
convolution. Sobel method matrix convolution is done by (10) and (11).  
 
Gx=(a2+2a3+a4)-(a0+2a7+a6)  (10) 
 
Gy=(a0+2a1+a2)-(a6+2a5+a4)  (11) 
 
Next, the gradient is calculated using (12) with the G value is gradient value or pixel value (x,y); Gx is 
horizontal direction Sobel gradient value; Gy is Sobel gradient value in the vertical direction.  
 
G = ‚àö(‚àÇf
‚àÇx)2
+(‚àÇf
‚àÇy)2
= ‚àöGx2+Gy2  (12) 
 
2.2.4. Canny edge detection  
The Canny method starts from entering an image in grayscale and then performs a Gaussian filtering 
process with=1.4. The aim is to filtering noise from the initial image to get a smooth edge image. 
Furthermore, the convolution is carri ed out with (13).  
 
1
159
[    24 5 42
4912 94
512 15 12 5
4912 94
24 5 42]    
  (13) 
 
The kernel above is a Gaussian kernel of order 5 √ó5 with=1.4. Then, the magnitude of the gradient 
uses (14) below  [5], [22] . The Canny kernel is convoluted with images, starting from the left row to the right 
side one pixel until it reaches the entire length horizontally. When it finished doing, the continued processing 
to reach the entire length vertically. The proce ss continue s on the second row until all horizontal and vertical 
pixels go through a convolution process according to the pattern of the Canny kernel.  
 
G=|Gx|+|Gy|  (14) 
 
Furthermore, to determine the direction of the edge used (15)  which is denoted ùúÉ. It is resulted from 
invers of tan (ùê∫ùë¶
ùê∫ùë•) which have value between 0 and 1. The value reconstruct a new image based on edge 
detection sharpened.  
 
Œ∏=arctan (Gy
Gx)  (15) 
 
Then the process of eliminating non -maximum values (non -maxima suppression) is carried out. If 
the intensity value is not maximum, the pixel value will be decreased to 0. Th e next step is the thresholding 
process using the threshold values T min and T max as in (16) . g(x) value determines the last result on binary 
value, 0 and 1. It is an edge of the image sharped. A number of the edge of images forms a feature map . 
 
g(x, y)={1, jika Tmin‚â§f(x,y)‚â§Tmax 
0, otherwise=  (16) 
 
2.2.5. Proposed method: combination edge detectio n 
The combination technique is an approach that researchers take to obtain maximum feature 
extraction results. The technique  is to combine the edge detection of Robert ( Gx1), Prewitt ( Gx2), Sobel ( Gx3), 
and Canny ( Gx4) based on (17) and (18).  The pixel mapping for each image on each edge detection will be 
aggregated by row horizontally and vertically. The pixels shift process along the horizontal will be summed 
based on the Gx notation. While the vertical direction, the pixels shift process vertica lly is added to the total 
number of shifts, Gy notation.  
ùê∫ùë•=ùê∫ùë•1+ùê∫ùë•2+ùê∫ùë•3+ùê∫ùë•4  (17)       ÔÅ≤          ISSN : 2252 -8938  
Int J Artif  Intell , Vol. 12, No. 3, September  2023: 1329-1341 1334  
 
ùê∫ùë¶=ùê∫ùë¶1+ùê∫ùë¶2+ùê∫ùë¶3+ùê∫ùë¶4  (18) 
 
In (17) and (18) show the process of the sum of edge detections in the horizontal ( Gx) and vertical 
(Gy) directions. Each edge detection will give each other a strengthening effect on the image object. If one 
edge detection gives a dark side at the edges, then the other edge detection will give a good effect, sharpening 
every edge of the face object. The sound effect on each edge would give a name to the outline of the 
eyebrows, eyes, nose, and mouth. This effect will provide a precise extraction in binary or numeric values to 
be continued in the classification and recognition process.  
 
2.3.  Classification using multi distance mea sure 
2.3.1. Euclidean distance  
 The Euclidean distance  (ED)  method compares the minimum distance of the test image with the 
training image database  [12], [22] . The ED of the two  vectors x and y is calculated by (19)  [23]: 
 
d(x, y)= (‚àë(xi-yi)2
i)1
2  (19) 
 
The smaller the value of d(x,  y), the more similar the two vec tors are compared. On the other hand, the 
greater the value of d(x,  y), the more different the two vectors are matched.  
 
2.3.2. Canberra distance  
Canberra distance (CD) is used to get the distance from a pair of points where the data is original 
and in a vector space. CD provides output in the form of actual (true) and false (false) values as shown in 
(20) [23]‚Äì[25].  
 
Dx,y= ‚àë|x- y|
(x+y)n
k=1  (20) 
 
Dxy as CD process , x as gallery image vector, y as probe image vector, and k as vector number . 
 
2.3.3. Mahalanobis distance  
This distance is defined  as the distance between two points involving a multiplier and can reduce 
distance distortion caused by linear combinations, as shown in (21)  [26]. The Mahalanobis  distance  (MD) -
based sco re is calculated as the minimum squared MD as shown in (21) [27]. 
 
ùê∑(ùë•,ùë¶)=‚àö(ùë•‚àíùë¶)ùëáùê∂‚àí1(ùë•‚àíùë¶)  (21) 
 
MD is obtained by calculating multiplication elements, both of transpose and inverse of covariance. 
Transpose and inverse of covariance to the difference between training data and testing data. MD will 
produce the square root of the product of both.  
 
2.4.  Performance measure  
The p erformance testing indicates the proposed method results from the summation of the four edge 
detection vectors. Furthermore, the distance measure calculates the error between the training data and the 
testing data. The confusion matrix determines the validation between both data. It also determines whether a 
model performs better or worse.  percentage of accuracy obtained from multi -edge detection and multi -
classifier techniques . 
The p ercentage of accuracy is the level of performance.  The confusion matrix denotes the error r ate 
in the vector form with  a 2√ó2 matrix which deals with the four terms such as true positive signif ies correctly 
identified, true negative, false positive , and fal se negative  [22]. 
 
Confusion  Matrix=[ùëáùëüùë¢ùëí ùëÅùëíùëîùëéùë°ùëñùë£ùëí  (ùëáùëÅ)ùêπùëéùëôùë†ùëí ùëÉùëúùë†ùëñùë°ùëñùë£ùëí (ùêπùëÉ)
ùêπùëéùëôùë†ùëí ùëÅùëíùëîùëéùë°ùëñùë£ùëí  (ùêπùëÅ)ùëáùëüùë¢ùëí ùëÉùëùùëúùë†ùëñùë°ùëñùë£ùëí  (ùëáùëÉ)]  
 
These measure can be constructed by metrics: (1)  true positive is the number of valid classified of images 
gallery a nd faces probe; (2) true negative is the number of valid classified of outer images galley and faces 
probe  [28], [29]  as shown in (22).  
 Int J Artif  Intell   ISSN:  2252 -8938  ÔÅ≤ 
 
 Facial recognition using multi edge detection and distance measure  (Indo Intan ) 1335  
Accuracy = (ùëáùëüùë¢ùëí ùëÉùëúùë†ùëñùë°ùëñùë£ùëí +ùëáùëüùë¢ùëí ùëÅùëíùëîùëéùë°ùëñùë£ùëí
ùëáùëúùë°ùëéùëô ùëõùë¢ùëöùëèùëíùëü  ùëúùëì ùëñùëöùëéùëîùëíùë†)ùë• 100%  (22) 
 
 
3. RESULTS AND DISCUSSION   
3.1.  Data acquisition  
The data acquisition stage was resizing from the original size to a 100 √ó100 image, and from a color 
image to a greyscale image as shown in Figure 2. Each pixel has an image component of red, green, and blue 
(RGB) as the primary  component of the image color. This value went  through a gr eyscale process that add ed 
up each color component value and then divide d by 3. Its results were greyscale d images on  a 0-255 scale as 
shown in Figure 2.   
Each face image has five poses on position: straight front view; 15o right tilt view; 15o left tilt v iew. 
Facial expression  and eyes sight are flexible, smile, flat , or free. The registered on the face is stored on face 
gallery or image database.  
 
 
 
 
Figure  2. Image dataset in image gallery [30] 
 
 
3.2.  Face extra ctor using mu lti-edge detection  
There were  five stages of the edge detection process at the feature extraction stage. The detection 
techniques were  Robert, Prewitt, Sobel, Canny, and Combination.  They resulted in pixel value in 5√ó5 size  
(Figure 3). A 25 pixels image -shaped feature map based on  the extraction pattern of multi -edge detection . 
Multi -edge detection had a mean of 116,333 pixels. The proposed method showed higher pixel value than 
others because it was the sum result of Robert, Prewitt, Sobel, and Canny. The  feature map values were 
differentiator s of an image  and the others. They were so identity value of the person concerned when the 
training and testing were processing.  
 
 
 
 
Figure 3. Feature map of 5 √ó5 size image  
 
 
Edge detections had different fluctuation  values of a face image as Figure 4. The combination of 
adding convolution value of the four edge detections. Feature extraction had different values depending on 
the type used. The proposed method had a higher value because it had been the sum of the four  feature 
extractions, Robert, Sobel, Prewitt , Canny , and Combination  1 and Combination 2  respectively. This value 
had not indicated accuracy, but it indicated as the lowest to the highest feature vector.  Combination 1 
represents the feature extraction valu e of the summation of the four edge detections. Combination 2 
represents the results of Combination 1 thresholding on 255 level as the highest level of greyscale. 
Thresholding cuts the feature extraction values if its values are greater than 255 as 255.  
      ÔÅ≤          ISSN : 2252 -8938  
Int J Artif  Intell , Vol. 12, No. 3, September  2023: 1329-1341 1336  
 
 
Figure 4. Fluctuation of multi edge detection  
 
 
The feature extractions proposed have a visualization of edge detection such  on Figure 5. They are i) 
Robert edge detection , ii) Prewitt edge detection , iii) Sobel edge detection , iv) Canny edge detection , and v) 
Combination edge detection.  Robert edge detection  has smoother and very thin gradations  or contours on the 
eyebrows, eyes, nose, and mouth; visually, it is challenging to capture the object because some points are 
given contour reinforcem ent. Prewitt  edge detection has edges that are very sharp with amplification at the 
boundaries of the face, eyebrows, eyeballs, and nose very clearly, except the mouth is contoured with slightly 
blurred lines. Sobel edge detection strengthens the thin but more straightforward outlines between the 
boundaries of the face, eyebrows, eyeballs, nose, and mouth so that the sketch of a human face is perfecter 
and visible, somewhat close to the photo negative. Furthermore,  Canny edge detection has very bright line 
boundaries so the differences in the contours of the facial properties are not apparent, only the edges do not 
show the contour differences between the facial property boundaries, so the information that this is a human 
face is still blurry.  The process of  perfecting the results of multi -edge detection is found in combination . It 
combines the four other edge detection.  
 
 
  
(a) 
 (b) 
 
  
(c) 
 (d) 
 
 
(e) 
 
Figure 5. Visualization of multi edge detection. They are (a) Robert edge detection, (b) Prewitt edge 
detection, (c) Sobel edge detection, (d) Canny edge detection and (e) Combination edge detection  [30]  
 
 
Int J Artif  Intell   ISSN:  2252 -8938  ÔÅ≤ 
 
 Facial recognition using multi edge detection and distance measure  (Indo Intan ) 1337  
It appears that the lines and contours of each facial property are  prominent and bright at each edge. 
The light pixel value effect indicates it is less susceptible to noise than Sobel, who has fragile outlines even 
though a face's information is clear. Combination edge detection has advantages over the other four edge 
detections in terms of: outline, very clearly visible property boundaries of the face, eyebrows, eye curvature, 
eyeball, nose, and mouth; the contours, the area boundaries of each property, the height and the area, are very 
clearly visible so that if there i s some blur or noise at some point, the facial properties will still be visible. 
The strengthening of these two lines, both the outline and the contour, provides an effective reinforcement to 
indicate the human face.  
 
3.2.  Classification using multi -distance measure  
Distance measure s determine the identity of the original face from the feature extraction in the 
cluster of dissimilarities between  the training data and the testing data. If the value of the distance measure is 
getting smaller (if the dis tance‚â§0) then the performance is getting better, on the contrary, if the distance is 
getting bigger then the performance is getting worse.  
Table 2 shows classification based on distance measure contains three classifiers that depend on 
feature extractors u sed. The classifiers outputs have a correlation with extractors. The face recognition 
process compares the value of the probe data (testing) with the face gallery, the value chosen as its identity is 
the smallest value of the distance measure. If the class ification results show the same identity as the identity 
in the face gallery, then the data is said to be valid, otherwise, the data is said to be invalid.  
The result of classifiers using the multi -classifier ED, CD, and  MD shows that ED ha s 27 valid 
samp les using Robert and combination edge detection; The CD has 100 valid samples using Sobel and 
Combination, and MD has 27 valid  samples on Robert and Combination. It appears that the ED, CD, and MD 
have high invalid  samples of 17 samples (Canny), 17 samples  (Canny, respectively), and MD has 19 samples 
(Canny).  
 
 
Table 2. The classification of distance measure  
Edge 
detection  Euclidean distan ce Canberra distance  Mahalanobis distance 
Valid  Invalid  Valid  Invalid  Valid  Invalid  
Robert  20 10 30 0 12 18 
Prewitt  21 9 29 1 21 9 
Sobel  26 4 30 0 25 5 
Canny  13 17 13 17 11 19 
Combination  27 3 30 0 27 3 
5 Canny  Combination  - - Canny  Combination  
 
 
Table 3 Rank of multi edge detection and multi classifier shows ED calculation results in the highest 
rank on combination edge detection and the lowest rank on Canny edge detection. The CD calculation results 
in the highest rank on Robert, Sobel, and Combination and the lowest rank on Canny.  The validation betwee n 
training data and testing data is based on the Confusion matrix as previously mentioned. The smallest 
distance value determines the face class or the authentic owner is the same as in the face gallery. The greater 
value indicates the more significant dif ference. The validation results obtained show that there are many 
value s closeness between the gallery faces and the probe faces.  
 
 
Table 3. Rank of multi edge detection and multi classifier  
Rank  Euclidean distance  Canberra distance  Mahalanobis distance  
True  False  True  False  True  False  
1 Combination  Canny  Robert, Sobel, Combination  Canny  Combination  Canny  
2 Sobel  Robert  Prewitt  Prewitt  Sobel  Robert  
3 Prewitt  Prewitt  Canny  Robert, Sobel, Combination  Prewitt  Prewitt  
4 Robert  Sobel  - - Robert  Sobel  
 
 
3.3.  Performance measure  
Performance measurement uses an accuracy calculation based on the genuine acceptance rate  (GAR)  
and FAR . GAR indicates the level of excellence of the proposed method. On the other hand, FAR indicates 
the level of weakness. The adv antages and disadvantages refer to the level of recognition. The higher the 
GAR value, the better the system accuracy , and vice versa. In contrast to FAR, the lower the FAR, the better 
the performance of the proposed method, and the higher the value, the l ower the performance in Table 4.  
Figure 6  results of distance measure regression indicate metrics in Figure 6 (a) GAR standard d eviation of 
distance measure  and Figure 6 (b) FA R standard d eviation of distance measure .        ÔÅ≤          ISSN : 2252 -8938  
Int J Artif  Intell , Vol. 12, No. 3, September  2023: 1329-1341 1338  
 
Table 4. The Recognition result of proposed method  
Distance measure  GAR mean GAR standard deviation  FAR mean FAR standar d deviation  
Euclidean distance  11,505.78 0 1,425.258  13,447 536.472  
Mahal anobis distance  11,008.260 1,133.066 12,672 787.629 
Canberra distance  3,882.133 294.965 0 0 
 
 
 
(a) 
 
 
(b) 
 
Figure 6. Results of distance measure regression (a) GAR standard d eviation of distance measure  and  
(b) FAR standard d eviation of distance measure   
 
 
The CD GAR has a lower standard deviation so its accuracy outperforms other distance measures 
(Figure 6). The CD has a 0 FAR value because it does not have an  invalid test image.  Comparing the 
standard  deviation (Figure 6) of classifiers results to distinguish b etween authentic face image and an -
authentic face image. The s tandard deviation shows the small till the high standard deviation in i) GAR 
standard deviations is small (Euclidean distance), middle (Mahalanobis distance), and high (Canberra 
distance); and ii) FAR standard deviation are small (Euclidean distance), middle (Mahalanobis distance), and 
high (Canberra distance). If the classifier has the a smaller standard deviation than other classifiers, then 
GAR and FAR are better. Otherwise, if the classifier has a standard devia tion higher than others, then GAR 
and FAR are worst. The better performance of GAR and FAR is Canberra Distance. It has a significant 
distinguishing element.  
Tabel 5 shows the performance of feature extraction and classifier ( %). The percentage value had 
gotten confusion matrix validation.  The value is different for each feature extraction and classifier. feature 
type and classification determine the best performance. The best performance on feature extraction is the 
combination outper forms  the four others. Likewise, the best classifier in Canberra distance outperforms ED 
and MD. 
 
 
Table 5. The performance measures  
Edge detection  Distance measure  
Euclidean distance  Canberra distance  Mahalanobis distance  
Robert  66.67% 100.00% 40.00% 
Prewitt  70.00% 96.66% 70.00% 
Sobel  86.66% 100.00% 83.33% 
Canny  43.33% 43.33% 36.66% 
Combination  90.00% 100.00% 90.00% 
 
 
Figure 7 describes comparing the performance of multi -edge detection and distance measure in two 
categories. There are five color curves, Sobel, Prewitt, Robert, and Canny, respectively:  (a) true positive; on 
the other hand ; and (b) false negative , the highest false negatives are Canny, Robert, Prewitt, Sobel, and 
Combination. The combination edge detection has the highest accu racy in face recognition compared to the 
Int J Artif  Intell   ISSN:  2252 -8938  ÔÅ≤ 
 
 Facial recognition using multi edge detection and distance measure  (Indo Intan ) 1339  
other four methods, as shown by each color of the line indicator. Every additional data has a better accuracy 
suitable for the increasing number of training data and testing data provided . 
The p roposed method result  has the best performance by  90.00% at ED, 100% at CD, and 90% at 
MD. The results outperform  [9], [15] ‚Äì[20], [22], [23]  using two or more feature extractor s, both edge 
detection and others. The combination edge detection has good performance equals research results 
conducted by [14], [23] ‚Äì[25].  
 
 
 
(a) 
 
 
(b) 
 
Figure 7. The performance of multi edge detection and distance measure: (a) true positive and (b) false 
positive  
 
 
The proposed  method performance outperforms the other four detections. It has the highest GAR. 
Otherwise, it has the lowest FAR. The trends showed a quite significant result than the previous research.  
Optimizing edge detection means providing a border pattern line re presenting a person's face image. The 
solution is to use a multi -layer feature extractor to provide optimal feature extraction results to solve this 
problem. The following process is through classification using a multi -distance measure which has a 
matchin g process between the probe face and the face gallery. Distance measure shows the best accuracy is 
given by the edge detection combined with a significant difference compared to the other four feature 
extraction techniques.  This proves that using multiple edge detection will reduce noise and distortion in the 
face image object. The initial stages will be optimal for the advanced processing stages of the face 
recognition system.  
 
 
4. CONCLUSION   
Combination edge detection provides the best performance among mul ti-edge detection. Comparing 
all edge detection classifier methods indicates that the combination edge detection contributes to a unique 
extraction value because it reduces noise and provides contour reinforcement to facial properties. This 
method can be u sed in developing facial recognition applications. The performance of the combination edge 
detection and classifier Canberra distance has a stable performance to be applied to facial recognition 
systems that use specifications of limited memory capacity an d device speed that is not too high. Further 
development for researchers at the feature extraction stage is recommended to use combination edge 
detection as face detection quickly. Next, verify and identify faces using other more detailed methods to 
      ÔÅ≤          ISSN : 2252 -8938  
Int J Artif  Intell , Vol. 12, No. 3, September  2023: 1329-1341 1340  
vector  accurately analyze facial properties. It will provide a proportional facial biometric performance that 
accommodates human needs to develop the latest human civilization.  
ACKNOWLEDGEMENTS  
We say we thank the Rector of Universitas Dipa Makassar  for the support in the form of motivation, 
encouragement, and funding for this research publication.  
 
 
REFERENCES  
[1] M. Jovic, S. M. Cisar, and P. Cisar, ‚ÄúDelphi application for face detection using the sobel operator,‚Äù in CINTI 2012 - 13th IEEE 
International Symposium on Computational Intelligence and Informatics, Proceedings , 2012, pp. 371 ‚Äì376,  
doi: 10.1109/CINTI.2012.6496791.  
[2] C. I. Gonzalez, P. Melin, J. R. Castro, O. Mendoza, and O. Castillo, ‚ÄúAn improved sobel edge detection method based on 
generalized type -2 fuzzy logic,‚Äù Soft Computing , vol. 20, no. 2, pp. 773 ‚Äì784, 2016, doi: 10.1007/s00500 -014-1541 -0. 
[3] Z. E. M. Osman, F . A. Hussin, and N. B. Z. Ali, ‚ÄúHardware implementation of an optimized processor architecture for SOBEL 
image edge detection operator,‚Äù in 2010 International Conference on Intelligent and Advanced Systems, ICIAS 2010 , 2010, no. 1, 
pp. 2 ‚Äì5, doi: 10.1109/IC IAS.2010.5716147.  
[4] P. Ganesan, V. Rajini, and R. I. Rajkumar, ‚ÄúSegmentation and edge detection of color images using CIELAB Color Space and 
Edge detectors,‚Äù in International Conference on ‚ÄúEmerging Trends in Robotics and Communication Technologies‚Äù, INT ERACT -
2010 , 2010, pp. 393 ‚Äì397, doi: 10.1109/INTERACT.2010.5706186.  
[5] S. Das, ‚ÄúComparison of various edge detection techniques,‚Äù in 2015 International Conference on Computing for Sustainable 
Global Development, INDIACom 2015 , 2015, pp. 393 ‚Äì396, doi: 10.14 257/ijsip.2016.9.2.13.  
[6] J. Liu and Y. Huang, ‚ÄúStudy of human face image edge detection based on DM642,‚Äù in 2010 3rd International Conference on 
Computer Science and Information Technology , 2014, vol. 8, pp. 175 ‚Äì179, doi: 10.1109/ICCSIT.2010.5563574.  
[7] W. Gao, L. Yang, X. Zhang, and H. Liu, ‚ÄúAn improved Sobel edge detection,‚Äù in Proceedings - 2010 3rd IEEE International 
Conference on Computer Science and Information Technology, ICCSIT 2010 , 2010, vol. 5, pp. 67 ‚Äì71,  
doi: 10.1109/ICCSIT.2010.5563693.  
[8] S. Liu, X. Tang, and D. Wang, ‚ÄúFacial expression recognition based on sobel operator and improved CNN -SVM,‚Äù in 2020 3rd 
IEEE International Conference on Information Communication and Signal Processing, ICICSP 2020 , 2020, pp . 236 ‚Äì240,  
doi: 10.1109/ICICSP50920.2020.9232063.  
[9] H. C. V. Lakshmi and S. Patilkulakarni, ‚ÄúSegmentation algorithm for multiple face detection for color images with skin tone 
regions,‚Äù in 2010 International Conference on Signal Acquisition and Processi ng, ICSAP 2010 , 2010, pp. 162 ‚Äì166,  
doi: 10.1109/ICSAP.2010.42.  
[10] Y. Ramadevi, T. Sridevi, B. Poornima, and B. Kalyani, ‚ÄúSegmentation and object recognition using edge detection techniques,‚Äù 
International Journal of Computer Science and Information Tech nology , vol. 2, no. 6, pp. 153 ‚Äì161, 2010,  
doi: 10.5121/ijcsit.2010.2614.  
[11] Y. Kortli, M. Jridi, A. Al Falou, and M. Atri, ‚ÄúA novel face detection approach using local binary pattern histogram and supp ort 
vector machine,‚Äù 2018 International Conference o n Advanced Systems and Electric Technologies, IC_ASET 2018 , no. March,  
pp. 28 ‚Äì33, 2018, doi: 10.1109/ASET.2018.8379829.  
[12] F. Smach, J. Miteran, M. Atri, J. Dubois, M. Abid, and J. P. Gauthier, ‚ÄúAn FPGA -based accelerator for fourier descriptors 
computin g for color object recognition using SVM,‚Äù Journal of Real -Time Image Processing , vol. 2, no. 4, pp. 249 ‚Äì258, 2007,  
doi: 10.1007/s11554 -007-0065 -6. 
[13] I. Intan, ‚ÄúCombining of feature extraction for real -time facial authentication system,‚Äù in 2017 5th In ternational Conference on 
Cyber and IT Service Management, CITSM 2017 , 2017, pp. 1 ‚Äì6, doi: 10.1109/CITSM.2017.8089223.  
[14] N. Boyko, O. Basystiuk, and N. Shakhovska, ‚ÄúPerformance evaluation and comparison of software for face recognition, based on 
Dlib and Opencv library,‚Äù in Proceedings of the 2018 IEEE 2nd International Conference on Data Stream Mining and 
Processing, DSMP 2 018, 2018, pp. 478 ‚Äì482, doi: 10.1109/DSMP.2018.8478556.  
[15] A. Noori Hashim and N. Kream Shalan, ‚ÄúFace recognition using hybrid techniques,‚Äù Journal of Engineering and Applied 
Sciences , vol. 14, no. 12, pp. 4158 ‚Äì4163, 2019, doi: 10.36478/jeasci.2019.4158. 4163.  
[16] S. H.KrishnaVeni, K. L. Shunmuganathan, and L. Padma Suresh, ‚ÄúA Novel NSCT based illuminant invariant extraction with 
optimized odge detection technique for face recognition,‚Äù International Journal of Computer Applications , vol. 70, no. 3,  
pp. 7‚Äì10, 2013, doi: 10.5120/11940 -7733.  
[17] M. R. Faraji and X. Qi, ‚ÄúFace recognition under illumination variations based on eight local directional patterns,‚Äù IET Biometrics , 
vol. 4, no. 1, pp. 10 ‚Äì17, Mar. 2015, doi: 10.1049/iet -bmt.2014.0033.  
[18] A. S. O.  Ali, V. Sagayan, A. M. Saeed, H. Ameen, and A. Aziz, ‚ÄúAge -invariant face recognition system using combined shape 
and texture features,‚Äù IET Biometrics , vol. 4, no. 2, pp. 98 ‚Äì115, 2015, doi: 10.1049/iet -bmt.2014.0018.  
[19] N. Kihal, S. Chitroub, A. Polette , I. Brunette, and J. Meunier, ‚ÄúEfficient multimodal ocular biometric system for person 
authentication based on iris texture and corneal shape,‚Äù IET Biometrics , vol. 6, no. 6, pp. 379 ‚Äì386, Nov. 2017, doi: 10.1049/iet -
bmt.2016.0067.  
[20] D. Manju and V. Rad ha, ‚ÄúA novel approach for pose invariant face recognition in surveillance videos,‚Äù Procedia Computer 
Science , vol. 167, no. 2019, pp. 890 ‚Äì899, 2020, doi: 10.1016/j.procs.2020.03.428.  
[21] X. Chen and W. Cheng, ‚ÄúFacial expression recognition based on edge d etection,‚Äù International Journal of Computer Science & 
Engineering Survey , vol. 6, no. 2, pp. 1 ‚Äì9, Apr. 2015, doi: 10.5121/ijcses.2015.6201.  
[22] M. Razzok, A. Badri, I. El Mourabit, Y. Ruichek, and A. Sahel, ‚ÄúA new pedestrian recognition system based on e dge detection 
and different census transform features under weather conditions,‚Äù IAES International Journal of Artificial Intelligence (IJ -AI), 
vol. 11, no. 2, pp. 582 ‚Äì592, Jun. 2022, doi: 10.11591/ijai.v11.i2.pp582 -592. 
[23] Y. Sari, M. Alkaff, and R. A. Pramunendar, ‚ÄúIris recognition based on distance similarity and PCA,‚Äù in AIP Conference 
Proceedings , 2018, vol. 1977, no. June, p. 020044, doi: 10.1063/1.5042900.  
[24] R. Vashistha and S. Nagar, ‚ÄúAn intelligent system for clustering using hybridization of distance function in learning vector 
quantization algorithm,‚Äù in 2017 Second International Conference on Electrical, Computer and Communication Technologies 
(ICECCT) , Feb. 2017, pp. 1 ‚Äì7, doi: 10.1109/ICECCT.2017.8117856.  
[25] D. Gonz√°lez -Jim√©nez et al. , ‚ÄúDistance measures for Gabor jets -based face authentication: a comparative evaluation,‚Äù Lecture 
Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) , vol. Int J Artif  Intell   ISSN:  2252 -8938  ÔÅ≤ 
 
 Facial recognition using multi edge detection and distance measure  (Indo Intan ) 1341  
4642 LNCS, pp. 474 ‚Äì483, 2007, d oi: 10.1007/978 -3-540-74549 -5_50.  
[26] Y. Wang, G. Nie, N. Hu, and B. Chen, ‚ÄúBearing health assessment based on different types of masses voting match and 
Mahalanobis mistance under different operating conditions,‚Äù in Proceedings of the 2nd International C onference on Artificial 
Intelligence and Advanced Manufacture , Oct. 2020, pp. 340 ‚Äì343, doi: 10.1145/3421766.3421870.  
[27] R. Kamoi and K. Kobayashi, ‚ÄúWhy is the Mahalanobis distance effective for anomaly detection?,‚Äù arXiv preprint 
arXiv:2003.00402 , Feb. 2 020, doi: 10.48550/arXiv.2003.00402.  
[28] H. A. Abu Alfeilat et al. , ‚ÄúEffects of distance measure choice on K -nearest neighbor classifier performance: a review,‚Äù Big Data , 
vol. 7, no. 4, pp. 221 ‚Äì248, 2019, doi: 10.1089/big.2018.0175.  
[29] A. Pillai, R. Sou ndrapandiyan, S. Satapathy, S. C. Satapathy, K. H. Jung, and R. Krishnan, ‚ÄúLocal diagonal extrema number 
pattern: a new feature descriptor for face recognition,‚Äù Future Generation Computer Systems , vol. 81, no. November, pp. 297 ‚Äì
306, 2018, doi: 10.1016/j.f uture.2017.09.055.  
[30] M. Tavares, ‚ÄúThe ORL database for training and testing,‚Äù Kaggle.Com , 2020. https://www.kaggle.com/tavarez/the -orl-database -
for-training -and-testing /is accessed 2021 May 4 th. 
 
 
BIOGRAPHIES OF AUTHORS  
 
 Indo Intan      holds a Bachelor of Engineering ( S.T.) in Electrical Engineering, a 
Master  of Computer, Control, and Electronic Engineering from Hasanuddin  University,  
Makassar,  Indonesia  in 20 10, with the Thesis ‚ÄúPattern Recognition of Thyroid Carcinoma 
Hystophatologic u sing ANFIS‚Äù . He is currently lecturing with the department of Informatics  
Engineering at Universitas Dipa Makassar, Makassar, Indonesia. Her research interests are in 
image processing , pattern recognition, machine learning and internet of thing.  She can be  
contacted at email: indo.intan@undipa.ac.id.  
  
 Nurdin      earned a Bachelor's degree in Computer Science at STMIK Dipanegara 
Makassar, Indonesia in 1999. He earned a Master's degree in Engineering from Hasanuddin 
University, Indonesia in 2007. He is currently a lecturer in the Department of Informatics 
Engineerin g at Dipa University Makassar. His research includes computer security, network 
security, cyber security, machine learning, and data mining . He can be contacted at email: 
nurdin@undipa.ac.id.  
  
 Fitriaty Pangerang      earned a Bachelor's degree in at electrical engineering 
Hasanuddin University in 1997. She He earned a Master's degree in Engineering from 
Hasanuddin University, Indonesia in 2007. She is currently a lecturer in the Department of 
Electronics Engineering at Polytechnic Negeri Ujung Pandang. She research es electronic 
control, electronic instrumentation, and electronic digital system. She can be contacted at 
email: fpangerang@gmail.com . 
 
