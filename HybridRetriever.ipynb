{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting retriv\n",
      "  Downloading retriv-0.2.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from retriv) (1.26.3)\n",
      "Requirement already satisfied: nltk in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from retriv) (3.8.1)\n",
      "Collecting numba>=0.54.1 (from retriv)\n",
      "  Downloading numba-0.59.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tqdm in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from retriv) (4.66.1)\n",
      "Collecting optuna (from retriv)\n",
      "  Downloading optuna-3.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting krovetzstemmer (from retriv)\n",
      "  Downloading KrovetzStemmer-0.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pystemmer==2.0.1 (from retriv)\n",
      "  Downloading PyStemmer-2.0.1.tar.gz (559 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.3/559.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting unidecode (from retriv)\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: scikit-learn in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from retriv) (1.2.2)\n",
      "Collecting ranx (from retriv)\n",
      "  Downloading ranx-0.3.19-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting indxr (from retriv)\n",
      "  Downloading indxr-0.1.5-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oneliner-utils (from retriv)\n",
      "  Downloading oneliner_utils-0.1.2-py3-none-any.whl.metadata (656 bytes)\n",
      "Requirement already satisfied: torch in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from retriv) (2.2.1)\n",
      "Requirement already satisfied: torchvision in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from retriv) (0.17.1)\n",
      "Collecting torchaudio (from retriv)\n",
      "  Downloading torchaudio-2.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: transformers[torch] in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from retriv) (4.38.2)\n",
      "Requirement already satisfied: faiss-cpu in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from retriv) (1.8.0)\n",
      "Collecting autofaiss (from retriv)\n",
      "  Downloading autofaiss-2.17.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting multipipe (from retriv)\n",
      "  Downloading multipipe-0.1.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba>=0.54.1->retriv)\n",
      "  Downloading llvmlite-0.42.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting fire<0.6.0,>=0.4.0 (from autofaiss->retriv)\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas<3,>=1.1.5 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from autofaiss->retriv) (2.2.1)\n",
      "Requirement already satisfied: pyarrow<16,>=6.0.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from autofaiss->retriv) (15.0.0)\n",
      "Requirement already satisfied: fsspec>=2022.1.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from autofaiss->retriv) (2023.10.0)\n",
      "Collecting embedding-reader<2,>=1.5.1 (from autofaiss->retriv)\n",
      "  Downloading embedding_reader-1.7.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: orjson in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from indxr->retriv) (3.9.15)\n",
      "Requirement already satisfied: multiprocess in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from multipipe->retriv) (0.70.16)\n",
      "Requirement already satisfied: click in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from nltk->retriv) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from nltk->retriv) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from nltk->retriv) (2023.12.25)\n",
      "Collecting lz4 (from oneliner-utils->retriv)\n",
      "  Downloading lz4-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna->retriv)\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna->retriv)\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from optuna->retriv) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from optuna->retriv) (2.0.27)\n",
      "Requirement already satisfied: PyYAML in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from optuna->retriv) (6.0.1)\n",
      "Requirement already satisfied: tabulate in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from ranx->retriv) (0.9.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from ranx->retriv) (1.12.0)\n",
      "Collecting ir-datasets (from ranx->retriv)\n",
      "  Downloading ir_datasets-0.5.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: rich in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from ranx->retriv) (13.7.1)\n",
      "Collecting cbor2 (from ranx->retriv)\n",
      "  Downloading cbor2-5.6.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting seaborn (from ranx->retriv)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting fastparquet (from ranx->retriv)\n",
      "  Downloading fastparquet-2024.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from scikit-learn->retriv) (3.2.0)\n",
      "Requirement already satisfied: filelock in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from torch->retriv) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from torch->retriv) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from torch->retriv) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from torch->retriv) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from torch->retriv) (3.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from torchvision->retriv) (10.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from transformers[torch]->retriv) (0.21.4)\n",
      "Requirement already satisfied: requests in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from transformers[torch]->retriv) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from transformers[torch]->retriv) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from transformers[torch]->retriv) (0.4.2)\n",
      "Collecting accelerate>=0.21.0 (from transformers[torch]->retriv)\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: psutil in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers[torch]->retriv) (5.9.8)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna->retriv)\n",
      "  Downloading Mako-1.3.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from fire<0.6.0,>=0.4.0->autofaiss->retriv) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from fire<0.6.0,>=0.4.0->autofaiss->retriv) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from pandas<3,>=1.1.5->autofaiss->retriv) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from pandas<3,>=1.1.5->autofaiss->retriv) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from pandas<3,>=1.1.5->autofaiss->retriv) (2024.1)\n",
      "Collecting cramjam>=2.3 (from fastparquet->ranx->retriv)\n",
      "  Downloading cramjam-2.8.2-cp311-cp311-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from ir-datasets->ranx->retriv) (4.12.3)\n",
      "Collecting inscriptis>=2.2.0 (from ir-datasets->ranx->retriv)\n",
      "  Downloading inscriptis-2.5.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from ir-datasets->ranx->retriv) (5.1.0)\n",
      "Collecting trec-car-tools>=2.5.4 (from ir-datasets->ranx->retriv)\n",
      "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
      "Collecting warc3-wet>=0.2.3 (from ir-datasets->ranx->retriv)\n",
      "  Downloading warc3_wet-0.2.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets->ranx->retriv)\n",
      "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting zlib-state>=0.1.3 (from ir-datasets->ranx->retriv)\n",
      "  Downloading zlib-state-0.1.6.tar.gz (9.5 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ijson>=3.1.3 (from ir-datasets->ranx->retriv)\n",
      "  Downloading ijson-3.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting pyautocorpus>=0.1.1 (from ir-datasets->ranx->retriv)\n",
      "  Downloading pyautocorpus-0.1.12-cp311-cp311-macosx_10_9_universal2.whl.metadata (1.6 kB)\n",
      "Collecting unlzw3>=0.2.1 (from ir-datasets->ranx->retriv)\n",
      "  Downloading unlzw3-0.2.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from requests->transformers[torch]->retriv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from requests->transformers[torch]->retriv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from requests->transformers[torch]->retriv) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from requests->transformers[torch]->retriv) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from jinja2->torch->retriv) (2.1.5)\n",
      "Requirement already satisfied: dill>=0.3.8 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from multiprocess->multipipe->retriv) (0.3.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from rich->ranx->retriv) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from rich->ranx->retriv) (2.17.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from seaborn->ranx->retriv) (3.8.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from sympy->torch->retriv) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from beautifulsoup4>=4.4.1->ir-datasets->ranx->retriv) (2.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->ranx->retriv) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx->retriv) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx->retriv) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx->retriv) (4.48.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx->retriv) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx->retriv) (3.1.1)\n",
      "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets->ranx->retriv)\n",
      "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading retriv-0.2.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.59.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading autofaiss-2.17.0-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading indxr-0.1.5-py3-none-any.whl (11 kB)\n",
      "Downloading multipipe-0.1.0-py3-none-any.whl (4.6 kB)\n",
      "Downloading oneliner_utils-0.1.2-py3-none-any.whl (8.7 kB)\n",
      "Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ranx-0.3.19-py3-none-any.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.2.1-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading embedding_reader-1.7.0-py3-none-any.whl (18 kB)\n",
      "Downloading llvmlite-0.42.0-cp311-cp311-macosx_11_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cbor2-5.6.2-cp311-cp311-macosx_11_0_arm64.whl (69 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading fastparquet-2024.2.0-cp311-cp311-macosx_11_0_arm64.whl (684 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m684.9/684.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ir_datasets-0.5.6-py3-none-any.whl (335 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.2/335.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.3.3-cp311-cp311-macosx_11_0_arm64.whl (212 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cramjam-2.8.2-cp311-cp311-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ijson-3.2.3-cp311-cp311-macosx_11_0_arm64.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading inscriptis-2.5.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyautocorpus-0.1.12-cp311-cp311-macosx_10_9_universal2.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Downloading unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
      "Downloading warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
      "Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pystemmer, krovetzstemmer, fire, warc3-wet-clueweb09, zlib-state, cbor\n",
      "  Building wheel for pystemmer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pystemmer: filename=PyStemmer-2.0.1-cp311-cp311-macosx_11_0_arm64.whl size=128556 sha256=241f869a98246955a6e4a4ea6a4e5e6634feed4db23473e00f8c6237369aa3f1\n",
      "  Stored in directory: /Users/rohitmalhotra/Library/Caches/pip/wheels/bb/91/be/e06c54464d33bb822dc7ccfde8cb70cd72e58855a18c8fcef9\n",
      "  Building wheel for krovetzstemmer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for krovetzstemmer: filename=KrovetzStemmer-0.8-cp311-cp311-macosx_11_0_arm64.whl size=153298 sha256=cc7a9c2d503223ceae213b31839730468e0eaca5ec0848b5925363111e64f51b\n",
      "  Stored in directory: /Users/rohitmalhotra/Library/Caches/pip/wheels/3a/c1/dd/0200a30b35de8aa1a7e25a5f59c75eb144058e23229597cade\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=5deecd1b10caf615705281076a627b0b2cf9d4adb2e9063a1739db5cbeae67b8\n",
      "  Stored in directory: /Users/rohitmalhotra/Library/Caches/pip/wheels/a7/ee/a5/19e91481be8bea594935d137578bfe77d6bf905e4595336f6b\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=30397637166ff1048a58cc293fa2a7ab1d2d07af0c7526d57873312b3534ed13\n",
      "  Stored in directory: /Users/rohitmalhotra/Library/Caches/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
      "  Building wheel for zlib-state (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zlib-state: filename=zlib_state-0.1.6-cp311-cp311-macosx_11_0_arm64.whl size=9615 sha256=abacdb2285f53c0cd9545d49f850f6a5d14cb0215955b69e9e2959b159a6afdb\n",
      "  Stored in directory: /Users/rohitmalhotra/Library/Caches/pip/wheels/ab/d9/3b/924df3472ffe001302489725b2cd3dadf4c351ea31c0cd1816\n",
      "  Building wheel for cbor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-macosx_11_0_arm64.whl size=19508 sha256=d0b9ee960a869bb6395352af941ef5ddd63becbfcc1cb3c999ca2cbfb6a76bc8\n",
      "  Stored in directory: /Users/rohitmalhotra/Library/Caches/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
      "Successfully built pystemmer krovetzstemmer fire warc3-wet-clueweb09 zlib-state cbor\n",
      "Installing collected packages: warc3-wet-clueweb09, warc3-wet, pystemmer, krovetzstemmer, ijson, cbor, zlib-state, unlzw3, unidecode, trec-car-tools, pyautocorpus, Mako, lz4, llvmlite, indxr, fire, cramjam, colorlog, cbor2, oneliner-utils, numba, multipipe, inscriptis, alembic, torchaudio, seaborn, optuna, ir-datasets, fastparquet, embedding-reader, accelerate, ranx, autofaiss, retriv\n",
      "Successfully installed Mako-1.3.2 accelerate-0.28.0 alembic-1.13.1 autofaiss-2.17.0 cbor-1.0.0 cbor2-5.6.2 colorlog-6.8.2 cramjam-2.8.2 embedding-reader-1.7.0 fastparquet-2024.2.0 fire-0.5.0 ijson-3.2.3 indxr-0.1.5 inscriptis-2.5.0 ir-datasets-0.5.6 krovetzstemmer-0.8 llvmlite-0.42.0 lz4-4.3.3 multipipe-0.1.0 numba-0.59.0 oneliner-utils-0.1.2 optuna-3.5.0 pyautocorpus-0.1.12 pystemmer-2.0.1 ranx-0.3.19 retriv-0.2.3 seaborn-0.13.2 torchaudio-2.2.1 trec-car-tools-2.6 unidecode-1.3.8 unlzw3-0.2.2 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install retriv\n",
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from retriv import HybridRetriever\n",
    "\n",
    "hr = HybridRetriever(\n",
    "    # Shared params ------------------------------------------------------------\n",
    "    index_name=\"new-index\",\n",
    "    # Sparse retriever params --------------------------------------------------\n",
    "    sr_model=\"bm25\",\n",
    "    min_df=1,\n",
    "    tokenizer=\"whitespace\",\n",
    "    stemmer=\"english\",\n",
    "    stopwords=\"english\",\n",
    "    do_lowercasing=True,\n",
    "    do_ampersand_normalization=True,\n",
    "    do_special_chars_normalization=True,\n",
    "    do_acronyms_normalization=True,\n",
    "    do_punctuation_removal=True,\n",
    "    # Dense retriever params ---------------------------------------------------\n",
    "    dr_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    normalize=True,\n",
    "    max_length=128,\n",
    "    use_ann=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('splitDocuments.pkl','rb') as f:\n",
    "  all_splits = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(flatten_extend(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(persist_directory=\"all-MiniLM-L6-v2DB\", embedding_function=embedding_function)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='(b) How do you interpret the term “promising”? (c) What do you think the research community prioritized when you started? (d) What was the scope of the work that your research group did? (e) What was your relationship to computing resources at the start of your career? (f) What did your software workflow look like when you first started doing research? (Prompt:\\n\\nWhat tools, frameworks, libraries did you use?)', metadata={'source': 'Web Scholar PDFs/1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4.pdf'}),\n",
       " Document(page_content='eassignei=TBegin(6)ei+1=...=ej=TIntermediate(7)ThepredictionofBIOtagsismodeledasamulti-classclassificationproblemwiththeobjectiveasLEE=EDhXi−logpθ(ei|[c,v,ρ])i(8)DocumentClassificationWeusetheembeddingofthestarting[CLS]tokenfordocumentclassifica-tion.ThelogitsarepredictedwithanMLPheadontopofthe[CLS]embedding.Letlbethecorrectclass,theobjectiveisLDC=EDh−logpθ(l|[c,v,ρ])i(9)BAdditionalRelatedWorksB.1DatasetsSmallerdocumentdatasetsTheFormUnder-standinginNoisyScannedDocuments(FUNSDdataset(Jaumeetal.,', metadata={'source': 'Web Scholar PDFs/8ccda6de0223bcd897d5dc0efc8f33222a899d0d.pdf'}),\n",
       " Document(page_content='tal.,2022).Ourmodel,METRO-T0,isaT0modelpretrainedwithM', metadata={'source': 'Web Scholar PDFs/38aaf8a29df6deeff0bf64cc835d242a25b10337.pdf'}),\n",
       " Document(page_content='4.1 RQ1 (number of tokens): do all languages convey the same information with the same number of tokens?', metadata={'source': 'Web Scholar PDFs/17fbffb05fa14e21d1c506fd5f0f568b955fe983.pdf'}),\n",
       " Document(page_content='(e) What would you define as the start of your NLP research career? (e.g. start of PhD, research as\\n\\nan undergrad, etc). (Prompt: When was this?)\\n\\n2. I’d like to hear your thoughts on what the field was like near the beginning of your career.\\n\\n(a) When you started in your field, what did people generally think were the most promising\\n\\ndirections? (Prompt: do you agree?)', metadata={'source': 'Web Scholar PDFs/1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4.pdf'}),\n",
       " Document(page_content='Table 2 Decision rules from the literature.\\n\\nAuthor\\n\\nRule\\n\\nMCC\\n\\nCarnero Rosell et al. (2019)\\n\\n(𝑖 − 𝑧) > 1.2, (𝑧 − 𝑌 ) > 0.15, (𝑌𝐴𝐵 − 𝐽𝑉 𝑒𝑔𝑎) > 1.6, z< 22\\n\\n0.935\\n\\nBurningham et al. (2013)\\n\\n(𝑧 − 𝐽 )𝑉 𝑒𝑔𝑎 > 2.5, 𝐽 < 17.5\\n\\n0.921\\n\\n\\x00\\x1b\\x00\\x15\\x00\\x18\\x00\\x14\\x00\\x18\\x00\\x17\\x00\\x13\\x00\\x1b\\x00\\x14\\x00\\x16\\n\\n\\x00\\x19\\x00\\x13\\x00\\x13\\n\\n\\x00\\x1b\\x00\\x13\\x00\\x13\\n\\n\\x00)\\x00D\\x00O\\x00V\\x00H\\x007\\x00U\\x00X\\x00H\\x003\\x00U\\x00H\\x00G\\x00L\\x00F\\x00W\\x00H\\x00G\\x00\\x03\\x00&\\x00O\\x00D\\x00V\\x00V\\x00)\\x00D\\x00O\\x00V\\x00H\\x007\\x00U\\x00X\\x00H\\x00$\\x00F\\x00W\\x00X\\x00D\\x00O\\x00\\x03\\x00&\\x00O\\x00D\\x00V\\x00V\\x00\\x03\\n\\n\\x00\\x17\\x00\\x13\\x00\\x13\\n\\n\\x00\\x18\\x00\\x13\\x00\\x13\\n\\n\\x00\\x15\\x00\\x13\\x00\\x13\\n\\n\\x00\\x14\\x00\\x13\\x00\\x13\\n\\n\\x00\\x1a\\x00\\x13\\x00\\x13\\n\\n\\x00\\x16\\x00\\x13\\x00\\x13\\n\\n2.5\\n\\n1\\n\\n1\\n\\n2i [mag]\\n\\n10.0\\n\\n2\\n\\n5.0\\n\\nBD\\n\\n0.0\\n\\nother classes\\n\\n2.5\\n\\n3\\n\\n0\\n\\n5.0z-y\\n\\n7.5\\n\\n\\x00\\x17\\x00\\x13\\x00\\x13', metadata={'source': 'Web Scholar PDFs/8fe90d2b761248689659bad9cf61c65c53568cc9.pdf'}),\n",
       " Document(page_content='in Epi-2.', metadata={'source': 'Web Scholar PDFs/e0401ca2d4fd6d0ed55130a4a24b33ed90111479.pdf'}),\n",
       " Document(page_content='Second-order logic is the union of all classes of the second-order hierarchy, and similarly local second-order logic is the union of all classes of the local second-order hierarchy. The classes Σfo 1 will be referred to as the the existential fragments of second-order logic and local second-order logic, respectively.\\n\\n1 and Σlfo', metadata={'source': 'Web Scholar PDFs/ce2e66604a71390a3f8759bb57da68ac66e13280.pdf'})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retriever.get_relevant_documents(\"What number do all of the LTI classes start with\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Andrew Carnegie\\n\\nA self-educated \"working boy\" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world\\'s largest steel producing company by the end of the 19th century.', metadata={'source': 'Data/history_of_cmu/01.txt'}),\n",
       " Document(page_content='engineer students to take courses in humanities and social sciences in order to better understand the needs of society. Carnegie died in 1919, but his vision for an educated public lived on after him.', metadata={'source': 'Data/history_of_cmu/03.txt'}),\n",
       " Document(page_content='Carnegie Tech\\n\\nPost\\n\\nwar Years', metadata={'source': 'Data/history_of_cmu/04.txt'}),\n",
       " Document(page_content='At one point the richest man in the world, Carnegie believed that \"to die rich is to die disgraced.\" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed. In 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance', metadata={'source': 'Data/history_of_cmu/02.txt'})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Andrew Carnegie known for?\"\n",
    "vectorstore.similarity_search(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting small-text\n",
      "  Downloading small_text-1.3.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dill in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (0.3.8)\n",
      "Requirement already satisfied: scipy in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (1.2.2)\n",
      "Requirement already satisfied: tqdm in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (4.66.1)\n",
      "Requirement already satisfied: packaging in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from small-text) (23.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from scikit-learn>=0.24.1->small-text) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages (from scikit-learn>=0.24.1->small-text) (3.2.0)\n",
      "Downloading small_text-1.3.3-py3-none-any.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: small-text\n",
      "Successfully installed small-text-1.3.3\n"
     ]
    }
   ],
   "source": [
    "# 1. test hybrid retriever\n",
    "# 2. llm-embedder + bge reranker\n",
    "# 3. filco context filtering\n",
    "\n",
    "\n",
    "!pip install small-text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from small_text import TransformersDataset\n",
    "\n",
    "transformer_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    transformer_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"FINALQUESTIONS.txt\")\n",
    "targets = f.readlines()\n",
    "target = [i.strip() for i in targets]\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open(\"types.txt\")\n",
    "labels = f.readlines()\n",
    "labels = [i.strip() for i in labels]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Academic Calendar',\n",
       " 'Buggy News',\n",
       " 'Faculty',\n",
       " 'History of CMU',\n",
       " 'History of Drama',\n",
       " 'History of SCS',\n",
       " 'Kiltie Band',\n",
       " 'Programs',\n",
       " 'Research',\n",
       " 'Tartan Facts'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTargets = []\n",
    "finalLabels = []\n",
    "\n",
    "for count, i in enumerate(labels):\n",
    "    if \"kiltie\" in targets[count].lower() or i == \"Kiltie Band\":\n",
    "    #     finalLabels.append(\"kiltie\")\n",
    "\n",
    "        continue\n",
    "    \n",
    "\n",
    "    if i == \"History of CMU\" or i == \"History of Drama\" or i == \"History of SCS\":\n",
    "        finalLabels.append(0)\n",
    "\n",
    "    if i == \"Programs\" or i == \"Faculty\":\n",
    "        finalLabels.append(1)\n",
    "\n",
    "    if i == \"Academic Calendar\":\n",
    "        finalLabels.append(2)\n",
    "\n",
    "    if i == \"Research\":\n",
    "        finalLabels.append(3)\n",
    "\n",
    "    if i == \"Tartan Facts\" or \"Buggy News\":\n",
    "        finalLabels.append(4)\n",
    "\n",
    "    finalTargets.append(targets[count])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohitmalhotra/miniconda3/lib/python3.11/site-packages/small_text/utils/annotations.py:67: ExperimentalWarning: The function from_arrays is experimental and maybe subject to change soon.\n",
      "  warnings.warn(f'The {subject} {func_or_class.__name__} is experimental '\n"
     ]
    }
   ],
   "source": [
    "from small_text import TransformersDataset\n",
    "import numpy as np\n",
    "\n",
    "# target_labels = np.arange(num_classes)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "transformer_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    transformer_model_name\n",
    ")\n",
    "\n",
    "train = TransformersDataset.from_arrays(finalTargets,\n",
    "                                        finalLabels,\n",
    "                                        tokenizer,\n",
    "                                        max_length=60,\n",
    "                                        target_labels=len(set(finalTargets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409fea82d1204540b40616afcb267ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411747df27d8401d852c5a5499d2c4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d134619ac55945b0acfa13ab71a52a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc225abed4ce49c2aef39258be04786e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0013dc420206405fa4f08325fa9a7318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from small_text import (\n",
    "    PoolBasedActiveLearner,\n",
    "    PredictionEntropy,\n",
    "    TransformerBasedClassificationFactory,\n",
    "    TransformerModelArguments,\n",
    "    random_initialization_balanced\n",
    ")\n",
    "\n",
    "\n",
    "# simulates an initial labeling to warm-start the active learning process\n",
    "def initialize_active_learner(active_learner, y_train):\n",
    "\n",
    "    indices_initial = random_initialization_balanced(y_train, n_samples=20)\n",
    "    active_learner.initialize_data(indices_initial, y_train[indices_initial])\n",
    "\n",
    "    return indices_initial\n",
    "\n",
    "\n",
    "transformer_model = TransformerModelArguments(transformer_model_name)\n",
    "clf_factory = TransformerBasedClassificationFactory(transformer_model, \n",
    "                                                    len(set(finalTargets)), \n",
    "                                                    kwargs=dict({'device': 'cpu', \n",
    "                                                                 'mini_batch_size': 32,\n",
    "                                                                 'class_weight': 'balanced'\n",
    "                                                                }))\n",
    "query_strategy = PredictionEntropy()\n",
    "\n",
    "active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train)\n",
    "indices_labeled = initialize_active_learner(active_learner, train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Iteration #0 (40 samples)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39my[indices_queried]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Return the labels for the current query to the active learner.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mactive_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m indices_labeled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([indices_queried, indices_labeled])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/active_learner.py:243\u001b[0m, in \u001b[0;36mPoolBasedActiveLearner.update\u001b[0;34m(self, y, indices_validation)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignored\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m concatenate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, y)\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices_validation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_queried \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/active_learner.py:393\u001b[0m, in \u001b[0;36mPoolBasedActiveLearner._retrain\u001b[0;34m(self, indices_validation)\u001b[0m\n\u001b[1;32m    390\u001b[0m dataset\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices_validation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_labeled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:378\u001b[0m, in \u001b[0;36mTransformerBasedClassification.fit\u001b[0;34m(self, train_set, validation_set, weights, early_stopping, model_selection, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_class_weights(sub_train)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_default_criterion(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights_,\n\u001b[1;32m    376\u001b[0m                                              use_sample_weights\u001b[38;5;241m=\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_train_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_scheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:401\u001b[0m, in \u001b[0;36mTransformerBasedClassification._fit_main\u001b[0;34m(self, sub_train, sub_valid, weights, early_stopping, model_selection, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mget_tmp_dir_base()) \u001b[38;5;28;01mas\u001b[39;00m tmp_dir:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_model_selection(optimizer, model_selection)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:432\u001b[0m, in \u001b[0;36mTransformerBasedClassification._train\u001b[0;34m(self, sub_train, sub_valid, weights, early_stopping, model_selection, optimizer, scheduler, tmp_dir)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop:\n\u001b[1;32m    430\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m--> 432\u001b[0m     train_acc, train_loss, valid_acc, valid_loss, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43msub_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43msub_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     timedelta \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_epoch(epoch, timedelta, sub_train, sub_valid, train_acc, train_loss,\n\u001b[1;32m    445\u001b[0m                     valid_acc, valid_loss)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:468\u001b[0m, in \u001b[0;36mTransformerBasedClassification._train_loop_epoch\u001b[0;34m(self, num_epoch, sub_train, sub_valid, weights, early_stopping, model_selection, optimizer, scheduler, tmp_dir)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     validate_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m train_loss, train_acc, valid_loss, valid_acc, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop_process_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_acc, train_loss, valid_acc, valid_loss, stop\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:502\u001b[0m, in \u001b[0;36mTransformerBasedClassification._train_loop_process_batches\u001b[0;34m(self, num_epoch, sub_train_, sub_valid_, weights, early_stopping, model_selection, optimizer, scheduler, tmp_dir, validate_every)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, masks, \u001b[38;5;28mcls\u001b[39m, weight, \u001b[38;5;241m*\u001b[39m_) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_iter):\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop:\n\u001b[0;32m--> 502\u001b[0m         loss, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    505\u001b[0m         train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:560\u001b[0m, in \u001b[0;36mTransformerBasedClassification._train_single_batch\u001b[0;34m(self, x, masks, cls, weight, optimizer)\u001b[0m\n\u001b[1;32m    557\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m weight\n\u001b[1;32m    558\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 560\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    564\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # ...where each iteration consists of labelling 20 samples\n",
    "    indices_queried = active_learner.query(num_samples=20)\n",
    "\n",
    "    # Simulate user interaction here. Replace this for real-world usage.\n",
    "    y = train.y[indices_queried]\n",
    "\n",
    "    # Return the labels for the current query to the active learner.\n",
    "    active_learner.update(y)\n",
    "\n",
    "    indices_labeled = np.concatenate([indices_queried, indices_labeled])\n",
    "    \n",
    "    print('---------------')\n",
    "    print(f'Iteration #{i} ({len(indices_labeled)} samples)')\n",
    "    # results.append(evaluate(active_learner, train[indices_labeled], test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
