{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline\n",
    "\n",
    "1. gpt4all embeddings\n",
    "2. llama2; 7b params; 6 bits quanitization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorDB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip3: command not found\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install --upgrade --quiet  langchain langchain-community langchainhub gpt4all chromadb \n",
    "# !pip3 install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohitmalhotra/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "# data = loader.load()\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "# all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevea/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:10<00:00, 23.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "model_name = \"Salesforce/SFR-Embedding-Mistral\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "\n",
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"Salesforce/SFR-Embedding-Mistral\")\n",
    "\n",
    "\n",
    "# vectorstore = Chroma(persist_directory=\"gpt4allDB\", embedding_function=GPT4AllEmbeddings())\n",
    "# vectorstore = Chroma(persist_directory=\"all-MiniLM-L6-v2DB\", embedding_function=embedding_function)\n",
    "vectorstore = Chroma(persist_directory=\"sfr-mistralDB/\", embedding_function=embedding_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VectorDB Search Test\n",
    "\n",
    "question = \"What is buggy?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.21.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement typing-extensions~=3.7.4, but you'll have typing-extensions 4.7.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement wrapt~=1.12.1, but you'll have wrapt 1.11.2 which is incompatible.\u001b[0m\n",
      "--2024-03-13 15:58:09--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf\n",
      "Resolving huggingface.co (huggingface.co)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.163.189.37, 3.163.189.74, 3.163.189.90, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.163.189.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/cedd825c615575bcd58eb97cae65ec972afd17b00129f361b6bc3f3633dcd01d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q6_K.gguf%3B+filename%3D%22llama-2-7b-chat.Q6_K.gguf%22%3B&Expires=1710626290&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMDYyNjI5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlL2NlZGQ4MjVjNjE1NTc1YmNkNThlYjk3Y2FlNjVlYzk3MmFmZDE3YjAwMTI5ZjM2MWI2YmMzZjM2MzNkY2QwMWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Cjp0r3QftCon1IN6sVNj9OEedAStEut0iNgX35QnttRNxMmNlQpHdAchiKZiydeDbeyaK2A5nD1h2npeXYSBU6tMiXFx2Rb2eoQXM5eYPc5vbEsoLYVnY3CuideWgoOGj-K78kiqQMZtaiQd0rRY0RGiY1E5qvItDsmBmnQaUmi6-MXPL4hqw2Yp-BTQaN2qyLLiOF2vg%7EDVe4mcBCQOGFCTqdAPZsjxW1eLj9Gi4rgWCwlX6vwlDVnfF8f59XUr2%7EYDkijDAWCqcQUQTC7KxQKpPeamYBPVRO8HyhFJw5FJOlzqRePv8V7vmARt0RG-LAYxNn8EPIEZhOPpbM31kQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-03-13 15:58:10--  https://cdn-lfs.huggingface.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/cedd825c615575bcd58eb97cae65ec972afd17b00129f361b6bc3f3633dcd01d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q6_K.gguf%3B+filename%3D%22llama-2-7b-chat.Q6_K.gguf%22%3B&Expires=1710626290&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMDYyNjI5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlL2NlZGQ4MjVjNjE1NTc1YmNkNThlYjk3Y2FlNjVlYzk3MmFmZDE3YjAwMTI5ZjM2MWI2YmMzZjM2MzNkY2QwMWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Cjp0r3QftCon1IN6sVNj9OEedAStEut0iNgX35QnttRNxMmNlQpHdAchiKZiydeDbeyaK2A5nD1h2npeXYSBU6tMiXFx2Rb2eoQXM5eYPc5vbEsoLYVnY3CuideWgoOGj-K78kiqQMZtaiQd0rRY0RGiY1E5qvItDsmBmnQaUmi6-MXPL4hqw2Yp-BTQaN2qyLLiOF2vg%7EDVe4mcBCQOGFCTqdAPZsjxW1eLj9Gi4rgWCwlX6vwlDVnfF8f59XUr2%7EYDkijDAWCqcQUQTC7KxQKpPeamYBPVRO8HyhFJw5FJOlzqRePv8V7vmARt0RG-LAYxNn8EPIEZhOPpbM31kQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.23, 108.138.94.122, 108.138.94.14, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.23|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5529194176 (5.1G) [binary/octet-stream]\n",
      "Saving to: ‘llama-2-7b-chat.Q6_K.gguf’\n",
      "\n",
      "llama-2-7b-chat.Q6_ 100%[===================>]   5.15G  38.7MB/s    in 2m 4s   \n",
      "\n",
      "2024-03-13 16:00:15 (42.4 MB/s) - ‘llama-2-7b-chat.Q6_K.gguf’ saved [5529194176/5529194176]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet  llama-cpp-python\n",
    "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5272.34 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# rag_prompt.messages\n",
    "prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use one sentence maximum and keep the answer CONCISE. Keep the answer CONCISE.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))\n",
    "rag_prompt.messages = [prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"SubmissionData/test/questions.txt\", \"r\")\n",
    "questions = f.readlines()\n",
    "f.close()\n",
    "\n",
    "questions = [i.strip() for i in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      12.89 ms /    79 runs   (    0.16 ms per token,  6128.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7679.10 ms /   504 tokens (   15.24 ms per token,    65.63 tokens per second)\n",
      "llama_print_timings:        eval time =   13675.96 ms /    78 runs   (  175.33 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   21570.50 ms /   582 tokens\n",
      "  1%|▏         | 1/79 [00:22<28:44, 22.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       9.91 ms /    63 runs   (    0.16 ms per token,  6357.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5915.81 ms /   371 tokens (   15.95 ms per token,    62.71 tokens per second)\n",
      "llama_print_timings:        eval time =   10732.68 ms /    62 runs   (  173.11 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   16815.42 ms /   433 tokens\n",
      "  3%|▎         | 2/79 [00:39<24:27, 19.05s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       5.05 ms /    33 runs   (    0.15 ms per token,  6537.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5473.68 ms /   363 tokens (   15.08 ms per token,    66.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5406.86 ms /    32 runs   (  168.96 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:       total time =   10966.68 ms /   395 tokens\n",
      "  4%|▍         | 3/79 [00:50<19:30, 15.40s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       6.71 ms /    43 runs   (    0.16 ms per token,  6407.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5878.86 ms /   392 tokens (   15.00 ms per token,    66.68 tokens per second)\n",
      "llama_print_timings:        eval time =    7200.89 ms /    42 runs   (  171.45 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   13195.16 ms /   434 tokens\n",
      "  5%|▌         | 4/79 [01:03<18:12, 14.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      21.29 ms /   130 runs   (    0.16 ms per token,  6105.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8565.87 ms /   527 tokens (   16.25 ms per token,    61.52 tokens per second)\n",
      "llama_print_timings:        eval time =   22744.05 ms /   129 runs   (  176.31 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   31683.11 ms /   656 tokens\n",
      "  6%|▋         | 5/79 [01:35<25:38, 20.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.16 ms /    20 runs   (    0.16 ms per token,  6335.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4384.37 ms /   270 tokens (   16.24 ms per token,    61.58 tokens per second)\n",
      "llama_print_timings:        eval time =    3110.71 ms /    19 runs   (  163.72 ms per token,     6.11 tokens per second)\n",
      "llama_print_timings:       total time =    7549.27 ms /   289 tokens\n",
      "  8%|▊         | 6/79 [01:42<19:50, 16.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       5.90 ms /    36 runs   (    0.16 ms per token,  6105.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9472.52 ms /   577 tokens (   16.42 ms per token,    60.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6417.79 ms /    35 runs   (  183.37 ms per token,     5.45 tokens per second)\n",
      "llama_print_timings:       total time =   15993.03 ms /   612 tokens\n",
      "  9%|▉         | 7/79 [01:58<19:30, 16.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       5.32 ms /    31 runs   (    0.17 ms per token,  5822.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7316.62 ms /   461 tokens (   15.87 ms per token,    63.01 tokens per second)\n",
      "llama_print_timings:        eval time =    5302.19 ms /    30 runs   (  176.74 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   12711.63 ms /   491 tokens\n",
      " 10%|█         | 8/79 [02:11<17:56, 15.17s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.48 ms /    22 runs   (    0.16 ms per token,  6325.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6156.87 ms /   400 tokens (   15.39 ms per token,    64.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3668.88 ms /    21 runs   (  174.71 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =    9887.51 ms /   421 tokens\n",
      " 11%|█▏        | 9/79 [02:21<15:48, 13.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       6.22 ms /    36 runs   (    0.17 ms per token,  5786.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7892.37 ms /   443 tokens (   17.82 ms per token,    56.13 tokens per second)\n",
      "llama_print_timings:        eval time =    6094.53 ms /    35 runs   (  174.13 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   14090.19 ms /   478 tokens\n",
      " 13%|█▎        | 10/79 [02:36<15:49, 13.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.85 ms /    23 runs   (    0.17 ms per token,  5980.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9451.17 ms /   556 tokens (   17.00 ms per token,    58.83 tokens per second)\n",
      "llama_print_timings:        eval time =    3990.44 ms /    22 runs   (  181.38 ms per token,     5.51 tokens per second)\n",
      "llama_print_timings:       total time =   13512.18 ms /   578 tokens\n",
      " 14%|█▍        | 11/79 [02:49<15:32, 13.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.04 ms /    24 runs   (    0.17 ms per token,  5940.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3905.94 ms /   232 tokens (   16.84 ms per token,    59.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4216.72 ms /    23 runs   (  183.34 ms per token,     5.45 tokens per second)\n",
      "llama_print_timings:       total time =    8193.05 ms /   255 tokens\n",
      " 15%|█▌        | 12/79 [02:57<13:28, 12.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       8.88 ms /    55 runs   (    0.16 ms per token,  6193.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6808.36 ms /   455 tokens (   14.96 ms per token,    66.83 tokens per second)\n",
      "llama_print_timings:        eval time =    9199.33 ms /    54 runs   (  170.36 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =   16162.27 ms /   509 tokens\n",
      " 16%|█▋        | 13/79 [03:14<14:40, 13.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       9.10 ms /    58 runs   (    0.16 ms per token,  6374.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3039.11 ms /   146 tokens (   20.82 ms per token,    48.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9860.30 ms /    57 runs   (  172.99 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   13057.78 ms /   203 tokens\n",
      " 18%|█▊        | 14/79 [03:27<14:25, 13.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      12.32 ms /    79 runs   (    0.16 ms per token,  6410.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7677.03 ms /   505 tokens (   15.20 ms per token,    65.78 tokens per second)\n",
      "llama_print_timings:        eval time =   13782.76 ms /    78 runs   (  176.70 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   21688.62 ms /   583 tokens\n",
      " 19%|█▉        | 15/79 [03:49<16:56, 15.89s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.88 ms /    25 runs   (    0.16 ms per token,  6448.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4594.21 ms /   290 tokens (   15.84 ms per token,    63.12 tokens per second)\n",
      "llama_print_timings:        eval time =    4339.57 ms /    24 runs   (  180.82 ms per token,     5.53 tokens per second)\n",
      "llama_print_timings:       total time =    9003.75 ms /   314 tokens\n",
      " 20%|██        | 16/79 [03:58<14:32, 13.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       7.21 ms /    46 runs   (    0.16 ms per token,  6384.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4266.23 ms /   261 tokens (   16.35 ms per token,    61.18 tokens per second)\n",
      "llama_print_timings:        eval time =    7958.89 ms /    45 runs   (  176.86 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   12358.70 ms /   306 tokens\n",
      " 22%|██▏       | 17/79 [04:10<13:52, 13.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.05 ms /    26 runs   (    0.16 ms per token,  6421.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5854.14 ms /   385 tokens (   15.21 ms per token,    65.77 tokens per second)\n",
      "llama_print_timings:        eval time =    4416.90 ms /    25 runs   (  176.68 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   10346.55 ms /   410 tokens\n",
      " 23%|██▎       | 18/79 [04:21<12:45, 12.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       8.87 ms /    58 runs   (    0.15 ms per token,  6537.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5649.89 ms /   321 tokens (   17.60 ms per token,    56.82 tokens per second)\n",
      "llama_print_timings:        eval time =    9685.76 ms /    57 runs   (  169.93 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   15497.70 ms /   378 tokens\n",
      " 24%|██▍       | 19/79 [04:36<13:27, 13.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       7.31 ms /    47 runs   (    0.16 ms per token,  6428.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6826.17 ms /   445 tokens (   15.34 ms per token,    65.19 tokens per second)\n",
      "llama_print_timings:        eval time =    7948.72 ms /    46 runs   (  172.80 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   14908.96 ms /   491 tokens\n",
      " 25%|██▌       | 20/79 [04:51<13:41, 13.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.07 ms /    20 runs   (    0.15 ms per token,  6506.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8592.68 ms /   538 tokens (   15.97 ms per token,    62.61 tokens per second)\n",
      "llama_print_timings:        eval time =    3318.96 ms /    19 runs   (  174.68 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   11971.95 ms /   557 tokens\n",
      " 27%|██▋       | 21/79 [05:04<12:55, 13.37s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.33 ms /    21 runs   (    0.16 ms per token,  6311.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4496.59 ms /   261 tokens (   17.23 ms per token,    58.04 tokens per second)\n",
      "llama_print_timings:        eval time =    3393.91 ms /    20 runs   (  169.70 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =    7951.05 ms /   281 tokens\n",
      " 28%|██▊       | 22/79 [05:12<11:11, 11.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      12.14 ms /    78 runs   (    0.16 ms per token,  6424.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2269.67 ms /   122 tokens (   18.60 ms per token,    53.75 tokens per second)\n",
      "llama_print_timings:        eval time =   12767.42 ms /    77 runs   (  165.81 ms per token,     6.03 tokens per second)\n",
      "llama_print_timings:       total time =   15259.58 ms /   199 tokens\n",
      " 29%|██▉       | 23/79 [05:27<11:59, 12.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      15.05 ms /    97 runs   (    0.16 ms per token,  6445.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3250.94 ms /   188 tokens (   17.29 ms per token,    57.83 tokens per second)\n",
      "llama_print_timings:        eval time =   16381.41 ms /    96 runs   (  170.64 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   19913.08 ms /   284 tokens\n",
      " 30%|███       | 24/79 [05:47<13:45, 15.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      14.98 ms /    97 runs   (    0.15 ms per token,  6476.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6502.17 ms /   445 tokens (   14.61 ms per token,    68.44 tokens per second)\n",
      "llama_print_timings:        eval time =   16737.52 ms /    96 runs   (  174.35 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =   23519.39 ms /   541 tokens\n",
      " 32%|███▏      | 25/79 [06:11<15:49, 17.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       2.79 ms /    18 runs   (    0.16 ms per token,  6442.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4092.65 ms /   251 tokens (   16.31 ms per token,    61.33 tokens per second)\n",
      "llama_print_timings:        eval time =    2869.13 ms /    17 runs   (  168.77 ms per token,     5.93 tokens per second)\n",
      "llama_print_timings:       total time =    7012.01 ms /   268 tokens\n",
      " 33%|███▎      | 26/79 [06:18<12:45, 14.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       6.26 ms /    36 runs   (    0.17 ms per token,  5750.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5219.81 ms /   342 tokens (   15.26 ms per token,    65.52 tokens per second)\n",
      "llama_print_timings:        eval time =    6118.97 ms /    35 runs   (  174.83 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =   11447.67 ms /   377 tokens\n",
      " 34%|███▍      | 27/79 [06:29<11:45, 13.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      38.15 ms /   239 runs   (    0.16 ms per token,  6264.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9268.93 ms /   571 tokens (   16.23 ms per token,    61.60 tokens per second)\n",
      "llama_print_timings:        eval time =   41263.96 ms /   238 runs   (  173.38 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   51286.39 ms /   809 tokens\n",
      " 35%|███▌      | 28/79 [07:21<21:10, 24.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    30 runs   (    0.15 ms per token,  6487.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7370.65 ms /   495 tokens (   14.89 ms per token,    67.16 tokens per second)\n",
      "llama_print_timings:        eval time =    5047.31 ms /    29 runs   (  174.05 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   12507.19 ms /   524 tokens\n",
      " 37%|███▋      | 29/79 [07:33<17:40, 21.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       2.31 ms /    15 runs   (    0.15 ms per token,  6485.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6176.59 ms /   396 tokens (   15.60 ms per token,    64.11 tokens per second)\n",
      "llama_print_timings:        eval time =    2323.58 ms /    14 runs   (  165.97 ms per token,     6.03 tokens per second)\n",
      "llama_print_timings:       total time =    8544.61 ms /   410 tokens\n",
      " 38%|███▊      | 30/79 [07:42<14:14, 17.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       0.32 ms /     2 runs   (    0.16 ms per token,  6309.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9423.12 ms /   578 tokens (   16.30 ms per token,    61.34 tokens per second)\n",
      "llama_print_timings:        eval time =     191.17 ms /     1 runs   (  191.17 ms per token,     5.23 tokens per second)\n",
      "llama_print_timings:       total time =    9623.25 ms /   579 tokens\n",
      " 39%|███▉      | 31/79 [07:52<12:06, 15.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      10.31 ms /    65 runs   (    0.16 ms per token,  6305.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7613.13 ms /   505 tokens (   15.08 ms per token,    66.33 tokens per second)\n",
      "llama_print_timings:        eval time =   11385.17 ms /    64 runs   (  177.89 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   19199.60 ms /   569 tokens\n",
      " 41%|████      | 32/79 [08:11<12:49, 16.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.79 ms /    24 runs   (    0.16 ms per token,  6334.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7622.79 ms /   486 tokens (   15.68 ms per token,    63.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4198.23 ms /    23 runs   (  182.53 ms per token,     5.48 tokens per second)\n",
      "llama_print_timings:       total time =   11893.79 ms /   509 tokens\n",
      " 42%|████▏     | 33/79 [08:23<11:34, 15.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       7.48 ms /    48 runs   (    0.16 ms per token,  6421.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3776.60 ms /   190 tokens (   19.88 ms per token,    50.31 tokens per second)\n",
      "llama_print_timings:        eval time =    7968.06 ms /    47 runs   (  169.53 ms per token,     5.90 tokens per second)\n",
      "llama_print_timings:       total time =   11889.73 ms /   237 tokens\n",
      " 43%|████▎     | 34/79 [08:35<10:37, 14.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.39 ms /    23 runs   (    0.19 ms per token,  5242.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5044.45 ms /   310 tokens (   16.27 ms per token,    61.45 tokens per second)\n",
      "llama_print_timings:        eval time =    3676.34 ms /    22 runs   (  167.11 ms per token,     5.98 tokens per second)\n",
      "llama_print_timings:       total time =    8798.27 ms /   332 tokens\n",
      " 44%|████▍     | 35/79 [08:44<09:13, 12.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.00 ms /    19 runs   (    0.16 ms per token,  6324.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6977.85 ms /   452 tokens (   15.44 ms per token,    64.78 tokens per second)\n",
      "llama_print_timings:        eval time =    3103.78 ms /    18 runs   (  172.43 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:       total time =   10144.45 ms /   470 tokens\n",
      " 46%|████▌     | 36/79 [08:54<08:30, 11.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    32 runs   (    0.15 ms per token,  6477.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8275.03 ms /   525 tokens (   15.76 ms per token,    63.44 tokens per second)\n",
      "llama_print_timings:        eval time =    5373.51 ms /    31 runs   (  173.34 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   13746.21 ms /   556 tokens\n",
      " 47%|████▋     | 37/79 [09:08<08:43, 12.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    30 runs   (    0.15 ms per token,  6454.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2719.99 ms /   139 tokens (   19.57 ms per token,    51.10 tokens per second)\n",
      "llama_print_timings:        eval time =    4925.34 ms /    29 runs   (  169.84 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =    7733.78 ms /   168 tokens\n",
      " 48%|████▊     | 38/79 [09:16<07:33, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.67 ms /    23 runs   (    0.16 ms per token,  6260.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4448.88 ms /   257 tokens (   17.31 ms per token,    57.77 tokens per second)\n",
      "llama_print_timings:        eval time =    3607.41 ms /    22 runs   (  163.97 ms per token,     6.10 tokens per second)\n",
      "llama_print_timings:       total time =    8125.04 ms /   279 tokens\n",
      " 49%|████▉     | 39/79 [09:24<06:49, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       0.95 ms /     6 runs   (    0.16 ms per token,  6302.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11161.95 ms /   699 tokens (   15.97 ms per token,    62.62 tokens per second)\n",
      "llama_print_timings:        eval time =     909.77 ms /     5 runs   (  181.95 ms per token,     5.50 tokens per second)\n",
      "llama_print_timings:       total time =   12092.35 ms /   704 tokens\n",
      " 51%|█████     | 40/79 [09:36<07:03, 10.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      10.01 ms /    64 runs   (    0.16 ms per token,  6396.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7302.77 ms /   454 tokens (   16.09 ms per token,    62.17 tokens per second)\n",
      "llama_print_timings:        eval time =   10754.04 ms /    63 runs   (  170.70 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   18251.73 ms /   517 tokens\n",
      " 52%|█████▏    | 41/79 [09:55<08:18, 13.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.30 ms /    27 runs   (    0.16 ms per token,  6274.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4919.11 ms /   322 tokens (   15.28 ms per token,    65.46 tokens per second)\n",
      "llama_print_timings:        eval time =    4550.04 ms /    26 runs   (  175.00 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:       total time =    9552.96 ms /   348 tokens\n",
      " 53%|█████▎    | 42/79 [10:04<07:27, 12.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       1.11 ms /     7 runs   (    0.16 ms per token,  6300.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6391.20 ms /   415 tokens (   15.40 ms per token,    64.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1068.35 ms /     6 runs   (  178.06 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =    7481.09 ms /   421 tokens\n",
      " 54%|█████▍    | 43/79 [10:12<06:26, 10.73s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       8.08 ms /    47 runs   (    0.17 ms per token,  5813.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6109.41 ms /   409 tokens (   14.94 ms per token,    66.95 tokens per second)\n",
      "llama_print_timings:        eval time =    7793.19 ms /    46 runs   (  169.42 ms per token,     5.90 tokens per second)\n",
      "llama_print_timings:       total time =   14052.27 ms /   455 tokens\n",
      " 56%|█████▌    | 44/79 [10:26<06:51, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.79 ms /    29 runs   (    0.17 ms per token,  6055.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1611.40 ms /    69 tokens (   23.35 ms per token,    42.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4511.16 ms /    28 runs   (  161.11 ms per token,     6.21 tokens per second)\n",
      "llama_print_timings:       total time =    6209.92 ms /    97 tokens\n",
      " 57%|█████▋    | 45/79 [10:32<05:44, 10.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       3.69 ms /    24 runs   (    0.15 ms per token,  6511.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5625.28 ms /   360 tokens (   15.63 ms per token,    64.00 tokens per second)\n",
      "llama_print_timings:        eval time =    3787.13 ms /    23 runs   (  164.66 ms per token,     6.07 tokens per second)\n",
      "llama_print_timings:       total time =    9483.96 ms /   383 tokens\n",
      " 58%|█████▊    | 46/79 [10:42<05:29,  9.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       6.34 ms /    39 runs   (    0.16 ms per token,  6153.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6153.99 ms /   392 tokens (   15.70 ms per token,    63.70 tokens per second)\n",
      "llama_print_timings:        eval time =    6323.63 ms /    38 runs   (  166.41 ms per token,     6.01 tokens per second)\n",
      "llama_print_timings:       total time =   12599.24 ms /   430 tokens\n",
      " 59%|█████▉    | 47/79 [10:55<05:45, 10.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      22.55 ms /   142 runs   (    0.16 ms per token,  6296.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7648.58 ms /   493 tokens (   15.51 ms per token,    64.46 tokens per second)\n",
      "llama_print_timings:        eval time =   24404.36 ms /   141 runs   (  173.08 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   32507.45 ms /   634 tokens\n",
      " 61%|██████    | 48/79 [11:27<08:57, 17.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      15.69 ms /   101 runs   (    0.16 ms per token,  6436.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8759.74 ms /   535 tokens (   16.37 ms per token,    61.07 tokens per second)\n",
      "llama_print_timings:        eval time =   17328.17 ms /   100 runs   (  173.28 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   26405.86 ms /   635 tokens\n",
      " 62%|██████▏   | 49/79 [11:54<10:03, 20.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    42 runs   (    0.15 ms per token,  6508.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3518.19 ms /   208 tokens (   16.91 ms per token,    59.12 tokens per second)\n",
      "llama_print_timings:        eval time =    6970.05 ms /    41 runs   (  170.00 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =   10616.41 ms /   249 tokens\n",
      " 63%|██████▎   | 50/79 [12:05<08:21, 17.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       7.14 ms /    46 runs   (    0.16 ms per token,  6439.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5924.23 ms /   395 tokens (   15.00 ms per token,    66.68 tokens per second)\n",
      "llama_print_timings:        eval time =    7695.67 ms /    45 runs   (  171.01 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   13763.72 ms /   440 tokens\n",
      " 65%|██████▍   | 51/79 [12:19<07:35, 16.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       7.63 ms /    49 runs   (    0.16 ms per token,  6425.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4320.73 ms /   272 tokens (   15.89 ms per token,    62.95 tokens per second)\n",
      "llama_print_timings:        eval time =    8067.50 ms /    48 runs   (  168.07 ms per token,     5.95 tokens per second)\n",
      "llama_print_timings:       total time =   12539.24 ms /   320 tokens\n",
      " 66%|██████▌   | 52/79 [12:31<06:49, 15.17s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       7.05 ms /    45 runs   (    0.16 ms per token,  6380.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6636.10 ms /   445 tokens (   14.91 ms per token,    67.06 tokens per second)\n",
      "llama_print_timings:        eval time =    7616.08 ms /    44 runs   (  173.09 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   14395.30 ms /   489 tokens\n",
      " 67%|██████▋   | 53/79 [12:46<06:29, 14.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.67 ms /    30 runs   (    0.16 ms per token,  6428.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4718.88 ms /   307 tokens (   15.37 ms per token,    65.06 tokens per second)\n",
      "llama_print_timings:        eval time =    4870.03 ms /    29 runs   (  167.93 ms per token,     5.95 tokens per second)\n",
      "llama_print_timings:       total time =    9681.44 ms /   336 tokens\n",
      " 68%|██████▊   | 54/79 [12:56<05:35, 13.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       2.85 ms /    17 runs   (    0.17 ms per token,  5971.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3914.74 ms /   224 tokens (   17.48 ms per token,    57.22 tokens per second)\n",
      "llama_print_timings:        eval time =    2666.42 ms /    16 runs   (  166.65 ms per token,     6.00 tokens per second)\n",
      "llama_print_timings:       total time =    6634.82 ms /   240 tokens\n",
      " 70%|██████▉   | 55/79 [13:02<04:34, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.99 ms /    32 runs   (    0.16 ms per token,  6414.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6281.33 ms /   384 tokens (   16.36 ms per token,    61.13 tokens per second)\n",
      "llama_print_timings:        eval time =    5249.52 ms /    31 runs   (  169.34 ms per token,     5.91 tokens per second)\n",
      "llama_print_timings:       total time =   11631.56 ms /   415 tokens\n",
      " 71%|███████   | 56/79 [13:14<04:24, 11.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       9.81 ms /    62 runs   (    0.16 ms per token,  6319.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6446.42 ms /   392 tokens (   16.44 ms per token,    60.81 tokens per second)\n",
      "llama_print_timings:        eval time =   10410.68 ms /    61 runs   (  170.67 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   17052.75 ms /   453 tokens\n",
      " 72%|███████▏  | 57/79 [13:31<04:50, 13.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       0.65 ms /     4 runs   (    0.16 ms per token,  6172.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3306.17 ms /   169 tokens (   19.56 ms per token,    51.12 tokens per second)\n",
      "llama_print_timings:        eval time =     522.59 ms /     3 runs   (  174.20 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:       total time =    3840.88 ms /   172 tokens\n",
      " 73%|███████▎  | 58/79 [13:35<03:39, 10.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    38 runs   (    0.16 ms per token,  6335.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4597.71 ms /   287 tokens (   16.02 ms per token,    62.42 tokens per second)\n",
      "llama_print_timings:        eval time =    6636.68 ms /    37 runs   (  179.37 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   11352.88 ms /   324 tokens\n",
      " 75%|███████▍  | 59/79 [13:47<03:35, 10.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =      12.01 ms /    75 runs   (    0.16 ms per token,  6243.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9460.23 ms /   596 tokens (   15.87 ms per token,    63.00 tokens per second)\n",
      "llama_print_timings:        eval time =   13030.95 ms /    74 runs   (  176.09 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:       total time =   22732.87 ms /   670 tokens\n",
      " 76%|███████▌  | 60/79 [14:11<04:40, 14.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       7.93 ms /    46 runs   (    0.17 ms per token,  5797.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6502.66 ms /   422 tokens (   15.41 ms per token,    64.90 tokens per second)\n",
      "llama_print_timings:        eval time =    7771.22 ms /    45 runs   (  172.69 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   14427.47 ms /   467 tokens\n",
      " 77%|███████▋  | 61/79 [14:25<04:24, 14.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       9.88 ms /    63 runs   (    0.16 ms per token,  6379.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3486.60 ms /   203 tokens (   17.18 ms per token,    58.22 tokens per second)\n",
      "llama_print_timings:        eval time =   10275.04 ms /    62 runs   (  165.73 ms per token,     6.03 tokens per second)\n",
      "llama_print_timings:       total time =   13953.77 ms /   265 tokens\n",
      " 78%|███████▊  | 62/79 [14:39<04:06, 14.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       4.44 ms /    29 runs   (    0.15 ms per token,  6527.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9077.16 ms /   533 tokens (   17.03 ms per token,    58.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4819.20 ms /    28 runs   (  172.11 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   13987.81 ms /   561 tokens\n",
      " 80%|███████▉  | 63/79 [14:53<03:49, 14.37s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       1.73 ms /    11 runs   (    0.16 ms per token,  6373.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6258.50 ms /   411 tokens (   15.23 ms per token,    65.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1734.86 ms /    10 runs   (  173.49 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =    8026.18 ms /   421 tokens\n",
      " 81%|████████  | 64/79 [15:02<03:07, 12.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       5.54 ms /    36 runs   (    0.15 ms per token,  6497.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2244.88 ms /   114 tokens (   19.69 ms per token,    50.78 tokens per second)\n",
      "llama_print_timings:        eval time =    5772.29 ms /    35 runs   (  164.92 ms per token,     6.06 tokens per second)\n",
      "llama_print_timings:       total time =    8121.86 ms /   149 tokens\n",
      " 82%|████████▏ | 65/79 [15:10<02:37, 11.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7679.45 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    41 runs   (    0.15 ms per token,  6503.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9687.10 ms /   590 tokens (   16.42 ms per token,    60.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6951.55 ms /    40 runs   (  173.79 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   16764.15 ms /   630 tokens\n",
      " 84%|████████▎ | 66/79 [15:27<02:48, 12.93s/it]Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "answers = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "  response = qa_chain.invoke(questions[i])\n",
    "  answers.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeListToFile(filename, l):\n",
    "    f = open(filename, \"w\")\n",
    "    for i in l:\n",
    "        f.write(i + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n",
    "writeListToFile(\"SubmissionData/system_outputs/system_output_3.txt\", answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
