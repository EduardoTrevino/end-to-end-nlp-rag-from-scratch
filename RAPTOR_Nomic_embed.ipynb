{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NOMIC_API_KEY=nk-4H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.014556885,\n",
       " 0.042907715,\n",
       " -0.19152832,\n",
       " -0.026611328,\n",
       " 0.04434204,\n",
       " -0.021774292,\n",
       " 0.04977417,\n",
       " 0.025512695,\n",
       " 0.041381836,\n",
       " 0.0018663406,\n",
       " 0.022338867,\n",
       " 0.040771484,\n",
       " 0.055603027,\n",
       " -0.015525818,\n",
       " -0.028305054,\n",
       " 0.0072402954,\n",
       " 0.028625488,\n",
       " -0.09265137,\n",
       " -0.005065918,\n",
       " 0.006893158,\n",
       " -0.044281006,\n",
       " -0.06561279,\n",
       " -0.03036499,\n",
       " 0.066711426,\n",
       " 0.040863037,\n",
       " -0.016738892,\n",
       " -0.03265381,\n",
       " 0.018630981,\n",
       " -0.057159424,\n",
       " 0.044006348,\n",
       " 0.026428223,\n",
       " 0.0009665489,\n",
       " 0.008293152,\n",
       " -0.02734375,\n",
       " 0.0045661926,\n",
       " -0.004306793,\n",
       " 0.019805908,\n",
       " 0.044067383,\n",
       " 0.008934021,\n",
       " 0.032958984,\n",
       " 0.03186035,\n",
       " 0.0025367737,\n",
       " -0.0054473877,\n",
       " -0.065979004,\n",
       " 0.032409668,\n",
       " -0.023498535,\n",
       " 0.05596924,\n",
       " 0.015716553,\n",
       " 0.072509766,\n",
       " -0.044281006,\n",
       " -0.023040771,\n",
       " -0.0048294067,\n",
       " 0.027633667,\n",
       " -0.06201172,\n",
       " 0.076538086,\n",
       " 0.015930176,\n",
       " 0.0390625,\n",
       " 0.008224487,\n",
       " 0.013786316,\n",
       " 0.02178955,\n",
       " 0.12438965,\n",
       " 0.001045227,\n",
       " 0.013343811,\n",
       " 0.06173706,\n",
       " 0.0236969,\n",
       " -0.056121826,\n",
       " -0.033050537,\n",
       " 0.047058105,\n",
       " -0.026657104,\n",
       " -0.040130615,\n",
       " -0.0039787292,\n",
       " 0.024215698,\n",
       " 0.0057525635,\n",
       " 0.01713562,\n",
       " -0.010902405,\n",
       " -0.00082063675,\n",
       " -0.06555176,\n",
       " -0.015716553,\n",
       " 0.034606934,\n",
       " 0.011703491,\n",
       " 0.026321411,\n",
       " 5.2273273e-05,\n",
       " 0.03527832,\n",
       " -0.04650879,\n",
       " 0.008308411,\n",
       " 0.01361084,\n",
       " 0.02494812,\n",
       " -0.005207062,\n",
       " -0.028213501,\n",
       " 0.012786865,\n",
       " 0.011360168,\n",
       " 0.01713562,\n",
       " 0.040771484,\n",
       " 0.033233643,\n",
       " -0.034179688,\n",
       " 0.025344849,\n",
       " 0.039215088,\n",
       " -0.0022583008,\n",
       " -0.021148682,\n",
       " -0.047973633,\n",
       " -0.014701843,\n",
       " 0.009391785,\n",
       " -0.0143585205,\n",
       " -0.008979797,\n",
       " 0.08251953,\n",
       " 0.017349243,\n",
       " -0.0035800934,\n",
       " 0.014320374,\n",
       " -0.034851074,\n",
       " -0.02279663,\n",
       " -0.05001831,\n",
       " 0.056549072,\n",
       " 0.019180298,\n",
       " -0.020431519,\n",
       " -0.0052871704,\n",
       " 0.023971558,\n",
       " 0.09161377,\n",
       " -0.03527832,\n",
       " 0.00894165,\n",
       " 0.04373169,\n",
       " 0.006099701,\n",
       " 0.016494751,\n",
       " 0.009109497,\n",
       " 0.08105469,\n",
       " -0.0045700073,\n",
       " -0.023284912,\n",
       " -0.03189087,\n",
       " 0.06427002,\n",
       " -0.0008196831,\n",
       " 0.004299164,\n",
       " 0.00819397,\n",
       " -0.03338623,\n",
       " -0.020401001,\n",
       " 0.01953125,\n",
       " -0.004852295,\n",
       " 0.010002136,\n",
       " -0.029754639,\n",
       " -0.04827881,\n",
       " 0.0038146973,\n",
       " 0.005882263,\n",
       " 0.023468018,\n",
       " -0.008102417,\n",
       " 0.0040779114,\n",
       " -0.031982422,\n",
       " 0.041809082,\n",
       " -0.06921387,\n",
       " 0.005393982,\n",
       " -0.046020508,\n",
       " -0.0051612854,\n",
       " -0.017028809,\n",
       " -0.055358887,\n",
       " -0.041656494,\n",
       " -0.013244629,\n",
       " -0.04763794,\n",
       " 0.049560547,\n",
       " -0.06524658,\n",
       " -0.03048706,\n",
       " 0.010398865,\n",
       " 0.027267456,\n",
       " 0.034362793,\n",
       " 0.037017822,\n",
       " -0.038146973,\n",
       " -0.03869629,\n",
       " 0.08288574,\n",
       " -0.015060425,\n",
       " -0.06756592,\n",
       " 0.010215759,\n",
       " 0.008766174,\n",
       " 0.0044403076,\n",
       " 0.043640137,\n",
       " 0.0010852814,\n",
       " -0.046936035,\n",
       " 0.056549072,\n",
       " -0.08392334,\n",
       " -0.008636475,\n",
       " -0.037353516,\n",
       " 0.008346558,\n",
       " -0.032714844,\n",
       " 0.05206299,\n",
       " -0.06921387,\n",
       " 0.08319092,\n",
       " -0.06842041,\n",
       " 0.07647705,\n",
       " 0.048217773,\n",
       " -0.036865234,\n",
       " -0.040283203,\n",
       " 0.014717102,\n",
       " 0.019805908,\n",
       " 0.018249512,\n",
       " -0.012237549,\n",
       " -0.004638672,\n",
       " 0.0140686035,\n",
       " -0.06719971,\n",
       " 0.0009775162,\n",
       " 0.016113281,\n",
       " -0.023345947,\n",
       " 0.008598328,\n",
       " -0.012283325,\n",
       " -0.04714966,\n",
       " -0.018463135,\n",
       " -0.031555176,\n",
       " -0.047180176,\n",
       " -0.023742676,\n",
       " 0.00907135,\n",
       " -0.06542969,\n",
       " 0.052215576,\n",
       " 0.03857422,\n",
       " 0.057647705,\n",
       " 0.0035247803,\n",
       " 0.00299263,\n",
       " 0.028274536,\n",
       " 0.023620605,\n",
       " 0.011169434,\n",
       " -0.030822754,\n",
       " 0.0096206665,\n",
       " 0.013221741,\n",
       " -0.009429932,\n",
       " -0.03692627,\n",
       " -0.035369873,\n",
       " 0.036224365,\n",
       " 0.04257202,\n",
       " -0.015342712,\n",
       " -0.012512207,\n",
       " -0.0060195923,\n",
       " -0.012039185,\n",
       " -0.0017910004,\n",
       " -0.013786316,\n",
       " -0.026016235,\n",
       " -0.00076913834,\n",
       " 0.022613525,\n",
       " -0.01121521,\n",
       " -0.08404541,\n",
       " 0.05810547,\n",
       " 0.006790161,\n",
       " 0.020568848,\n",
       " 0.031311035,\n",
       " -0.014183044,\n",
       " 0.072631836,\n",
       " 0.008613586,\n",
       " -0.002407074,\n",
       " 0.0033245087,\n",
       " 0.03869629,\n",
       " 0.0046577454,\n",
       " -0.018051147,\n",
       " -0.042419434,\n",
       " 0.012382507,\n",
       " 0.005252838,\n",
       " 0.034332275,\n",
       " 0.014007568,\n",
       " 0.03048706,\n",
       " 0.005039215,\n",
       " 0.0060806274,\n",
       " -0.035247803,\n",
       " 0.023468018,\n",
       " 0.01007843,\n",
       " -0.037475586,\n",
       " -0.03652954,\n",
       " 0.058380127,\n",
       " 0.049957275,\n",
       " 3.170967e-05,\n",
       " 0.0096588135,\n",
       " -0.04840088,\n",
       " 0.0014486313,\n",
       " -0.047302246,\n",
       " -0.041931152,\n",
       " -2.4437904e-05,\n",
       " -0.070007324,\n",
       " -0.047454834,\n",
       " -0.020492554,\n",
       " -0.022567749,\n",
       " 0.0069084167,\n",
       " 0.054595947,\n",
       " 0.034423828,\n",
       " 0.011764526,\n",
       " -0.02180481,\n",
       " 0.028579712,\n",
       " -0.012283325,\n",
       " 0.019119263,\n",
       " 0.005001068,\n",
       " 0.057800293,\n",
       " -0.03024292,\n",
       " -0.036254883,\n",
       " -0.017456055,\n",
       " 0.023864746,\n",
       " -0.009864807,\n",
       " 0.03527832,\n",
       " 0.0012998581,\n",
       " 0.019836426,\n",
       " 0.055664062,\n",
       " -0.0010633469,\n",
       " 0.010215759,\n",
       " 0.03189087,\n",
       " -0.040405273,\n",
       " 0.056884766,\n",
       " 0.026321411,\n",
       " 0.018432617,\n",
       " 0.054351807,\n",
       " 0.00031471252,\n",
       " 0.018798828,\n",
       " 0.028533936,\n",
       " 0.021575928,\n",
       " 0.04940796,\n",
       " 0.022445679,\n",
       " 0.02684021,\n",
       " -0.01852417,\n",
       " -0.0637207,\n",
       " 0.04876709,\n",
       " -0.0029563904,\n",
       " 0.03942871,\n",
       " 0.020889282,\n",
       " -0.029541016,\n",
       " -0.055419922,\n",
       " -0.014556885,\n",
       " 0.001411438,\n",
       " -0.02684021,\n",
       " 0.026733398,\n",
       " 0.034942627,\n",
       " 0.051635742,\n",
       " 0.012161255,\n",
       " 0.027648926,\n",
       " -0.0085372925,\n",
       " -0.049468994,\n",
       " -0.040893555,\n",
       " -0.08306885,\n",
       " -0.017532349,\n",
       " 0.0013837814,\n",
       " -0.0018444061,\n",
       " 0.053466797,\n",
       " -0.03640747,\n",
       " -0.043304443,\n",
       " -0.019241333,\n",
       " 0.04675293,\n",
       " -0.022918701,\n",
       " -0.05050659,\n",
       " -0.0016479492,\n",
       " 0.058685303,\n",
       " 0.016082764,\n",
       " -0.011047363,\n",
       " -0.019119263,\n",
       " 0.02230835,\n",
       " 0.022262573,\n",
       " 0.009521484,\n",
       " -0.029388428,\n",
       " -0.033691406,\n",
       " -0.0051078796,\n",
       " -0.009902954,\n",
       " -0.04257202,\n",
       " 0.020339966,\n",
       " 0.02293396,\n",
       " 0.01751709,\n",
       " -0.054260254,\n",
       " 0.017745972,\n",
       " 0.00680542,\n",
       " 0.0021629333,\n",
       " 0.022628784,\n",
       " 0.008262634,\n",
       " 0.05102539,\n",
       " 0.0054740906,\n",
       " 0.027389526,\n",
       " 0.026687622,\n",
       " 0.028411865,\n",
       " 0.030654907,\n",
       " 0.012931824,\n",
       " -0.049560547,\n",
       " 0.0030403137,\n",
       " 0.04031372,\n",
       " -0.015052795,\n",
       " -0.013267517,\n",
       " 0.08728027,\n",
       " 0.021255493,\n",
       " -0.046325684,\n",
       " -0.012023926,\n",
       " 0.025817871,\n",
       " 0.05770874,\n",
       " -0.0236969,\n",
       " -0.013076782,\n",
       " -0.084228516,\n",
       " -0.047912598,\n",
       " 0.014083862,\n",
       " -0.012321472,\n",
       " -0.0035972595,\n",
       " -0.022537231,\n",
       " 0.01953125,\n",
       " 0.04953003,\n",
       " 0.0206604,\n",
       " 0.01612854,\n",
       " -0.010665894,\n",
       " -0.03302002,\n",
       " -0.023239136,\n",
       " 0.027862549,\n",
       " -0.0038967133,\n",
       " 0.012557983,\n",
       " -0.05206299,\n",
       " 0.011932373,\n",
       " -0.039764404,\n",
       " -0.03201294,\n",
       " 0.023071289,\n",
       " -0.003376007,\n",
       " 0.009338379,\n",
       " -0.00067329407,\n",
       " -0.016281128,\n",
       " -0.04925537,\n",
       " 0.0062446594,\n",
       " 0.0062446594,\n",
       " -0.054992676,\n",
       " 0.034729004,\n",
       " -0.027557373,\n",
       " -0.029663086,\n",
       " 0.060577393,\n",
       " -0.017425537,\n",
       " 0.014732361,\n",
       " 0.0209198,\n",
       " 0.022537231,\n",
       " 0.0079422,\n",
       " -0.006477356,\n",
       " 0.04498291,\n",
       " 0.01399231,\n",
       " -0.028060913,\n",
       " -0.020263672,\n",
       " -0.023742676,\n",
       " 0.057891846,\n",
       " 0.026031494,\n",
       " 0.0099105835,\n",
       " -0.014289856,\n",
       " 0.013748169,\n",
       " 0.004020691,\n",
       " -0.0060272217,\n",
       " 0.038116455,\n",
       " 0.031402588,\n",
       " -0.06585693,\n",
       " -0.008369446,\n",
       " 0.058441162,\n",
       " 0.016204834,\n",
       " 0.00970459,\n",
       " -0.029067993,\n",
       " 0.0463562,\n",
       " 0.05114746,\n",
       " -0.011932373,\n",
       " 0.024215698,\n",
       " 0.025665283,\n",
       " 0.010192871,\n",
       " -0.03302002,\n",
       " -0.05050659,\n",
       " -0.0005002022,\n",
       " -0.0043563843,\n",
       " 0.08117676,\n",
       " 0.013458252,\n",
       " 0.021469116,\n",
       " -0.006214142,\n",
       " -0.008262634,\n",
       " -0.05218506,\n",
       " 0.0055351257,\n",
       " 0.04525757,\n",
       " -0.0009775162,\n",
       " 0.066711426,\n",
       " -0.0637207,\n",
       " -0.006793976,\n",
       " -0.026672363,\n",
       " 0.048736572,\n",
       " 0.054748535,\n",
       " 0.03881836,\n",
       " 0.005970001,\n",
       " -0.010192871,\n",
       " 0.040008545,\n",
       " 0.024978638,\n",
       " -0.038970947,\n",
       " 0.017303467,\n",
       " -0.04144287,\n",
       " 0.019836426,\n",
       " 0.06021118,\n",
       " -0.026275635,\n",
       " 0.0032081604,\n",
       " 0.056884766,\n",
       " -0.062805176,\n",
       " 0.021987915,\n",
       " -0.0028953552,\n",
       " 0.032836914,\n",
       " -0.023483276,\n",
       " -0.04046631,\n",
       " 0.026245117,\n",
       " -0.03994751,\n",
       " 0.07287598,\n",
       " -0.04989624,\n",
       " -0.04559326,\n",
       " 0.015655518,\n",
       " -0.003944397,\n",
       " 0.07220459,\n",
       " -0.00440979,\n",
       " 0.017486572,\n",
       " 0.034698486,\n",
       " 0.05999756,\n",
       " 0.014564514,\n",
       " 0.030899048,\n",
       " -0.06768799,\n",
       " -0.032989502,\n",
       " -0.0077171326,\n",
       " -0.015464783,\n",
       " 0.04864502,\n",
       " 0.01889038,\n",
       " -0.03778076,\n",
       " 0.021530151,\n",
       " 0.0051612854,\n",
       " -0.028167725,\n",
       " 0.034942627,\n",
       " 0.0031280518,\n",
       " -0.013061523,\n",
       " 0.0028839111,\n",
       " -0.05319214,\n",
       " -0.030197144,\n",
       " -0.028396606,\n",
       " 0.008483887,\n",
       " 0.01373291,\n",
       " 0.036468506,\n",
       " 0.034301758,\n",
       " 0.013870239,\n",
       " -0.018508911,\n",
       " 0.0680542,\n",
       " 0.057159424,\n",
       " -0.04699707,\n",
       " 0.02444458,\n",
       " -0.0050811768,\n",
       " -0.04559326,\n",
       " -0.043395996,\n",
       " -0.0095825195,\n",
       " -0.032226562,\n",
       " 0.03869629,\n",
       " -0.0011196136,\n",
       " -0.0107421875,\n",
       " 0.095336914,\n",
       " -0.05267334,\n",
       " 0.01626587,\n",
       " 0.011779785,\n",
       " -0.03552246,\n",
       " -0.013435364,\n",
       " 0.003490448,\n",
       " 0.037109375,\n",
       " -0.010627747,\n",
       " 0.011627197,\n",
       " 0.0065841675,\n",
       " 0.007461548,\n",
       " -0.05831909,\n",
       " 0.0011425018,\n",
       " 0.00472641,\n",
       " -0.031982422,\n",
       " 0.057800293,\n",
       " -0.009422302,\n",
       " -0.02180481,\n",
       " -0.052856445,\n",
       " -0.007320404,\n",
       " -0.0007710457,\n",
       " 0.04373169,\n",
       " -0.1071167,\n",
       " -0.014503479,\n",
       " -0.019256592,\n",
       " 0.00016665459,\n",
       " -0.08068848,\n",
       " -0.011940002,\n",
       " 0.0053977966,\n",
       " -0.054260254,\n",
       " -0.03552246,\n",
       " 0.04083252,\n",
       " -0.013015747,\n",
       " 0.06329346,\n",
       " 0.0068473816,\n",
       " 0.041534424,\n",
       " -0.02407837,\n",
       " -0.025436401,\n",
       " 0.05819702,\n",
       " -0.011482239,\n",
       " -0.050201416,\n",
       " -0.03552246,\n",
       " -0.03338623,\n",
       " 0.023010254,\n",
       " -0.020690918,\n",
       " -0.032409668,\n",
       " 0.042053223,\n",
       " -0.0029335022,\n",
       " -0.008148193,\n",
       " -0.023712158,\n",
       " -0.050842285,\n",
       " 0.032714844,\n",
       " -0.045532227,\n",
       " 0.012191772,\n",
       " 0.05596924,\n",
       " 0.036865234,\n",
       " -0.010772705,\n",
       " -0.011497498,\n",
       " -0.03265381,\n",
       " 0.036010742,\n",
       " -0.05206299,\n",
       " -0.0680542,\n",
       " 0.014442444,\n",
       " 0.04849243,\n",
       " -0.00919342,\n",
       " 0.0018348694,\n",
       " 0.022033691,\n",
       " -0.04421997,\n",
       " -0.033843994,\n",
       " -0.023971558,\n",
       " -0.038391113,\n",
       " -0.010871887,\n",
       " -0.013648987,\n",
       " 0.038085938,\n",
       " -0.02331543,\n",
       " 0.007835388,\n",
       " 0.0413208,\n",
       " -0.025177002,\n",
       " 0.027145386,\n",
       " -0.028579712,\n",
       " -0.032714844,\n",
       " -0.00041651726,\n",
       " -0.052825928,\n",
       " -0.034179688,\n",
       " 0.004940033,\n",
       " 0.010894775,\n",
       " -0.061645508,\n",
       " 0.018325806,\n",
       " -0.01210022,\n",
       " -0.029846191,\n",
       " -0.028396606,\n",
       " 0.0020923615,\n",
       " -0.015220642,\n",
       " 0.051574707,\n",
       " -0.01826477,\n",
       " 0.07128906,\n",
       " -0.00970459,\n",
       " -0.052947998,\n",
       " -0.036010742,\n",
       " 0.0013198853,\n",
       " 0.033294678,\n",
       " -0.019866943,\n",
       " -0.034729004,\n",
       " -0.0871582,\n",
       " -0.017303467,\n",
       " 0.0029754639,\n",
       " -0.010925293,\n",
       " 0.044067383,\n",
       " -0.009124756,\n",
       " -0.038604736,\n",
       " 0.04336548,\n",
       " 0.020111084,\n",
       " -0.015182495,\n",
       " -0.043304443,\n",
       " -0.022445679,\n",
       " 0.08666992,\n",
       " -0.027633667,\n",
       " 0.019927979,\n",
       " -0.011253357,\n",
       " 0.059753418,\n",
       " -0.03265381,\n",
       " 0.09631348,\n",
       " 0.10394287,\n",
       " 0.028137207,\n",
       " 0.0053901672,\n",
       " 0.007881165,\n",
       " -0.0011453629,\n",
       " 0.003730774,\n",
       " -0.05822754,\n",
       " -0.027862549,\n",
       " -0.017974854,\n",
       " 0.016601562,\n",
       " -0.003320694,\n",
       " -0.033599854,\n",
       " -0.0231781,\n",
       " 0.016571045,\n",
       " -0.0021076202,\n",
       " -0.05267334,\n",
       " -0.0022068024,\n",
       " -0.03805542,\n",
       " -0.013465881,\n",
       " 0.0070152283,\n",
       " -0.039642334,\n",
       " -0.03366089,\n",
       " -0.03768921,\n",
       " 0.045928955,\n",
       " 0.02394104,\n",
       " -0.010604858,\n",
       " 0.046966553,\n",
       " -0.0070381165,\n",
       " 0.0023918152,\n",
       " -0.014289856,\n",
       " -0.0040130615,\n",
       " 0.0045433044,\n",
       " -0.0126953125,\n",
       " -0.033935547,\n",
       " -0.0016021729,\n",
       " -0.0061531067,\n",
       " -0.012023926,\n",
       " -0.004814148,\n",
       " -0.01197052,\n",
       " -0.072021484,\n",
       " -0.0848999,\n",
       " 0.025283813,\n",
       " -0.052215576,\n",
       " -0.016342163,\n",
       " -0.038635254,\n",
       " 7.915497e-05,\n",
       " -0.040618896,\n",
       " -0.059051514,\n",
       " 0.02432251,\n",
       " 0.02758789,\n",
       " 0.03692627,\n",
       " 0.0002465248,\n",
       " 0.01763916,\n",
       " -0.052520752,\n",
       " -0.0770874,\n",
       " 0.054016113,\n",
       " 0.023727417,\n",
       " -0.0062828064,\n",
       " 0.0033130646,\n",
       " 0.013549805,\n",
       " 0.025878906,\n",
       " 0.0028247833,\n",
       " 0.04724121,\n",
       " 0.054016113,\n",
       " 0.01889038,\n",
       " -0.04827881,\n",
       " -0.016326904,\n",
       " -0.021850586,\n",
       " 0.035308838,\n",
       " 0.030334473,\n",
       " -0.037994385,\n",
       " -0.04333496,\n",
       " -0.036468506,\n",
       " -0.033447266,\n",
       " -0.014755249,\n",
       " 0.018630981,\n",
       " 0.013046265,\n",
       " 0.010528564,\n",
       " 0.026031494,\n",
       " 0.024398804,\n",
       " -0.0046691895,\n",
       " -0.04660034,\n",
       " 0.045074463,\n",
       " 0.0066223145,\n",
       " -0.018554688,\n",
       " -0.06719971,\n",
       " -0.013809204,\n",
       " -0.044647217,\n",
       " -0.044158936,\n",
       " -0.060668945,\n",
       " -0.021987915,\n",
       " -0.004169464,\n",
       " 0.0076293945,\n",
       " -0.0014104843,\n",
       " -0.020843506,\n",
       " 0.041137695,\n",
       " -0.00069379807,\n",
       " 0.052215576,\n",
       " -0.050567627,\n",
       " 0.043823242,\n",
       " -0.00025105476,\n",
       " 0.005924225,\n",
       " 0.0044441223,\n",
       " 0.045837402,\n",
       " 0.06628418,\n",
       " 0.002576828,\n",
       " 0.046020508,\n",
       " 0.063964844,\n",
       " 0.031280518,\n",
       " 0.035980225,\n",
       " -0.023239136,\n",
       " -0.03427124,\n",
       " 0.028640747,\n",
       " 0.02168274,\n",
       " -0.059173584,\n",
       " -0.040771484,\n",
       " 0.00623703]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.embed_query(\"My query to look up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List with elements\n",
      "Index: 0, Value type: <class 'list'>, Value preview: [Document(page_content='3 2 0 2\\n\\nl u J\\n\\n0 1\\n\\\n",
      "Index: 1, Value type: <class 'list'>, Value preview: [Document(page_content=\"About Scotty\\n\\nThe Scotti\n",
      "Index: 2, Value type: <class 'list'>, Value preview: [Document(page_content='Sweepstakes Slang\\n\\nBuggy\n",
      "Index: 3, Value type: <class 'list'>, Value preview: [Document(page_content='2024-2025 OFFICIAL Academi\n",
      "Index: 4, Value type: <class 'list'>, Value preview: [Document(page_content='Andrew Carnegie\\n\\nA self-\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace 'your_file_path.pkl' with the path to your .pkl file\n",
    "file_path = 'splitDocuments.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    contents = pickle.load(file)\n",
    "\n",
    "def print_limited_structure(data, limit=5):\n",
    "    \"\"\"Print a limited view of the data structure.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        print(\"Dictionary with keys: values\")\n",
    "        for i, (key, value) in enumerate(data.items()):\n",
    "            print(f\"Key: {key}, Value type: {type(value)}, Value preview: {str(value)[:50]}\")\n",
    "            if i >= limit - 1:\n",
    "                break\n",
    "    elif isinstance(data, list):\n",
    "        print(\"List with elements\")\n",
    "        for i in range(min(limit, len(data))):\n",
    "            print(f\"Index: {i}, Value type: {type(data[i])}, Value preview: {str(data[i])[:50]}\")\n",
    "    elif isinstance(data, (set, tuple)):\n",
    "        print(f\"{type(data).__name__} with elements\")\n",
    "        for i, item in enumerate(data):\n",
    "            print(f\"Element: {i}, Value type: {type(item)}, Value preview: {str(item)[:50]}\")\n",
    "            if i >= limit - 1:\n",
    "                break\n",
    "    else:\n",
    "        print(\"Data type:\", type(data))\n",
    "        print(f\"Preview: {str(data)[:200]}\")  # Print the first 200 characters\n",
    "\n",
    "print_limited_structure(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAIjCAYAAAD1OgEdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfGUlEQVR4nO3deVyU9f7//+fAMAOCgKKAHDfKfS8t5diqJCmVmXUyLbGsToaWS2pWx1wqTVPTNM1P5XLSFjutWiouaSZuJGlaakloKWgojiACA9fvD7/Mzwk1xbkcwcf9duN2nPf1nte8rrmG0zy5rnmPxTAMQwAAAAAAj/LxdgMAAAAAUBERtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AKCcqFu3rvr06ePtNiq8iRMn6qqrrpKvr69atWpl6mN98803slgs+vjjj019HACAdxC2AMAL5s6dK4vFoi1btpxx+y233KJmzZpd9ON89dVXGjVq1EXXuVIsX75cw4YNU/v27TVnzhy98sorpeaUBKTz+SmPTp48qSlTpqht27YKCQmRv7+/GjRooP79+2v37t3ebk+StH79eo0aNUrZ2dnebgUAzsnq7QYAAOdn165d8vG5sL+RffXVV5oxYwaB6zytWrVKPj4+euedd2Sz2c44p3Hjxvrvf//rNjZixAgFBQXp+eefvxRtmubPP//U7bffrpSUFN1xxx3q2bOngoKCtGvXLn3wwQeaPXu2CgoKvN2m1q9fr9GjR6tPnz4KDQ31djsAcFaELQAoJ+x2u7dbuGC5ubkKDAz0dhvn7dChQwoICDhr0JKkiIgIPfjgg25j48ePV7Vq1UqNlzd9+vTR1q1b9fHHH6t79+5u28aOHVvuwyQAXGpcRggA5cRfP7NVWFio0aNHq379+vL391dYWJhuuOEGJSUlSTr1xnnGjBmSdMZL23JzczVkyBDVqlVLdrtdDRs21GuvvSbDMNweNy8vT0899ZSqVaumypUr66677tIff/whi8XidsZs1KhRslgs2rlzp3r27KkqVarohhtukCRt27ZNffr00VVXXSV/f39FRkbqkUceUVZWlttjldTYvXu3HnzwQYWEhKh69er6z3/+I8MwtH//fnXt2lXBwcGKjIzUpEmTzuu5czqdGjt2rK6++mrZ7XbVrVtXzz33nPLz811zLBaL5syZo9zcXNdzNXfu3POqfyZ79+7Vfffdp6pVq6pSpUpq166dlixZ8rf3y8/P1x133KGQkBCtX79eklRcXKzXX39dTZs2lb+/vyIiIvTvf/9bR48edbtv3bp1dccdd2jdunW6/vrr5e/vr6uuukrz58//28fduHGjlixZor59+5YKWtKpsP/aa6+5ja1atUo33nijAgMDFRoaqq5du+qnn35ym9OnTx/VrVu3VL2SY306i8Wi/v3767PPPlOzZs1kt9vVtGlTLV261O1+Q4cOlSRFR0e7jtVvv/0mSUpKStINN9yg0NBQBQUFqWHDhnruuef+dv8BwAyc2QIALzp27Jj+/PPPUuOFhYV/e99Ro0Zp3LhxevTRR3X99dfL4XBoy5Yt+v7773Xbbbfp3//+tw4cOKCkpKRSl70ZhqG77rpLq1evVt++fdWqVSstW7ZMQ4cO1R9//KEpU6a45vbp00cfffSRHnroIbVr105r1qxRfHz8Wfu67777VL9+fb3yyiuu4JaUlKS9e/fq4YcfVmRkpHbs2KHZs2drx44d2rBhQ6k33ffff78aN26s8ePHa8mSJXrppZdUtWpVvfXWW+rQoYNeffVVLViwQM8884yuu+463XTTTed8rh599FHNmzdP9957r4YMGaKNGzdq3Lhx+umnn/Tpp59Kkv773/9q9uzZ2rRpk95++21J0j//+c+/PQ5nkpmZqX/+8586ceKEnnrqKYWFhWnevHm666679PHHH6tbt25nvF9eXp66du2qLVu2aMWKFbruuuskSf/+9781d+5cPfzww3rqqaeUlpam6dOna+vWrfruu+/k5+fnqvHLL7/o3nvvVd++fZWQkKB3331Xffr0UevWrdW0adOz9vzFF19Ikh566KHz2scVK1aoc+fOuuqqqzRq1Cjl5eXpjTfeUPv27fX999+fMWCdj3Xr1umTTz7Rk08+qcqVK2vatGnq3r279u3bp7CwMN1zzz3avXu33n//fU2ZMkXVqlWTJFWvXl07duzQHXfcoRYtWmjMmDGy2+365Zdf9N1335WpFwC4aAYA4JKbM2eOIemcP02bNnW7T506dYyEhATX7ZYtWxrx8fHnfJzExETjTP9X/9lnnxmSjJdeeslt/N577zUsFovxyy+/GIZhGCkpKYYkY+DAgW7z+vTpY0gyXnzxRdfYiy++aEgyHnjggVKPd+LEiVJj77//viHJWLt2bakajz/+uGvM6XQaNWvWNCwWizF+/HjX+NGjR42AgAC35+RMUlNTDUnGo48+6jb+zDPPGJKMVatWucYSEhKMwMDAc9Y7k6ZNmxo333yz6/bAgQMNSca3337rGjt+/LgRHR1t1K1b1ygqKjIMwzBWr15tSDIWLVpkHD9+3Lj55puNatWqGVu3bnXd79tvvzUkGQsWLHB7zKVLl5Yar1OnTqnn9NChQ4bdbjeGDBlyzn3o1q2bIck4evToee1zq1atjPDwcCMrK8s19sMPPxg+Pj5G7969XWMJCQlGnTp1St2/5FifTpJhs9lcr7+SmpKMN954wzU2ceJEQ5KRlpbmdv8pU6YYkozDhw+f1z4AgNm4jBAAvGjGjBlKSkoq9dOiRYu/vW9oaKh27NihPXv2XPDjfvXVV/L19dVTTz3lNj5kyBAZhqGvv/5aklyXbz355JNu8wYMGHDW2k888USpsYCAANe/T548qT///FPt2rWTJH3//fel5j/66KOuf/v6+qpNmzYyDEN9+/Z1jYeGhqphw4bau3fvWXuRTu2rJA0ePNhtfMiQIZJ0Xpf2XaivvvpK119/vesySkkKCgrS448/rt9++007d+50m3/s2DF16tRJP//8s7755hu3JecXLVqkkJAQ3Xbbbfrzzz9dP61bt1ZQUJBWr17tVqtJkya68cYbXberV69+Xs+Tw+GQJFWuXPlv9+/gwYNKTU1Vnz59VLVqVdd4ixYtdNttt7me87KIjY3V1Vdf7VYzODj4b/uX5Fos4/PPP1dxcXGZewAATyFsAYAXXX/99YqNjS31U6VKlb+975gxY5Sdna0GDRqoefPmGjp0qLZt23Zej5uenq6oqKhSb6wbN27s2l7yvz4+PoqOjnabV69evbPW/utcSTpy5IiefvppRUREKCAgQNWrV3fNO3bsWKn5tWvXdrtdsgR5ySVjp4//9XNLf1WyD3/tOTIyUqGhoa599aT09HQ1bNiw1Phfn98SAwcO1ObNm7VixYpSl/rt2bNHx44dU3h4uKpXr+72k5OTo0OHDrnN/+tzJ0lVqlT52+cpODhYknT8+PHz2j9JZ93HP//8U7m5uX9b50zK2r906vLT9u3b69FHH1VERIR69Oihjz76iOAFwGv4zBYAlFM33XSTfv31V33++edavny53n77bU2ZMkWzZs1yOzN0qZ1+FqvEv/71L61fv15Dhw5Vq1atFBQUpOLiYt1+++1nfCPs6+t7XmOSSi3ocTaX8/dede3aVR988IHGjx+v+fPnuy3xX1xcrPDwcC1YsOCM961evbrb7bI+T40aNZIkbd++3e3M2MU62/NeVFR0xvGLOc4BAQFau3atVq9erSVLlmjp0qX68MMP1aFDBy1fvvystQHALJzZAoByrGrVqnr44Yf1/vvva//+/WrRooXbCoFne6Nbp04dHThwoNRZjJ9//tm1veR/i4uLlZaW5jbvl19+Oe8ejx49qpUrV+rZZ5/V6NGj1a1bN91222266qqrzrvGxSjZh79ebpmZmans7GzXvnr6MXft2lVq/K/Pb4m7775b7777rhYuXKjExES3bVdffbWysrLUvn37M54FbdmypUd6vvPOOyVJ77333t/OLen/bPtYrVo115L/VapUOeOXD1/MGcVzBWcfHx917NhRkydP1s6dO/Xyyy9r1apVpS63BIBLgbAFAOXUX5dNDwoKUr169dyWMy95w/vXN7tdunRRUVGRpk+f7jY+ZcoUWSwWde7cWZIUFxcnSXrzzTfd5r3xxhvn3WfJ2YS/npl4/fXXz7vGxejSpcsZH2/y5MmSdM6VFS/mMTdt2qTk5GTXWG5urmbPnq26deuqSZMmpe7Tu3dvTZs2TbNmzdLw4cNd4//6179UVFSksWPHlrqP0+k8Y5Api5iYGN1+++16++239dlnn5XaXlBQoGeeeUaSVKNGDbVq1Urz5s1ze/wff/xRy5cvdz3n0qmweOzYMbdLXA8ePOhaBbIszva6PnLkSKm5JZ9/O/33AgAuFS4jBIByqkmTJrrlllvUunVrVa1aVVu2bNHHH3+s/v37u+a0bt1akvTUU08pLi5Ovr6+6tGjh+68807deuutev755/Xbb7+pZcuWWr58uT7//HMNHDjQtUBB69at1b17d73++uvKyspyLf2+e/duSed3aV5wcLBuuukmTZgwQYWFhfrHP/6h5cuXlzpbZpaWLVsqISFBs2fPVnZ2tm6++WZt2rRJ8+bN0913361bb73V44/57LPP6v3331fnzp311FNPqWrVqpo3b57S0tL0v//9z+0ywdP1799fDodDzz//vEJCQvTcc8/p5ptv1r///W+NGzdOqamp6tSpk/z8/LRnzx4tWrRIU6dO1b333uuRvufPn69OnTrpnnvu0Z133qmOHTsqMDBQe/bs0QcffKCDBw+6vmtr4sSJ6ty5s2JiYtS3b1/X0u8hISFuZ1d79Oih4cOHq1u3bnrqqad04sQJzZw5Uw0aNDjj4ijno+R1/fzzz6tHjx7y8/PTnXfeqTFjxmjt2rWKj49XnTp1dOjQIb355puqWbOm22IlAHDJeHMpRAC4UpUs/b558+Yzbr/55pv/dun3l156ybj++uuN0NBQIyAgwGjUqJHx8ssvGwUFBa45TqfTGDBggFG9enXDYrG4LbV9/PhxY9CgQUZUVJTh5+dn1K9f35g4caJRXFzs9ri5ublGYmKiUbVqVSMoKMi4++67jV27dhmS3JZiL1nK+0zLbv/+++9Gt27djNDQUCMkJMS47777jAMHDpx1+fi/1jjbkuxnep7OpLCw0Bg9erQRHR1t+Pn5GbVq1TJGjBhhnDx58rwe5+/8del3wzCMX3/91bj33nuN0NBQw9/f37j++uuNxYsXu805fen30w0bNsyQZEyfPt01Nnv2bKN169ZGQECAUblyZaN58+bGsGHDjAMHDrjm1KlT54xfB3DzzTeX6u9sTpw4Ybz22mvGddddZwQFBRk2m82oX7++MWDAALcl2Q3DMFasWGG0b9/eCAgIMIKDg40777zT2LlzZ6may5cvN5o1a2bYbDajYcOGxnvvvXfWpd8TExNL3f+vr33DMIyxY8ca//jHPwwfHx/XMvArV640unbtakRFRRk2m82IiooyHnjgAWP37t3nte8A4GkWwzjPTxYDAPD/pKam6pprrtF7772nXr16ebsdAAAuS3xmCwBwTnl5eaXGXn/9dfn4+Oimm27yQkcAAJQPfGYLAHBOEyZMUEpKim699VZZrVZ9/fXX+vrrr/X444+rVq1a3m4PAIDLFpcRAgDOKSkpSaNHj9bOnTuVk5Oj2rVr66GHHtLzzz8vq5W/2QEAcDaELQAAAAAwAZ/ZAgAAAAATELYAAAAAwARcbH8eiouLdeDAAVWuXPm8vsATAAAAQMVkGIaOHz+uqKios35JfQnC1nk4cOAAK24BAAAAcNm/f79q1qx5zjmErfNQuXJlSaee0ODgYC93AwAAAMBbHA6HatWq5coI50LYOg8llw4GBwcTtgAAAACc18eLWCADAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABN4NWwVFRXpP//5j6KjoxUQEKCrr75aY8eOlWEYrjmGYWjkyJGqUaOGAgICFBsbqz179rjVOXLkiHr16qXg4GCFhoaqb9++ysnJcZuzbds23XjjjfL391etWrU0YcKES7KPAAAAAK5MXg1br776qmbOnKnp06frp59+0quvvqoJEybojTfecM2ZMGGCpk2bplmzZmnjxo0KDAxUXFycTp486ZrTq1cv7dixQ0lJSVq8eLHWrl2rxx9/3LXd4XCoU6dOqlOnjlJSUjRx4kSNGjVKs2fPvqT7CwAAAODKYTFOP410id1xxx2KiIjQO++84xrr3r27AgIC9N5778kwDEVFRWnIkCF65plnJEnHjh1TRESE5s6dqx49euinn35SkyZNtHnzZrVp00aStHTpUnXp0kW///67oqKiNHPmTD3//PPKyMiQzWaTJD377LP67LPP9PPPP/9tnw6HQyEhITp27BhfagwAAABcwS4kG1gvUU9n9M9//lOzZ8/W7t271aBBA/3www9at26dJk+eLElKS0tTRkaGYmNjXfcJCQlR27ZtlZycrB49eig5OVmhoaGuoCVJsbGx8vHx0caNG9WtWzclJyfrpptucgUtSYqLi9Orr76qo0ePqkqVKm595efnKz8/33Xb4XBIkpxOp5xOpynPBQAAAIDL34XkAa+GrWeffVYOh0ONGjWSr6+vioqK9PLLL6tXr16SpIyMDElSRESE2/0iIiJc2zIyMhQeHu623Wq1qmrVqm5zoqOjS9Uo2fbXsDVu3DiNHj26VL9btmxRYGBgWXcXAAAAQDmXm5t73nO9GrY++ugjLViwQAsXLlTTpk2VmpqqgQMHKioqSgkJCV7ra8SIERo8eLDrtsPhUK1atdSmTRsuIwQAAACuYCVXvZ0Pr4atoUOH6tlnn1WPHj0kSc2bN1d6errGjRunhIQERUZGSpIyMzNVo0YN1/0yMzPVqlUrSVJkZKQOHTrkVtfpdOrIkSOu+0dGRiozM9NtTsntkjmns9vtstvtpcatVqusVq8+ZQAAAAC86ELygFdXIzxx4oR8fNxb8PX1VXFxsSQpOjpakZGRWrlypWu7w+HQxo0bFRMTI0mKiYlRdna2UlJSXHNWrVql4uJitW3b1jVn7dq1KiwsdM1JSkpSw4YNS11CCAAAAACe4NWwdeedd+rll1/WkiVL9Ntvv+nTTz/V5MmT1a1bN0mSxWLRwIED9dJLL+mLL77Q9u3b1bt3b0VFRenuu++WJDVu3Fi33367HnvsMW3atEnfffed+vfvrx49eigqKkqS1LNnT9lsNvXt21c7duzQhx9+qKlTp7pdKggAAAAAnuTVpd+PHz+u//znP/r000916NAhRUVF6YEHHtDIkSNdKwcahqEXX3xRs2fPVnZ2tm644Qa9+eabatCggavOkSNH1L9/f3355Zfy8fFR9+7dNW3aNAUFBbnmbNu2TYmJidq8ebOqVaumAQMGaPjw4efVJ0u/AwAAAJAuLBt4NWyVF4QtAAAAANKFZQOvXkYIAAAAABUVS+sBl4nDhw9f0FKiFyo4OFjVq1c3rT4AAADcEbaAy8Dhw4fVs2c/ZWXlm/YYYWF2LVw4k8AFAABwiRC2gMuAw+FQVla+7PYhCgio5fH6eXn7lZU1SQ6Hg7AFAABwiRC2gMtIQEAtBQZebUrtfPNOmgEAAOAMWCADAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMIFXw1bdunVlsVhK/SQmJkqSTp48qcTERIWFhSkoKEjdu3dXZmamW419+/YpPj5elSpVUnh4uIYOHSqn0+k255tvvtG1114ru92uevXqae7cuZdqFwEAAABcoazefPDNmzerqKjIdfvHH3/Ubbfdpvvuu0+SNGjQIC1ZskSLFi1SSEiI+vfvr3vuuUffffedJKmoqEjx8fGKjIzU+vXrdfDgQfXu3Vt+fn565ZVXJElpaWmKj4/XE088oQULFmjlypV69NFHVaNGDcXFxV36nUa5dvjwYTkcDo/XTU9PL/VHAgAAAJRvFsMwDG83UWLgwIFavHix9uzZI4fDoerVq2vhwoW69957JUk///yzGjdurOTkZLVr105ff/217rjjDh04cEARERGSpFmzZmn48OE6fPiwbDabhg8friVLlujHH390PU6PHj2UnZ2tpUuXnldfDodDISEhOnbsmIKDgz2/4ygXDh8+rJ49+ykrK9/jtfPzc7V/f6Zatlyk0NAmHq+fm/ursrMHatGi13X11Vd7vD4AAMCV4kKygVfPbJ2uoKBA7733ngYPHiyLxaKUlBQVFhYqNjbWNadRo0aqXbu2K2wlJyerefPmrqAlSXFxcerXr5927Niha665RsnJyW41SuYMHDjwrL3k5+crP///f0NdcibD6XRy9uEKlp2drWPHnAoMHKyAgJoerr1JPj4TJBXK19fzrzGrtVhWq6+Ki4t5DQMAAFyEC3kvddmErc8++0zZ2dnq06ePJCkjI0M2m02hoaFu8yIiIpSRkeGac3rQKtlesu1ccxwOh/Ly8hQQEFCql3Hjxmn06NGlxrds2aLAwMAy7R/Kv7y8PPXsGSertUi+voc8WruwMEQ5OQkKCsqQn1+OR2tLUlFRnpzOOKWnp+vQIc/2DgAAcCXJzc0977mXTdh655131LlzZ0VFRXm7FY0YMUKDBw923XY4HKpVq5batGnDZYRXsLS0ND333HSFhsaqUqVoj9bOylqj7dvnqXnz+QoLa+rR2pJ04kSasrOna8GCWEVHe7Z3AACAK8mFfH7/sghb6enpWrFihT755BPXWGRkpAoKCpSdne12diszM1ORkZGuOZs2bXKrVbJa4elz/rqCYWZmpoKDg894VkuS7Ha77HZ7qXGr1Sqr9bJ4yuAFPj4+cjqL5HT6qKjIs68Dp9OigoJCU2qfqn+qdx8fH17DAAAAF+FC3ktdFt+zNWfOHIWHhys+Pt411rp1a/n5+WnlypWusV27dmnfvn2KiYmRJMXExGj79u1ul0UlJSUpODhYTZo0cc05vUbJnJIaAAAAAGAGr4et4uJizZkzRwkJCW4pMSQkRH379tXgwYO1evVqpaSk6OGHH1ZMTIzatWsnSerUqZOaNGmihx56SD/88IOWLVumF154QYmJia4zU0888YT27t2rYcOG6eeff9abb76pjz76SIMGDfLK/gIAAAC4Mnj9eqIVK1Zo3759euSRR0ptmzJlinx8fNS9e3fl5+crLi5Ob775pmu7r6+vFi9erH79+ikmJkaBgYFKSEjQmDFjXHOio6O1ZMkSDRo0SFOnTlXNmjX19ttv8x1bAAAAAEzl9bDVqVMnne2rvvz9/TVjxgzNmDHjrPevU6eOvvrqq3M+xi233KKtW7deVJ8AAAAAcCG8fhkhAAAAAFREhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwAReD1t//PGHHnzwQYWFhSkgIEDNmzfXli1bXNsNw9DIkSNVo0YNBQQEKDY2Vnv27HGrceTIEfXq1UvBwcEKDQ1V3759lZOT4zZn27ZtuvHGG+Xv769atWppwoQJl2T/AAAAAFyZvBq2jh49qvbt28vPz09ff/21du7cqUmTJqlKlSquORMmTNC0adM0a9Ysbdy4UYGBgYqLi9PJkyddc3r16qUdO3YoKSlJixcv1tq1a/X444+7tjscDnXq1El16tRRSkqKJk6cqFGjRmn27NmXdH8BAAAAXDms3nzwV199VbVq1dKcOXNcY9HR0a5/G4ah119/XS+88IK6du0qSZo/f74iIiL02WefqUePHvrpp5+0dOlSbd68WW3atJEkvfHGG+rSpYtee+01RUVFacGCBSooKNC7774rm82mpk2bKjU1VZMnT3YLZQAAAADgKV4NW1988YXi4uJ03333ac2aNfrHP/6hJ598Uo899pgkKS0tTRkZGYqNjXXdJyQkRG3btlVycrJ69Oih5ORkhYaGuoKWJMXGxsrHx0cbN25Ut27dlJycrJtuukk2m801Jy4uTq+++qqOHj3qdiZNkvLz85Wfn++67XA4JElOp1NOp9OU5wKXv+LiYlmtvrJai+Xr69nXgdVqyGbzM6X2qfqnei8uLuY1DAAAcBEu5L2UV8PW3r17NXPmTA0ePFjPPfecNm/erKeeeko2m00JCQnKyMiQJEVERLjdLyIiwrUtIyND4eHhbtutVquqVq3qNuf0M2an18zIyCgVtsaNG6fRo0eX6nfLli0KDAy8iD1GeZaXl6eePeNktabL1/eQR2sXFuapc+cEBQVlyM8v5+/vcIGKivLkdMYpPT1dhw55tncAAIArSW5u7nnP9WrYKi4uVps2bfTKK69Ikq655hr9+OOPmjVrlhISErzW14gRIzR48GDXbYfDoVq1aqlNmzYKDg72Wl/wrrS0ND333HSFhsaqUqXov7/DBcjKWqPt2+epefP5Cgtr6tHaknTiRJqys6drwYLYUn94AAAAwPkruertfHg1bNWoUUNNmjRxG2vcuLH+97//SZIiIyMlSZmZmapRo4ZrTmZmplq1auWa89e/1DudTh05csR1/8jISGVmZrrNKbldMud0drtddru91LjVapXV6tWnDF7k4+Mjp7NITqePioo8+zpwOi0qKCg0pfap+qd69/Hx4TUMAABwES7kvZRXVyNs3769du3a5Ta2e/du1alTR9KpxTIiIyO1cuVK13aHw6GNGzcqJiZGkhQTE6Ps7GylpKS45qxatUrFxcVq27ata87atWtVWFjompOUlKSGDRuWuoQQAAAAADzBq2Fr0KBB2rBhg1555RX98ssvWrhwoWbPnq3ExERJksVi0cCBA/XSSy/piy++0Pbt29W7d29FRUXp7rvvlnTqTNjtt9+uxx57TJs2bdJ3332n/v37q0ePHoqKipIk9ezZUzabTX379tWOHTv04YcfaurUqW6XCgIAAACAJ3n1eqLrrrtOn376qUaMGKExY8YoOjpar7/+unr16uWaM2zYMOXm5urxxx9Xdna2brjhBi1dulT+/v6uOQsWLFD//v3VsWNH+fj4qHv37po2bZpre0hIiJYvX67ExES1bt1a1apV08iRI1n2HQAAAIBpvP7hjTvuuEN33HHHWbdbLBaNGTNGY8aMOeucqlWrauHChed8nBYtWujbb78tc58AAAAAcCG8ehkhAAAAAFRUhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwAReDVujRo2SxWJx+2nUqJFr+8mTJ5WYmKiwsDAFBQWpe/fuyszMdKuxb98+xcfHq1KlSgoPD9fQoUPldDrd5nzzzTe69tprZbfbVa9ePc2dO/dS7B4AAACAK5jXz2w1bdpUBw8edP2sW7fOtW3QoEH68ssvtWjRIq1Zs0YHDhzQPffc49peVFSk+Ph4FRQUaP369Zo3b57mzp2rkSNHuuakpaUpPj5et956q1JTUzVw4EA9+uijWrZs2SXdTwAAAABXFqvXG7BaFRkZWWr82LFjeuedd7Rw4UJ16NBBkjRnzhw1btxYGzZsULt27bR8+XLt3LlTK1asUEREhFq1aqWxY8dq+PDhGjVqlGw2m2bNmqXo6GhNmjRJktS4cWOtW7dOU6ZMUVxc3CXdVwAAAABXDq+HrT179igqKkr+/v6KiYnRuHHjVLt2baWkpKiwsFCxsbGuuY0aNVLt2rWVnJysdu3aKTk5Wc2bN1dERIRrTlxcnPr166cdO3bommuuUXJysluNkjkDBw48a0/5+fnKz8933XY4HJIkp9NZ6hJFXDmKi4tltfrKai2Wr69nXwdWqyGbzc+U2qfqn+q9uLiY1zAAAMBFuJD3Ul4NW23bttXcuXPVsGFDHTx4UKNHj9aNN96oH3/8URkZGbLZbAoNDXW7T0REhDIyMiRJGRkZbkGrZHvJtnPNcTgcysvLU0BAQKm+xo0bp9GjR5ca37JliwIDA8u8vyjf8vLy1LNnnKzWdPn6HvJo7cLCPHXunKCgoAz5+eV4tLYkFRXlyemMU3p6ug4d8mzvAAAAV5Lc3NzznuvVsNW5c2fXv1u0aKG2bduqTp06+uijj84Ygi6VESNGaPDgwa7bDodDtWrVUps2bRQcHOy1vuBdaWlpeu656QoNjVWlStEerZ2VtUbbt89T8+bzFRbW1KO1JenEiTRlZ0/XggWxio72bO8AAABXkpKr3s6H1y8jPF1oaKgaNGigX375RbfddpsKCgqUnZ3tdnYrMzPT9RmvyMhIbdq0ya1GyWqFp8/56wqGmZmZCg4OPmugs9vtstvtpcatVqus1svqKcMl5OPjI6ezSE6nj4qKPPs6cDotKigoNKX2qfqnevfx8eE1DAAAcBEu5L2U11cjPF1OTo5+/fVX1ahRQ61bt5afn59Wrlzp2r5r1y7t27dPMTExkqSYmBht377d7bKopKQkBQcHq0mTJq45p9comVNSAwAAAADM4NWw9cwzz2jNmjX67bfftH79enXr1k2+vr564IEHFBISor59+2rw4MFavXq1UlJS9PDDDysmJkbt2rWTJHXq1ElNmjTRQw89pB9++EHLli3TCy+8oMTERNeZqSeeeEJ79+7VsGHD9PPPP+vNN9/URx99pEGDBnlz1wEAAABUcF69nuj333/XAw88oKysLFWvXl033HCDNmzYoOrVq0uSpkyZIh8fH3Xv3l35+fmKi4vTm2++6bq/r6+vFi9erH79+ikmJkaBgYFKSEjQmDFjXHOio6O1ZMkSDRo0SFOnTlXNmjX19ttvs+w7AAAAAFN5NWx98MEH59zu7++vGTNmaMaMGWedU6dOHX311VfnrHPLLbdo69atZeoRAAAAAMrisvrMFgAAAABUFIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMEGZwtbevXs93QcAAAAAVChlClv16tXTrbfeqvfee08nT570dE8AAAAAUO6VKWx9//33atGihQYPHqzIyEj9+9//1qZNmzzdGwAAAACUW2UKW61atdLUqVN14MABvfvuuzp48KBuuOEGNWvWTJMnT9bhw4c93ScAAAAAlCsXtUCG1WrVPffco0WLFunVV1/VL7/8omeeeUa1atVS7969dfDgQU/1CQAAAADlykWFrS1btujJJ59UjRo1NHnyZD3zzDP69ddflZSUpAMHDqhr166e6hMAAAAAyhVrWe40efJkzZkzR7t27VKXLl00f/58denSRT4+p7JbdHS05s6dq7p163qyVwAAAAAoN8oUtmbOnKlHHnlEffr0UY0aNc44Jzw8XO+8885FNQcAAAAA5VWZwtaePXv+do7NZlNCQkJZygMAAABAuVemz2zNmTNHixYtKjW+aNEizZs376KbAgAAAIDyrkxha9y4capWrVqp8fDwcL3yyisX3RQAAAAAlHdlClv79u1TdHR0qfE6depo3759F90UAAAAAJR3ZQpb4eHh2rZtW6nxH374QWFhYRfdFAAAAACUd2UKWw888ICeeuoprV69WkVFRSoqKtKqVav09NNPq0ePHp7uEQAAAADKnTKtRjh27Fj99ttv6tixo6zWUyWKi4vVu3dvPrMFAAAAACpj2LLZbPrwww81duxY/fDDDwoICFDz5s1Vp04dT/cHwEMKC/OVnp5uSu3g4GBVr17dlNoAAADlVZnCVokGDRqoQYMGnuoFgEkKCrKUnr5XAwaMl91u93j9sDC7Fi6cSeACAAA4TZnCVlFRkebOnauVK1fq0KFDKi4udtu+atUqjzQHwDOKinLkdNpksw1SaKhn/0CSl7dfWVmT5HA4CFsAAACnKVPYevrppzV37lzFx8erWbNmslgsnu4LgAn8/WsqMPBqj9fNz/d4SQAAgHKvTGHrgw8+0EcffaQuXbp4uh8AAAAAqBDKtPS7zWZTvXr1PN0LAAAAAFQYZQpbQ4YM0dSpU2UYhqf7AQAAAIAKoUyXEa5bt06rV6/W119/raZNm8rPz89t+yeffOKR5gAAAACgvCpT2AoNDVW3bt083QsAAAAAVBhlCltz5szxdB8AAAAAUKGU6TNbkuR0OrVixQq99dZbOn78uCTpwIEDysnJ8VhzAAAAAFBelenMVnp6um6//Xbt27dP+fn5uu2221S5cmW9+uqrys/P16xZszzdJwAAAACUK2U6s/X000+rTZs2Onr0qAICAlzj3bp108qVKz3WHAAAAACUV2U6s/Xtt99q/fr1stlsbuN169bVH3/84ZHGAAAAAKA8K9OZreLiYhUVFZUa//3331W5cuWLbgoAAAAAyrsyha1OnTrp9ddfd922WCzKycnRiy++qC5duniqNwAAAAAot8p0GeGkSZMUFxenJk2a6OTJk+rZs6f27NmjatWq6f333/d0jwAAAABQ7pQpbNWsWVM//PCDPvjgA23btk05OTnq27evevXq5bZgBgAAAABcqcoUtiTJarXqwQcf9GQvAAAAAFBhlClszZ8//5zbe/fuXaZmAAAAAKCiKFPYevrpp91uFxYW6sSJE7LZbKpUqRJhCwAAAMAVr0yrER49etTtJycnR7t27dINN9xQ5gUyxo8fL4vFooEDB7rGTp48qcTERIWFhSkoKEjdu3dXZmam2/327dun+Ph4VapUSeHh4Ro6dKicTqfbnG+++UbXXnut7Ha76tWrp7lz55apRwAAAAA4X2UKW2dSv359jR8/vtRZr/OxefNmvfXWW2rRooXb+KBBg/Tll19q0aJFWrNmjQ4cOKB77rnHtb2oqEjx8fEqKCjQ+vXrNW/ePM2dO1cjR450zUlLS1N8fLxuvfVWpaamauDAgXr00Ue1bNmysu8sAAAAAPyNMi+QccZiVqsOHDhwQffJyclRr1699H//93966aWXXOPHjh3TO++8o4ULF6pDhw6SpDlz5qhx48basGGD2rVrp+XLl2vnzp1asWKFIiIi1KpVK40dO1bDhw/XqFGjZLPZNGvWLEVHR2vSpEmSpMaNG2vdunWaMmWK4uLizthTfn6+8vPzXbcdDockyel0ljprhitHcXGxrFZfWa3F8vX17OvAajVks/mZUtvs+lbrqeeluLiY3w8AAFDhXcj7nTKFrS+++MLttmEYOnjwoKZPn6727dtfUK3ExETFx8crNjbWLWylpKSosLBQsbGxrrFGjRqpdu3aSk5OVrt27ZScnKzmzZsrIiLCNScuLk79+vXTjh07dM011yg5OdmtRsmc0y9X/Ktx48Zp9OjRpca3bNmiwMDAC9o/VBx5eXnq2TNOVmu6fH0PebR2YWGeOndOUFBQhvz8cjxa2+z6RUV5cjrjlJ6erkOHPPu8AAAAXG5yc3PPe26Zwtbdd9/tdttisah69erq0KGD6wzS+fjggw/0/fffa/PmzaW2ZWRkyGazKTQ01G08IiJCGRkZrjmnB62S7SXbzjXH4XAoLy/vjN8LNmLECA0ePNh12+FwqFatWmrTpo2Cg4PPe/9QsaSlpem556YrNDRWlSpFe7R2VtYabd8+T82bz1dYWFOP1ja7/okTacrOnq4FC2IVHe3Z5wUAAOByU3LV2/koU9gqLi4uy93c7N+/X08//bSSkpLk7+9/0fU8yW63y263lxq3Wq2yWj165SXKER8fHzmdRXI6fVRU5NnXgdNpUUFBoSm1za7vdJ56Xnx8fPj9AAAAFd6FvN/x2AIZFyolJUWHDh3Stdde6woxa9as0bRp02S1WhUREaGCggJlZ2e73S8zM1ORkZGSpMjIyFKrE5bc/rs5wcHBZzyrBQAAAACeUKY/Q59+id3fmTx58hnHO3bsqO3bt7uNPfzww2rUqJGGDx+uWrVqyc/PTytXrlT37t0lSbt27dK+ffsUExMjSYqJidHLL7+sQ4cOKTw8XJKUlJSk4OBgNWnSxDXnq6++cnucpKQkVw0AAAAAMEOZwtbWrVu1detWFRYWqmHDhpKk3bt3y9fXV9dee61rnsViOWuNypUrq1mzZm5jgYGBCgsLc4337dtXgwcPVtWqVRUcHKwBAwYoJiZG7dq1kyR16tRJTZo00UMPPaQJEyYoIyNDL7zwghITE12XAT7xxBOaPn26hg0bpkceeUSrVq3SRx99pCVLlpRl1wEAAADgvJQpbN15552qXLmy5s2bpypVqkg69UXHDz/8sG688UYNGTLEI81NmTJFPj4+6t69u/Lz8xUXF6c333zTtd3X11eLFy9Wv379FBMTo8DAQCUkJGjMmDGuOdHR0VqyZIkGDRqkqVOnqmbNmnr77bfPuuw7AAAAAHhCmcLWpEmTtHz5clfQkqQqVaropZdeUqdOncoctr755hu32/7+/poxY4ZmzJhx1vvUqVOn1GWCf3XLLbdo69atZeoJAAAAAMqiTAtkOBwOHT58uNT44cOHdfz48YtuCgAAAADKuzKFrW7duunhhx/WJ598ot9//12///67/ve//6lv37665557PN0jAAAAAJQ7ZbqMcNasWXrmmWfUs2dPFRYWnipktapv376aOHGiRxsEAAAAgPKoTGGrUqVKevPNNzVx4kT9+uuvkqSrr75agYGBHm0OAAAAAMqri/pS44MHD+rgwYOqX7++AgMDZRiGp/oCAAAAgHKtTGErKytLHTt2VIMGDdSlSxcdPHhQ0qnvxfLUsu8AAAAAUJ6VKWwNGjRIfn5+2rdvnypVquQav//++7V06VKPNQcAAAAA5VWZPrO1fPlyLVu2TDVr1nQbr1+/vtLT0z3SGAAAAACUZ2U6s5Wbm+t2RqvEkSNHZLfbL7opAAAAACjvyhS2brzxRs2fP99122KxqLi4WBMmTNCtt97qseYAAAAAoLwq02WEEyZMUMeOHbVlyxYVFBRo2LBh2rFjh44cOaLvvvvO0z0C5+3w4cNyOBym1E5PT5fT6TSlNgAAACqeMoWtZs2aaffu3Zo+fboqV66snJwc3XPPPUpMTFSNGjU83SNwXg4fPqyePfspKyvflPr5+bnavz9TISHm1AcAAEDFcsFhq7CwULfffrtmzZql559/3oyegDJxOBzKysqX3T5EAQG1PF7/6NENcjpfltNZ5PHaAAAAqHguOGz5+flp27ZtZvQCeERAQC0FBl7t8bp5eay0CQAAgPNXpgUyHnzwQb3zzjue7gUAAAAAKowyfWbL6XTq3Xff1YoVK9S6dWsFBga6bZ88ebJHmgMAAACA8uqCwtbevXtVt25d/fjjj7r22mslSbt373abY7FYPNcdAAAAAJRTFxS26tevr4MHD2r16tWSpPvvv1/Tpk1TRESEKc0BAAAAQHl1QZ/ZMgzD7fbXX3+t3NxcjzYEAAAAABVBmRbIKPHX8AUAAAAAOOWCwpbFYin1mSw+owUAAAAApV3QZ7YMw1CfPn1kt9slSSdPntQTTzxRajXCTz75xHMdAgAAAEA5dEFhKyEhwe32gw8+6NFmAAAAAKCiuKCwNWfOHLP6AAAAAIAK5aIWyAAAAAAAnBlhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADDBBa1GiMvH4cOH5XA4TKkdHBys6tWrm1IbAAAAuFIQtsqhw4cPq2fPfsrKyjelfliYXQsXziRwAQAAABeBsFUOORwOZWXly24fooCAWh6tnZe3X1lZk+RwOAhbAAAAwEUgbJVjAQG1FBh4tcfr5ptzwgwAAAC4orBABgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYAKvhq2ZM2eqRYsWCg4OVnBwsGJiYvT111+7tp88eVKJiYkKCwtTUFCQunfvrszMTLca+/btU3x8vCpVqqTw8HANHTpUTqfTbc4333yja6+9Vna7XfXq1dPcuXMvxe4BAAAAuIJ5NWzVrFlT48ePV0pKirZs2aIOHTqoa9eu2rFjhyRp0KBB+vLLL7Vo0SKtWbNGBw4c0D333OO6f1FRkeLj41VQUKD169dr3rx5mjt3rkaOHOmak5aWpvj4eN16661KTU3VwIED9eijj2rZsmWXfH8BAAAAXDms3nzwO++80+32yy+/rJkzZ2rDhg2qWbOm3nnnHS1cuFAdOnSQJM2ZM0eNGzfWhg0b1K5dOy1fvlw7d+7UihUrFBERoVatWmns2LEaPny4Ro0aJZvNplmzZik6OlqTJk2SJDVu3Fjr1q3TlClTFBcXd8a+8vPzlZ+f77rtcDgkSU6ns9RZM28oLi6W1eorq7VYvr6e7cdqPVW7uLj4stjXC2Hm8yJJVqshm83PpOfdvNpm1y/PrxkAAIALdSHvd7watk5XVFSkRYsWKTc3VzExMUpJSVFhYaFiY2Ndcxo1aqTatWsrOTlZ7dq1U3Jyspo3b66IiAjXnLi4OPXr1087duzQNddco+TkZLcaJXMGDhx41l7GjRun0aNHlxrfsmWLAgMDL35nL1JeXp569oyT1ZouX99DHq1dVJQnpzNO6enpOnTIs7XNZubzIkmFhXnq3DlBQUEZ8vPLKTe1za5fnl8zAAAAFyo3N/e853o9bG3fvl0xMTE6efKkgoKC9Omnn6pJkyZKTU2VzWZTaGio2/yIiAhlZGRIkjIyMtyCVsn2km3nmuNwOJSXl6eAgIBSPY0YMUKDBw923XY4HKpVq5batGmj4ODgi97ni5WWlqbnnpuu0NBYVaoU7dHaJ06kKTt7uhYsiFV0tGdrm83M50WSsrLWaPv2eWrefL7CwpqWm9pm1y/PrxkAAIALVXLV2/nwethq2LChUlNTdezYMX388cdKSEjQmjVrvNqT3W6X3W4vNW61WmW1ev0pk4+Pj5zOIjmdPioq8mw/Tuep2j4+PpfFvl4IM58XSXI6LSooKDTpeTevttn1y/NrBgAA4EJdyPsdr78zstlsqlevniSpdevW2rx5s6ZOnar7779fBQUFys7Odju7lZmZqcjISElSZGSkNm3a5FavZLXC0+f8dQXDzMxMBQcHn/GsFgAAAAB4wmX3PVvFxcXKz89X69at5efnp5UrV7q27dq1S/v27VNMTIwkKSYmRtu3b3f7nEhSUpKCg4PVpEkT15zTa5TMKakBAAAAAGbw6pmtESNGqHPnzqpdu7aOHz+uhQsX6ptvvtGyZcsUEhKivn37avDgwapataqCg4M1YMAAxcTEqF27dpKkTp06qUmTJnrooYc0YcIEZWRk6IUXXlBiYqLrMsAnnnhC06dP17Bhw/TII49o1apV+uijj7RkyRJv7joAAACACs6rYevQoUPq3bu3Dh48qJCQELVo0ULLli3TbbfdJkmaMmWKfHx81L17d+Xn5ysuLk5vvvmm6/6+vr5avHix+vXrp5iYGAUGBiohIUFjxoxxzYmOjtaSJUs0aNAgTZ06VTVr1tTbb7991mXfAQAAAMATvBq23nnnnXNu9/f314wZMzRjxoyzzqlTp46++uqrc9a55ZZbtHXr1jL1CAAAAABlcdl9ZgsAAAAAKgLCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAm8Or3bOHKdPjwYTkcDo/XTU9Pl9Pp9HhdAAAAoCwIW7ikDh8+rJ49+ykrK9/jtfPzc7V/f6ZCQjxfGwAAALhQhC1cUg6HQ1lZ+bLbhyggoJZHax89ukFO58tyOos8WhcAAAAoC8IWvCIgoJYCA6/2aM28vHSP1gMAAAAuBgtkAAAAAIAJOLMF4KIVFuYrPd28M4vBwcGqXr26afUBAADMQNgCcFEKCrKUnr5XAwaMl91uN+UxwsLsWrhwJoELAACUK4QtABelqChHTqdNNtsghYY28Hj9vLz9ysqaJIfDQdgCAADlCmELgEf4+9f0+KInJfJZzR8AAJRDLJABAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiALzVGKYWF+UpPTzeldnp6upxOpym1AQAAgMsJYQtuCgqylJ6+VwMGjJfdbvd4/fz8XO3fn6mQkHyP1wYAAAAuJ4QtuCkqypHTaZPNNkihoQ08Xv/o0Q1yOl+W01nk8doAAADA5YSwhTPy96+pwMCrPV43L8+cyxMBAACAyw0LZAAAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAm8GrbGjRun6667TpUrV1Z4eLjuvvtu7dq1y23OyZMnlZiYqLCwMAUFBal79+7KzMx0m7Nv3z7Fx8erUqVKCg8P19ChQ+V0Ot3mfPPNN7r22mtlt9tVr149zZ071+zdAwAAAHAF82rYWrNmjRITE7VhwwYlJSWpsLBQnTp1Um5urmvOoEGD9OWXX2rRokVas2aNDhw4oHvuuce1vaioSPHx8SooKND69es1b948zZ07VyNHjnTNSUtLU3x8vG699ValpqZq4MCBevTRR7Vs2bJLur8AAAAArhxWbz740qVL3W7PnTtX4eHhSklJ0U033aRjx47pnXfe0cKFC9WhQwdJ0pw5c9S4cWNt2LBB7dq10/Lly7Vz506tWLFCERERatWqlcaOHavhw4dr1KhRstlsmjVrlqKjozVp0iRJUuPGjbVu3TpNmTJFcXFxl3y/AQAAAFR8Xg1bf3Xs2DFJUtWqVSVJKSkpKiwsVGxsrGtOo0aNVLt2bSUnJ6tdu3ZKTk5W8+bNFRER4ZoTFxenfv36aceOHbrmmmuUnJzsVqNkzsCBA8/YR35+vvLz8123HQ6HJMnpdJa6PNEbiouLZbX6ymotlq+vZ/uxWg3ZbH6m1Da7Pr17p775vZ96vRcXF18Wv38AAODKdiHvRy6bsFVcXKyBAweqffv2atasmSQpIyNDNptNoaGhbnMjIiKUkZHhmnN60CrZXrLtXHMcDofy8vIUEBDgtm3cuHEaPXp0qR63bNmiwMDAsu+kh+Tl5alnzzhZreny9T3k0dqFhXnq3DlBQUEZ8vPL8Whts+vTu3fqm917UVGenM44paen69Ahz77eAQAALtTpH3n6O5dN2EpMTNSPP/6odevWebsVjRgxQoMHD3bddjgcqlWrltq0aaPg4GAvdnZKWlqanntuukJDY1WpUrRHa2dlrdH27fPUvPl8hYU19Whts+vTu3fqm937iRNpys6ergULYhUd7dnXOwAAwIUquertfFwWYat///5avHix1q5dq5o1a7rGIyMjVVBQoOzsbLezW5mZmYqMjHTN2bRpk1u9ktUKT5/z1xUMMzMzFRwcXOqsliTZ7XbZ7fZS41arVVar958yHx8fOZ1Fcjp9VFTk2X6cTosKCgpNqW12fXr3Tn3zez/1evfx8bksfv8AAMCV7ULej3h1NULDMNS/f399+umnWrVqVam/Wrdu3Vp+fn5auXKla2zXrl3at2+fYmJiJEkxMTHavn272+VFSUlJCg4OVpMmTVxzTq9RMqekBgAAAAB4mlf/TJyYmKiFCxfq888/V+XKlV2fsQoJCVFAQIBCQkLUt29fDR48WFWrVlVwcLAGDBigmJgYtWvXTpLUqVMnNWnSRA899JAmTJigjIwMvfDCC0pMTHSdnXriiSc0ffp0DRs2TI888ohWrVqljz76SEuWLPHavgMAAACo2Lx6ZmvmzJk6duyYbrnlFtWoUcP18+GHH7rmTJkyRXfccYe6d++um266SZGRkfrkk09c2319fbV48WL5+voqJiZGDz74oHr37q0xY8a45kRHR2vJkiVKSkpSy5YtNWnSJL399tss+w4AAADANF49s2UYxt/O8ff314wZMzRjxoyzzqlTp46++uqrc9a55ZZbtHXr1gvuEQAAAADKwqtntgAAAACgoiJsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYAKrtxsAgL9TWJiv9PR0U2oHBwerevXqptQGAABXNsIWgMtaQUGW0tP3asCA8bLb7R6vHxZm18KFMwlcAADA4whbAC5rRUU5cjptstkGKTS0gUdr5+XtV1bWJDkcDsIWAADwOMIWgHLB37+mAgOv9njd/HyPlwQAAJDEAhkAAAAAYArCFgAAAACYwKtha+3atbrzzjsVFRUli8Wizz77zG27YRgaOXKkatSooYCAAMXGxmrPnj1uc44cOaJevXopODhYoaGh6tu3r3JyctzmbNu2TTfeeKP8/f1Vq1YtTZgwwexdAwAAAHCF82rYys3NVcuWLTVjxowzbp8wYYKmTZumWbNmaePGjQoMDFRcXJxOnjzpmtOrVy/t2LFDSUlJWrx4sdauXavHH3/ctd3hcKhTp06qU6eOUlJSNHHiRI0aNUqzZ882ff8AAAAAXLm8ukBG586d1blz5zNuMwxDr7/+ul544QV17dpVkjR//nxFRETos88+U48ePfTTTz9p6dKl2rx5s9q0aSNJeuONN9SlSxe99tprioqK0oIFC1RQUKB3331XNptNTZs2VWpqqiZPnuwWygAAAADAky7b1QjT0tKUkZGh2NhY11hISIjatm2r5ORk9ejRQ8nJyQoNDXUFLUmKjY2Vj4+PNm7cqG7duik5OVk33XSTbDaba05cXJxeffVVHT16VFWqVCn12Pn5+co/bYkyh8MhSXI6nXI6nWbs7gUpLi6W1eorq7VYvr6e7cdqNWSz+ZlS2+z69O6d+uW791O/S8XFxZfF7zYAALj8Xch7hss2bGVkZEiSIiIi3MYjIiJc2zIyMhQeHu623Wq1qmrVqm5zoqOjS9Uo2XamsDVu3DiNHj261PiWLVsUGBhYxj3ynLy8PPXsGSerNV2+voc8WruwME+dOycoKChDfn45f3+Hy6g+vXunfnnuvagoT05nnNLT03XokGd/lwAAQMWUm5t73nMv27DlTSNGjNDgwYNdtx0Oh2rVqqU2bdooODjYi52dkpaWpueem67Q0FhVqhT993e4AFlZa7R9+zw1bz5fYWFNPVrb7Pr07p365bn3EyfSlJ09XQsWxJb6owwAAMCZlFz1dj4u27AVGRkpScrMzFSNGjVc45mZmWrVqpVrzl//Gu10OnXkyBHX/SMjI5WZmek2p+R2yZy/stvtstvtpcatVqusVu8/ZT4+PnI6i+R0+qioyLP9OJ0WFRQUmlLb7Pr07p365bv3U79LPj4+l8XvNgAAuPxdyHuGy/Z7tqKjoxUZGamVK1e6xhwOhzZu3KiYmBhJUkxMjLKzs5WSkuKas2rVKhUXF6tt27auOWvXrlVhYaFrTlJSkho2bHjGSwgBAAAAwBO8GrZycnKUmpqq1NRUSacuj0tNTdW+fftksVg0cOBAvfTSS/riiy+0fft29e7dW1FRUbr77rslSY0bN9btt9+uxx57TJs2bdJ3332n/v37q0ePHoqKipIk9ezZUzabTX379tWOHTv04YcfaurUqW6XCQIAAACAp3n1upktW7bo1ltvdd0uCUAJCQmaO3euhg0bptzcXD3++OPKzs7WDTfcoKVLl8rf3991nwULFqh///7q2LGjfHx81L17d02bNs21PSQkRMuXL1diYqJat26tatWqaeTIkSz7DgAAAMBUXg1bt9xyiwzDOOt2i8WiMWPGaMyYMWedU7VqVS1cuPCcj9OiRQt9++23Ze4TAAAAAC7UZfuZLQAAAAAozwhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmMDq7QYAwJsKC/OVnp5uSu3g4GBVr17dlNoAAODyR9gCcMUqKMhSevpeDRgwXna73eP1w8LsWrhwJoELAIArFGELwBWrqChHTqdNNtsghYY28GjtvLz9ysqaJIfDQdgCAOAKRdgCcMXz96+pwMCrPV43P9/jJQEAQDnCAhkAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAn4UmMAKKcOHz4sh8NhSu3g4GBVr17dlNoAAFwpCFsAYJLCwnylp6ebUjsrK0tDh76k48cNU+pXrixNnPgfhYWFmVKfMAcAuBIQtgDABAUFWUpP36sBA8bLbrd7vH5+fq72789Uw4ZTVLny1R6t7XBs19atz+jhh18wpXdJCguza+HCmQQuAECFRtgCABMUFeXI6bTJZhuk0NAGHq9/9OgGOZ0vy2qtocBAz4atvLx0U3vPy9uvrKxJcjgchC0AQIVG2AIAE/n71/R4GJJOBSKzmdW7JOXnm1IWAIDLCqsRAgAAAIAJCFsAAAAAYALCFgAAAACYgM9sAQAuOTOXxWdZeQDA5YKwBQC4pMxeFp9l5QEAlwvCFgDgkjJzWXyWlQcAXE4IWwAArzBraXmWlQcAXC5YIAMAAAAATMCZLQBAhWLm4huSVFBQIJvNZkptFvcAgIqFsAUAqDDMXnyjsDBfBw6k6R//qCer1fP/CWVxDwCoWAhbAIAKw8zFNyTp6NENyst7Wb6+T7G4BwDgbxG2AAAVjlmLb+TlpZtaPyeHSyABoCIhbAEAcBngEkgAqHiuqLA1Y8YMTZw4URkZGWrZsqXeeOMNXX/99d5uCwAALoEEgAroiglbH374oQYPHqxZs2apbdu2ev311xUXF6ddu3YpPDzc2+0BACCJSyDPhksgAZRHV0zYmjx5sh577DE9/PDDkqRZs2ZpyZIlevfdd/Xss896uTsAAMqv8n4JZOXK0sSJ/1FYWJjHa0vmBkUza5tdn5CLK8EVEbYKCgqUkpKiESNGuMZ8fHwUGxur5OTkUvPz8/OVn5/vun3s2DFJ0pEjR+R0Os1v+G84HA5ZLMXKy/tJksOjtQsKfpWfn48KCnYpN9fz+2pmfXr3Tn16v/S1za5P796pX557z8v7QRaLvyyWu2SzRXm0tiQVFPwsp/OAiovjPV4/L+83bd8+S337Pm9KqHA6C5SRsU81akTL19e33NS+FPUrV7Zo5MjBqlKlisdro2IKCQlRaGiot9uQw3Hq/bdhGH8712Kcz6xy7sCBA/rHP/6h9evXKyYmxjU+bNgwrVmzRhs3bnSbP2rUKI0ePfpStwkAAACgnNi/f79q1qx5zjlXxJmtCzVixAgNHjzYdbu4uFhHjhxRWFiYLBaL1/pyOByqVauW9u/fr+DgYK/1Ac/j2FZcHNuKi2NbcXFsKyaOa8V1qY+tYRg6fvy4oqL+/kz7FRG2qlWrJl9fX2VmZrqNZ2ZmKjIystR8u91e6przy+GUZYng4GD+T6KC4thWXBzbiotjW3FxbCsmjmvFdSmPbUhIyHnN8zG5j8uCzWZT69attXLlStdYcXGxVq5c6XZZIQAAAAB4yhVxZkuSBg8erISEBLVp00bXX3+9Xn/9deXm5rpWJwQAAAAAT7piwtb999+vw4cPa+TIkcrIyFCrVq20dOlSRUREeLu182a32/Xiiy+asqwuvItjW3FxbCsujm3FxbGtmDiuFdflfGyviNUIAQAAAOBSuyI+swUAAAAAlxphCwAAAABMQNgCAAAAABMQtgAAAADABIStcmTGjBmqW7eu/P391bZtW23atMnbLeFvrF27VnfeeaeioqJksVj02WefuW03DEMjR45UjRo1FBAQoNjYWO3Zs8dtzpEjR9SrVy8FBwcrNDRUffv2VU5OziXcC/zVuHHjdN1116ly5coKDw/X3XffrV27drnNOXnypBITExUWFqagoCB179691Ber79u3T/Hx8apUqZLCw8M1dOhQOZ3OS7kr+IuZM2eqRYsWri/GjImJ0ddff+3aznGtGMaPHy+LxaKBAwe6xji25dOoUaNksVjcfho1auTaznEt3/744w89+OCDCgsLU0BAgJo3b64tW7a4tpeH91GErXLiww8/1ODBg/Xiiy/q+++/V8uWLRUXF6dDhw55uzWcQ25urlq2bKkZM2accfuECRM0bdo0zZo1Sxs3blRgYKDi4uJ08uRJ15xevXppx44dSkpK0uLFi7V27Vo9/vjjl2oXcAZr1qxRYmKiNmzYoKSkJBUWFqpTp07Kzc11zRk0aJC+/PJLLVq0SGvWrNGBAwd0zz33uLYXFRUpPj5eBQUFWr9+vebNm6e5c+dq5MiR3tgl/D81a9bU+PHjlZKSoi1btqhDhw7q2rWrduzYIYnjWhFs3rxZb731llq0aOE2zrEtv5o2baqDBw+6ftatW+faxnEtv44ePar27dvLz89PX3/9tXbu3KlJkyapSpUqrjnl4n2UgXLh+uuvNxITE123i4qKjKioKGPcuHFe7AoXQpLx6aefum4XFxcbkZGRxsSJE11j2dnZht1uN95//33DMAxj586dhiRj8+bNrjlff/21YbFYjD/++OOS9Y5zO3TokCHJWLNmjWEYp46jn5+fsWjRItecn376yZBkJCcnG4ZhGF999ZXh4+NjZGRkuObMnDnTCA4ONvLz8y/tDuCcqlSpYrz99tsc1wrg+PHjRv369Y2kpCTj5ptvNp5++mnDMPidLc9efPFFo2XLlmfcxnEt34YPH27ccMMNZ91eXt5HcWarHCgoKFBKSopiY2NdYz4+PoqNjVVycrIXO8PFSEtLU0ZGhttxDQkJUdu2bV3HNTk5WaGhoWrTpo1rTmxsrHx8fLRx48ZL3jPO7NixY5KkqlWrSpJSUlJUWFjodmwbNWqk2rVrux3b5s2bu32xelxcnBwOh+ssCryrqKhIH3zwgXJzcxUTE8NxrQASExMVHx/vdgwlfmfLuz179igqKkpXXXWVevXqpX379kniuJZ3X3zxhdq0aaP77rtP4eHhuuaaa/R///d/ru3l5X0UYasc+PPPP1VUVOT2fwSSFBERoYyMDC91hYtVcuzOdVwzMjIUHh7utt1qtapq1aoc+8tEcXGxBg4cqPbt26tZs2aSTh03m82m0NBQt7l/PbZnOvYl2+A927dvV1BQkOx2u5544gl9+umnatKkCce1nPvggw/0/fffa9y4caW2cWzLr7Zt22ru3LlaunSpZs6cqbS0NN144406fvw4x7Wc27t3r2bOnKn69etr2bJl6tevn5566inNmzdPUvl5H2W9JI8CABVUYmKifvzxR7fPCKB8a9iwoVJTU3Xs2DF9/PHHSkhI0Jo1a7zdFi7C/v379fTTTyspKUn+/v7ebgce1LlzZ9e/W7RoobZt26pOnTr66KOPFBAQ4MXOcLGKi4vVpk0bvfLKK5Kka665Rj/++KNmzZqlhIQEL3d3/jizVQ5Uq1ZNvr6+pVbPyczMVGRkpJe6wsUqOXbnOq6RkZGlFkFxOp06cuQIx/4y0L9/fy1evFirV69WzZo1XeORkZEqKChQdna22/y/HtszHfuSbfAem82mevXqqXXr1ho3bpxatmypqVOnclzLsZSUFB06dEjXXnutrFarrFar1qxZo2nTpslqtSoiIoJjW0GEhoaqQYMG+uWXX/idLedq1KihJk2auI01btzYdZloeXkfRdgqB2w2m1q3bq2VK1e6xoqLi7Vy5UrFxMR4sTNcjOjoaEVGRrodV4fDoY0bN7qOa0xMjLKzs5WSkuKas2rVKhUXF6tt27aXvGecYhiG+vfvr08//VSrVq1SdHS02/bWrVvLz8/P7dju2rVL+/btczu227dvd/uPQFJSkoKDg0v9xwXeVVxcrPz8fI5rOdaxY0dt375dqamprp82bdqoV69ern9zbCuGnJwc/frrr6pRowa/s+Vc+/btS32tyu7du1WnTh1J5eh91CVZhgMX7YMPPjDsdrsxd+5cY+fOncbjjz9uhIaGuq2eg8vP8ePHja1btxpbt241JBmTJ082tm7daqSnpxuGYRjjx483QkNDjc8//9zYtm2b0bVrVyM6OtrIy8tz1bj99tuNa665xti4caOxbt06o379+sYDDzzgrV2CYRj9+vUzQkJCjG+++cY4ePCg6+fEiROuOU888YRRu3ZtY9WqVcaWLVuMmJgYIyYmxrXd6XQazZo1Mzp16mSkpqYaS5cuNapXr26MGDHCG7uE/+fZZ5811qxZY6SlpRnbtm0znn32WcNisRjLly83DIPjWpGcvhqhYXBsy6shQ4YY33zzjZGWlmZ89913RmxsrFGtWjXj0KFDhmFwXMuzTZs2GVar1Xj55ZeNPXv2GAsWLDAqVapkvPfee6455eF9FGGrHHnjjTeM2rVrGzabzbj++uuNDRs2eLsl/I3Vq1cbkkr9JCQkGIZxatnS//znP0ZERIRht9uNjh07Grt27XKrkZWVZTzwwANGUFCQERwcbDz88MPG8ePHvbA3KHGmYyrJmDNnjmtOXl6e8eSTTxpVqlQxKlWqZHTr1s04ePCgW53ffvvN6Ny5sxEQEGBUq1bNGDJkiFFYWHiJ9wane+SRR4w6deoYNpvNqF69utGxY0dX0DIMjmtF8tewxbEtn+6//36jRo0ahs1mM/7xj38Y999/v/HLL7+4tnNcy7cvv/zSaNasmWG3241GjRoZs2fPdtteHt5HWQzDMC7NOTQAAAAAuHLwmS0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQBAufbbb7/JYrEoNTXV260AAOCGsAUA8DqLxXLOn1GjRnm7xTP65Zdf9PDDD6tmzZqy2+2Kjo7WAw88oC1btlzSPgicAHB5snq7AQAADh486Pr3hx9+qJEjR2rXrl2usaCgIG+0dU5btmxRx44d1axZM7311ltq1KiRjh8/rs8//1xDhgzRmjVrvN0iAMDLOLMFAPC6yMhI109ISIgsFovrdnh4uCZPnuw6e9SqVSstXbr0rLWKior0yCOPqFGjRtq3b58k6fPPP9e1114rf39/XXXVVRo9erScTqfrPhaLRW+//ba6deumSpUqqX79+vriiy/O+hiGYahPnz6qX7++vv32W8XHx+vqq69Wq1at9OKLL+rzzz93zd2+fbs6dOiggIAAhYWF6fHHH1dOTo5r+y233KKBAwe61b/77rvVp08f1+26devqlVde0SOPPKLKlSurdu3amj17tmt7dHS0JOmaa66RxWLRLbfccs7nGwBwaRC2AACXtalTp2rSpEl67bXXtG3bNsXFxemuu+7Snj17Ss3Nz8/Xfffdp9TUVH377beqXbu2vv32W/Xu3VtPP/20du7cqbfeektz587Vyy+/7Hbf0aNH61//+pe2bdumLl26qFevXjpy5MgZe0pNTdWOHTs0ZMgQ+fiU/k9paGioJCk3N1dxcXGqUqWKNm/erEWLFmnFihXq37//BT8PkyZNUps2bbR161Y9+eST6tevn+vs36ZNmyRJK1as0MGDB/XJJ59ccH0AgOcRtgAAl7XXXntNw4cPV48ePdSwYUO9+uqratWqlV5//XW3eTk5OYqPj9fhw4e1evVqVa9eXdKpEPXss88qISFBV111lW677TaNHTtWb731ltv9+/TpowceeED16tXTK6+8opycHFeI+auSoNeoUaNz9r5w4UKdPHlS8+fPV7NmzdShQwdNnz5d//3vf5WZmXlBz0OXLl305JNPql69eho+fLiqVaum1atXS5JrX8PCwhQZGamqVateUG0AgDn4zBYA4LLlcDh04MABtW/f3m28ffv2+uGHH9zGHnjgAdWsWVOrVq1SQECAa/yHH37Qd99953Ymq6ioSCdPntSJEydUqVIlSVKLFi1c2wMDAxUcHKxDhw6dsS/DMM6r/59++kktW7ZUYGCgW+/FxcXatWuXIiIizqvOX/sruczybP0BAC4PnNkCAFQIXbp00bZt25ScnOw2npOTo9GjRys1NdX1s337du3Zs0f+/v6ueX5+fm73s1gsKi4uPuNjNWjQQJL0888/X3TfPj4+pcJbYWFhqXkX0h8A4PJA2AIAXLaCg4MVFRWl7777zm38u+++U5MmTdzG+vXrp/Hjx+uuu+5yWwnw2muv1a5du1SvXr1SP2f6vNX5aNWqlZo0aaJJkyadMfBkZ2dLkho3bqwffvhBubm5br37+PioYcOGkk5dAnj6aoxFRUX68ccfL6gfm83mui8A4PJB2AIAXNaGDh2qV199VR9++KF27dqlZ599VqmpqXr66adLzR0wYIBeeukl3XHHHVq3bp0kaeTIkZo/f75Gjx6tHTt26KefftIHH3ygF154ocw9WSwWzZkzR7t379aNN96or776Snv37tW2bdv08ssvq2vXrpKkXr16yd/fXwkJCfrxxx+1evVqDRgwQA899JDrEsIOHTpoyZIlWrJkiX7++Wf169fPFdbOV3h4uAICArR06VJlZmbq2LFjZd43AIDnELYAAJe1p556SoMHD9aQIUPUvHlzLV26VF988YXq169/xvkDBw7U6NGj1aVLF61fv15xcXFavHixli9fruuuu07t2rXTlClTVKdOnYvq6/rrr9eWLVtUr149PfbYY2rcuLHuuusu7dixw7V4R6VKlbRs2TIdOXJE1113ne6991517NhR06dPd9V55JFHlJCQoN69e+vmm2/WVVddpVtvvfWCerFarZo2bZreeustRUVFucIeAMC7LMb5fsoXAAAAAHDeOLMFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYIL/D0VUN53QSRy8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "\n",
    "# Assuming contents is your loaded .pkl data and is a list of lists\n",
    "# where each sublist contains document objects with a page_content attribute\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Disable the check for all special tokens\n",
    "    num_tokens = len(encoding.encode(string, disallowed_special=()))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# Extracting page_content from each document object\n",
    "docs_texts = []\n",
    "for sublist in contents:\n",
    "    for doc in sublist:  # Assuming each element in sublist is a document object\n",
    "        docs_texts.append(doc.page_content)\n",
    "\n",
    "# Calculate the number of tokens for each document's page_content\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 2 0 2\\n\\nl u J\\n\\n0 1\\n\\n] L C . s c [\\n\\n2 v 5 8 1 8 1 . 5 0 3 2 : v i X r a\\n\\nSyntax and Semantics Meet in the Middle: Probing the Syntax-Semantics Interface of LMs Through Agentivity\\n\\nLindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig Language Technologies Institute Carnegie Mellon University {ltjuatja, mengyan3, lsl, gneubig}@cs.cmu.edu\\n\\n1\\n\\nAbstract', 'Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models han- dle the interactions in meaning across words and larger syntactic formsi.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitiz- ing the unique linguistic properties', 'of a sub- set of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level pri- ors given a specific syntactic context. Over- all, GPT-3 text-davinci-003 performs extremely well across all experiments, outper- forming all other models tested by far. In fact, the results are even better correlated with hu- man judgements than both', 'syntactic and seman- tic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and dis- covery than select corpora for certain tasks.1', 'Introduction\\n\\nan agent in the writing event. On the other hand, the subject of (1b), this passage, doesnt do any writingit is what is created in the event of writ- ing. In contrast to this author, this passage is a patient. The agent and patient roles are not dis- crete categories, but rather prototypes on opposite ends of a continuum. These protoroles have a number of contributing properties such as causing an event for agents and undergoing change of state for patients (Dowty, 1991).', 'The contrast between the minimal pair in (1) suggests that there are lexical semantic properties of the subjects that give rise to these two distinct readings: one that describes how the subject gen- erally does an action as in (1a), and another that describes how an event generally unfolds when the subject undergoes an action as in (1b). Intuitively, a speaker may know from the meaning of author that authors are animate, have some degree of voli- tion, and typically write things, whereas', 'passages (of text) are inanimate, have no volition, and are typically written. The knowledge of these aspects of meaning must somehow interact with the syn- tactic form of the sentences in (1) to disambiguate between the two possible readings, and an agent or patient role for the subject follows from the mean- ing of the statement as a whole.', 'Consider the English sentences in (1) below:\\n\\nNow consider the (somewhat unusual) sentences\\n\\n(1)\\n\\na. This author writes easily.\\n\\nin (2) which use the transitive form of write:\\n\\nb. This passage writes easily.\\n\\n(2)\\n\\na. Something writes this author easily.', 'These sentences display an interesting property of certain optionally transitive verbs in English. Al- though they share an identical surface syntactic structurea noun phrase in subject position fol- lowed by the intransitive form of the verb and an adverb phrase modifying the verbthey entail very different things about the roles of their subjects.\\n\\nThe subject of (1a) is someone that does the action of writing; in other words, this author is\\n\\nb. This passage writes something easily.', 'At first glance, the above sentences (with the same sense of write as in 1) are infelicitous unless we imagine some obscure context where this author is something like a character in a text and this pas- sage is somehow anthropomorphized and capable of writing; these contexts go against our natural intuitions of the semantics of passage and au- thor.2 Unlike the syntactic form of the sentences\\n\\n1Code\\n\\nis\\n\\navailable\\n\\nlindiatjuatja/lm_sem\\n\\nat https://github.com/']\n"
     ]
    }
   ],
   "source": [
    "print(docs_texts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevea/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|| 3/3 [00:01<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"Salesforce/SFR-Embedding-Mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevea/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|| 3/3 [00:01<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"Salesforce/SFR-Embedding-Mistral\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.7783623 -0.5919665  4.3199406 ...  7.873832  -7.350942   2.5289717]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming docs_texts \n",
    "embeddings = np.array([model.encode(doc) for doc in docs_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Modules\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "print(\"Imported Modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "# Previously, embed_cluster_texts was designed to take texts, embed them, and then cluster.\n",
    "# Now, you have pre-computed embeddings, so you modify your approach to directly use those embeddings.\n",
    "\n",
    "def cluster_texts_with_precomputed_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Clusters texts using precomputed embeddings and returns a DataFrame with cluster labels.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: numpy.ndarray, the precomputed embeddings for the texts.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    # Assuming perform_clustering is a function you've defined that expects embeddings\n",
    "    # and returns cluster labels (modify this part according to your actual clustering implementation)\n",
    "    cluster_labels = perform_clustering(embeddings, dim=10, threshold=0.1)  # Example parameters\n",
    "\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = docs_texts  # Store original texts\n",
    "    df[\"embd\"] = list(embeddings)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = model.encode(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of information about language Technology Institute Faculty at Carnegie Mellon University (CMU).  \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "    #   Split formatted_txt if it's too long\n",
    "        if len(formatted_txt) > 2000:  # Adjust the number based on your template length\n",
    "            text_chunks = split_text(formatted_txt, 1500)  # Example chunk size, adjust as needed\n",
    "            summary_parts = []\n",
    "            for chunk in text_chunks:\n",
    "                # Update the context in your template accordingly\n",
    "                temp_prompt = template.format(context=chunk)\n",
    "                # Invoke the chain with the updated prompt for each chunk\n",
    "                summary_part = chain.invoke({\"context\": temp_prompt})\n",
    "                summary_parts.append(summary_part)\n",
    "            # Combine the individual summaries into one\n",
    "            combined_summary = ' '.join(summary_parts)\n",
    "            summaries.append(combined_summary)\n",
    "        else:\n",
    "            # Proceed as before if the text is within the limit\n",
    "            summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "# Function to split text into smaller parts\n",
    "def split_text(text, chunk_size):\n",
    "    # Split the text into chunks of approximately 'chunk_size' characters\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered_texts = cluster_texts_with_precomputed_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  3 2 0 2\\n\\nl u J\\n\\n0 1\\n\\n] L C . s c [\\n\\n2 ...   \n",
      "1  Recent advances in large language models have ...   \n",
      "2  of a sub- set of optionally transitive English...   \n",
      "3  syntactic and seman- tic corpus statistics. Th...   \n",
      "4  Introduction\\n\\nan agent in the writing event....   \n",
      "5  The contrast between the minimal pair in (1) s...   \n",
      "6  passages (of text) are inanimate, have no voli...   \n",
      "7  Consider the English sentences in (1) below:\\n...   \n",
      "\n",
      "                                                embd  cluster  \n",
      "0  [5.068998, -2.2683353, 2.1725945, -1.4779862, ...  [214.0]  \n",
      "1  [5.7855673, -1.0487652, 0.7265795, -1.2334801,...  [214.0]  \n",
      "2  [3.226253, -1.0835481, 1.3811967, 0.23921128, ...  [564.0]  \n",
      "3  [6.0345435, -3.3031888, 0.27811188, -1.2451488...  [214.0]  \n",
      "4  [5.4009376, -0.12218815, 2.6706438, -1.7392813...  [913.0]  \n",
      "5  [2.4489007, -1.1088563, -1.977748, -3.9637551,...  [913.0]  \n",
      "6  [4.161246, -0.48568708, 1.7780865, -3.9184294,...  [913.0]  \n",
      "7  [3.8849313, 0.29882342, -1.5985764, -0.2426184...  [913.0]  \n"
     ]
    }
   ],
   "source": [
    "print(df_clustered_texts[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "file_path = 'df_clustered_texts.csv'\n",
    "df_clustered_texts.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1097 clusters--\n"
     ]
    }
   ],
   "source": [
    "df_clusters = df_clustered_texts.copy()\n",
    "# Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "expanded_list = []\n",
    "\n",
    "# Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "for index, row in df_clusters.iterrows():\n",
    "    for cluster in row[\"cluster\"]:\n",
    "        expanded_list.append(\n",
    "            {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "        )\n",
    "\n",
    "# Create a new DataFrame from the expanded list\n",
    "expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "# Retrieve unique cluster identifiers for processing\n",
    "all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "print(f\"--Generated {len(all_clusters)} clusters--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5272.34 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      23.30 ms /   143 runs   (    0.16 ms per token,  6138.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8401.76 ms /   491 tokens (   17.11 ms per token,    58.44 tokens per second)\n",
      "llama_print_timings:        eval time =   24004.57 ms /   142 runs   (  169.05 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:       total time =   32790.43 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      39.86 ms /   256 runs   (    0.16 ms per token,  6422.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6294.80 ms /   374 tokens (   16.83 ms per token,    59.41 tokens per second)\n",
      "llama_print_timings:        eval time =   43439.81 ms /   255 runs   (  170.35 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =   50459.72 ms /   629 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      36.91 ms /   237 runs   (    0.16 ms per token,  6420.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5767.89 ms /   376 tokens (   15.34 ms per token,    65.19 tokens per second)\n",
      "llama_print_timings:        eval time =   40179.32 ms /   236 runs   (  170.25 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =   46616.70 ms /   612 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      31.80 ms /   206 runs   (    0.15 ms per token,  6478.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6653.19 ms /   428 tokens (   15.54 ms per token,    64.33 tokens per second)\n",
      "llama_print_timings:        eval time =   35446.93 ms /   205 runs   (  172.91 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   42693.83 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      46.62 ms /   256 runs   (    0.18 ms per token,  5491.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6004.14 ms /   367 tokens (   16.36 ms per token,    61.12 tokens per second)\n",
      "llama_print_timings:        eval time =   44077.77 ms /   255 runs   (  172.85 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   50945.40 ms /   622 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      44.51 ms /   256 runs   (    0.17 ms per token,  5751.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6060.30 ms /   387 tokens (   15.66 ms per token,    63.86 tokens per second)\n",
      "llama_print_timings:        eval time =   44080.38 ms /   255 runs   (  172.86 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   50952.67 ms /   642 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      43.96 ms /   256 runs   (    0.17 ms per token,  5823.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8006.21 ms /   481 tokens (   16.64 ms per token,    60.08 tokens per second)\n",
      "llama_print_timings:        eval time =   43794.28 ms /   255 runs   (  171.74 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:       total time =   52608.85 ms /   736 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      41.86 ms /   256 runs   (    0.16 ms per token,  6115.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7971.71 ms /   512 tokens (   15.57 ms per token,    64.23 tokens per second)\n",
      "llama_print_timings:        eval time =   44906.57 ms /   256 runs   (  175.42 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   53711.75 ms /   768 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      44.01 ms /   256 runs   (    0.17 ms per token,  5817.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8245.60 ms /   535 tokens (   15.41 ms per token,    64.88 tokens per second)\n",
      "llama_print_timings:        eval time =   45239.19 ms /   255 runs   (  177.41 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:       total time =   54332.80 ms /   790 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      45.74 ms /   256 runs   (    0.18 ms per token,  5596.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6838.90 ms /   458 tokens (   14.93 ms per token,    66.97 tokens per second)\n",
      "llama_print_timings:        eval time =   45304.20 ms /   255 runs   (  177.66 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:       total time =   53033.80 ms /   713 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      48.89 ms /   256 runs   (    0.19 ms per token,  5236.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6521.96 ms /   428 tokens (   15.24 ms per token,    65.62 tokens per second)\n",
      "llama_print_timings:        eval time =   45349.30 ms /   255 runs   (  177.84 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   52754.51 ms /   683 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      43.47 ms /   256 runs   (    0.17 ms per token,  5888.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6651.23 ms /   433 tokens (   15.36 ms per token,    65.10 tokens per second)\n",
      "llama_print_timings:        eval time =   44249.15 ms /   255 runs   (  173.53 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   51787.70 ms /   688 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      39.41 ms /   256 runs   (    0.15 ms per token,  6496.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6984.51 ms /   449 tokens (   15.56 ms per token,    64.29 tokens per second)\n",
      "llama_print_timings:        eval time =   43953.15 ms /   255 runs   (  172.37 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:       total time =   51760.66 ms /   704 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8402.07 ms\n",
      "llama_print_timings:      sample time =      40.20 ms /   256 runs   (    0.16 ms per token,  6367.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7373.33 ms /   476 tokens (   15.49 ms per token,    64.56 tokens per second)\n",
      "llama_print_timings:        eval time =   44072.58 ms /   255 runs   (  172.83 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:       total time =   52292.18 ms /   731 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     temp_prompt \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mformat(context\u001b[38;5;241m=\u001b[39mchunk)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Invoke the chain with the updated prompt for each chunk\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     summary_part \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     summary_parts\u001b[38;5;241m.\u001b[39mappend(summary_part)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Combine the individual summaries into one\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/runnables/base.py:2089\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2088\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2089\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2092\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:246\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    244\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 246\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    257\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:541\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    535\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    540\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:714\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    699\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m         )\n\u001b[1;32m    701\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    702\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    703\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m         )\n\u001b[1;32m    713\u001b[0m     ]\n\u001b[0;32m--> 714\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:578\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    577\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    579\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:565\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    557\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    562\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 565\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    569\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    573\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    574\u001b[0m         )\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1153\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1152\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1156\u001b[0m     )\n\u001b[1;32m   1157\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_community/llms/llamacpp.py:288\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombined_text_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/langchain_community/llms/llamacpp.py:341\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    340\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 341\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpart\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/llama_cpp/llama.py:1000\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m    998\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1000\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_eos\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/llama_cpp/llama.py:682\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    684\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    685\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    686\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    700\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    701\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/llama_cpp/llama.py:522\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    518\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    520\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    521\u001b[0m )\n\u001b[0;32m--> 522\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-rag/lib/python3.12/site-packages/llama_cpp/_internals.py:311\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to split text into smaller parts\n",
    "def split_text(text, chunk_size):\n",
    "    # Split the text into chunks of approximately 'chunk_size' characters\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "level = 1\n",
    "# Summarization\n",
    "template = \"\"\"Here is a sub-set of information about language Technology Institute Faculty at Carnegie Mellon University (CMU).  \n",
    "Give a detailed summary of the documentation provided.\n",
    "Documentation:\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# Format text within each cluster for summarization\n",
    "summaries = []\n",
    "\n",
    "for i in all_clusters:\n",
    "    df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "    formatted_txt = fmt_txt(df_cluster)\n",
    "#   Split formatted_txt if it's too long\n",
    "    if len(formatted_txt) > 2000:  # Adjust the number based on your template length\n",
    "        text_chunks = split_text(formatted_txt, 1500)  # Example chunk size, adjust as needed\n",
    "        summary_parts = []\n",
    "        for chunk in text_chunks:\n",
    "            # Update the context in your template accordingly\n",
    "            temp_prompt = template.format(context=chunk)\n",
    "            # Invoke the chain with the updated prompt for each chunk\n",
    "            summary_part = chain.invoke({\"context\": temp_prompt})\n",
    "            summary_parts.append(summary_part)\n",
    "        # Combine the individual summaries into one\n",
    "        combined_summary = ' '.join(summary_parts)\n",
    "        summaries.append(combined_summary)\n",
    "    else:\n",
    "        # Proceed as before if the text is within the limit\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "        \n",
    "# Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "df_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"summaries\": summaries,\n",
    "        \"level\": [level] * len(summaries),\n",
    "        \"cluster\": list(all_clusters),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming llm and other necessary imports and variables (like template, chain) are already defined\n",
    "\n",
    "def recursive_summarization(texts: List[str], level: int, n_levels: int) -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    if level > n_levels:\n",
    "        return {}  # Base case: reached the maximum level of recursion\n",
    "\n",
    "    # Your summarization logic here (similar to what you've already defined)\n",
    "    # This includes: expanding df, creating summaries, etc.\n",
    "    # Note: You might need to adjust it to accept a list of texts as input\n",
    "\n",
    "    # For demonstration, let's use placeholders for df_clusters and df_summary\n",
    "    df_clusters = None  # Placeholder for the DataFrame that includes clusters\n",
    "    df_summary = None  # Placeholder for the DataFrame that includes summaries\n",
    "    \n",
    "    # Here, you would perform the summarization logic that you've described\n",
    "    # This includes expanding the DataFrame, summarizing each cluster, etc.\n",
    "    # Since the detailed implementation depends on the specifics of your setup, we'll focus on the recursion logic\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results = {level: (df_clusters, df_summary)}\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique() if df_summary is not None else 0\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist() if df_summary is not None else []\n",
    "        next_level_results = recursive_summarization(new_texts, level + 1, n_levels)\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize your recursion\n",
    "n_levels = 3  # Desired number of recursion levels\n",
    "initial_texts = df_clustered_texts[\"text\"].tolist()  # Assuming df_clustered_texts is your initial DataFrame\n",
    "results = recursive_summarization(initial_texts, level=1, n_levels=n_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37655\n"
     ]
    }
   ],
   "source": [
    "print(len(docs_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_texts = docs_texts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37655\n"
     ]
    }
   ],
   "source": [
    "print(len(leaf_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5272.34 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    all_texts.extend(summaries)\n",
    "    \n",
    "def mistral_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents using the Mistral model.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (List[str]): A list of texts to embed.\n",
    "\n",
    "    Returns:\n",
    "    - List of embeddings as numpy arrays.\n",
    "    \"\"\"\n",
    "    # Ensure texts is a list for batch processing\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(texts)  # convert_to_tensor=False to get numpy array\n",
    "    return embeddings\n",
    "\n",
    "# Build the vector store with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=mistral_embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# # Initialize Llama model\n",
    "# n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "# n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "# llm = LlamaCpp(\n",
    "#     model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "#     n_gpu_layers=n_gpu_layers,\n",
    "#     n_batch=n_batch,\n",
    "#     n_ctx=2048,\n",
    "#     f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "# Define your custom RAG prompt\n",
    "prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=['context', 'question'],\n",
    "        template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use one sentence maximum and keep the answer CONCISE. Keep the answer CONCISE.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define post-processing function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Set up the RAG chain\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Load questions from questions.txt\n",
    "with open(\"SubmissionData/test/questions.txt\", \"r\") as f:\n",
    "    questions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Output directory for answers\n",
    "output_dir = \"SubmissionData/system_outputs/\"\n",
    "answer_file = os.path.join(output_dir, \"Raptor.txt\")\n",
    "\n",
    "# Run the question-answering loop and save answers\n",
    "answers = []\n",
    "with tqdm(total=len(questions), desc=\"Answering questions\") as progress_bar:\n",
    "    with open(answer_file, \"w\") as f:\n",
    "        for question in questions:\n",
    "            response = qa_chain.invoke(question)\n",
    "            f.write(response + \"\\n\")\n",
    "            answers.append(response)\n",
    "            progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How to define a RAG chain? Give me a specific code example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group texts by their cluster ID\n",
    "clustered_texts = defaultdict(list)\n",
    "for _, row in df_clustered_texts.iterrows():\n",
    "    cluster_id = row['cluster'][0]  # Assuming each text belongs to one cluster\n",
    "    clustered_texts[cluster_id].append(row['text'])\n",
    "\n",
    "# Summarize texts within each cluster (simple join for this example)\n",
    "cluster_summaries = {cluster_id: ' '.join(texts) for cluster_id, texts in clustered_texts.items()}\n",
    "\n",
    "# For a more advanced summarization, you might use a model like BART or T5 loaded from HuggingFace Transformers\n",
    "# and pass `texts` to it for generating summaries. This is just a placeholder for your summarization logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not good empirical data on what the distribution for any particular topic should be if systemic biases did not exist in either Wikipedia or society (the world as it could and should be [2]). Therefore, in this track we adopted a compromise: we averaged the empirical distribution of groups among relevant documents with the world population (for location) or equality (for gender) to derive the target group distribution. We represented the group alignment of a document d with an alignment vector ad  [0, 1]|G|. adg is document ds alignment with group g. A  [0, 1]|D||G| is the alignment matrix for all documents. aworld denotes the distribution of the world.4 We considered fairness with respect to two group sets, Ggeo and Ggender. We operationalized this inter- sectional objective by letting G = Ggeo  Ggender, the Cartesian product of the two group sets. Further, alignment under either group set may be unknown; we represented this case by treating unknown as its own group (g?) in each set. In the product set, a documents alignment may be unknown for either or both groups.\n",
      "\n",
      "In all metrics, we use log discounting to compute attention weights: Within each work-needed bin of relevant documents, group exposure is fairly distributed according to the average of the distribution of relevant documents and the distribution of global population (the same average target as before). 1\n",
      "\n",
      "|L|\n",
      "\n",
      "X\n",
      "\n",
      "LL\n",
      "\n",
      "wL1\n",
      "\n",
      "d\n",
      "\n",
      "This forms an exposure vector (cid:15)  R|D|.\n",
      "\n",
      "It is aggregated into a group exposure vector , including\n",
      "\n",
      "unknown as a group:\n",
      "\n",
      " = AT(cid:15)\n",
      "\n",
      "Our implementation rearranges the mean and aggregate operations, but the result is mathematically\n",
      "\n",
      "equivalent. We then compare these system exposures with the target exposures (cid:15) for each query. This starts with the per-document ideal exposure; if mw is the number of relevant documents with work-needed level w  {1, 2, 3, 4}, then according to Diaz et al. [1] the ideal exposure for document d is computed as:\n",
      "\n",
      "(cid:15) d\n",
      "\n",
      "=\n",
      "\n",
      "1 mwd\n",
      "\n",
      "mwdX\n",
      "\n",
      "i=m>wd +1\n",
      "\n",
      "vi\n",
      "\n",
      "We use this to compute the non-averaged target distribution :\n",
      "\n",
      " = AT(cid:15) Since we include unknown as a group, we have a challenge with computing the target distribution by averaging the empirical distribution of relevant documents and the global population  global population does not provide any information on the proportion of relevant articles for which the fairness attributes are relevant. Our solution, therefore, is to average the distribution of known-group documents with the world population, and re-normalize so the nal distribution is a probability distribution, but derive the proportion of known- to unknown-group documents entirely from the empirical distribution of relevant documents. Extended to handle partially-unknown documents, this procedure proceeds as follows: Average the distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender).\n",
      "\n",
      "Average the distribution of documents with unknown location but known gender with the equality gender distribution.\n",
      "\n",
      "Average the distribution of documents with unknown gender but known location with the world pop- ulation. Implementation Now, to do this for every query, well use a function that takes a data frame for a querys relevant docs and performs all of the above operations:\n",
      "\n",
      "def query xalign(qdf):\n",
      "\n",
      "pages = qdf['page id'] pages = pages[pages.isin(page xalign.indexes['page'])] q xa = page xalign.loc[pages.values, :, :] q am = q xa.sum(axis=0)\n",
      "\n",
      "# clear and normalize q am[0, 0] = 0\n",
      "\n",
      "28\n",
      "\n",
      "q am = q am / q am.sum() 1.095016e-03 6.526425e-03 3.311463e-06\n",
      "\n",
      "Now with that function, we can compute the alignment vector for each query.\n",
      "\n",
      "train qtarget = train qrels.groupby('id').apply(query xalign) train qtarget\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5 page work = pages.set index('page id').quality score disc.astype(pd.CategoricalDtype(ordered=True)) page work = page work.cat.reorder categories(work order) page work.name = 'quality'\n",
      "\n",
      "A.6.1 Work and Target Exposure\n",
      "\n",
      "The rst thing we need to do to prepare the metric is to compute the work-needed for each topics pages, and use that to compute the target exposure for each (relevant) page in the topic. This is because an ideal ranking orders relevant documents in decreasing order of work needed, followed by irrelevant documents. All relevant documents at a given work level should receive the same expected exposure.\n",
      "\n",
      "First, look up the work for each query page (query page work, or qpw):\n",
      "\n",
      "qpw = qrels.join(page work, on='page id') qpw id 572 1 0 627 1 1 903 1 2 1193 1 3 1542 1 4 ... ... ... 2199072 150 63656179 2199073 150 63807245 2199074 150 64614938 2199075 150 64716982 2199076 150 65355704\n",
      "\n",
      "page_id quality C FA C B GA ... Start NaN C C C\n",
      "\n",
      "[2199077 rows x 3 columns]\n",
      "\n",
      "And now use that to compute the number of documents at each work level:\n",
      "\n",
      "qwork = qpw.groupby(['id', 'quality'])['page id'].count() qwork\n",
      "\n",
      "id 1\n",
      "\n",
      "quality Stub Start C B\n",
      "\n",
      "1527 2822 1603 610\n",
      "\n",
      "35\n",
      "\n",
      "GA\n",
      "\n",
      "240\n",
      "\n",
      "... 138 127 35 16 8 Name: page_id, Length: 636, dtype: int64\n",
      "\n",
      "150 Start C B GA FA\n",
      "\n",
      "Now we need to convert this into target exposure levels. This function will, given a series of counts for\n",
      "\n",
      "each work level, compute the expected exposure a page at that work level should receive.\n",
      "\n",
      "def qw tgt exposure(qw counts: pd.Series) -> pd.Series:\n",
      "\n",
      "if 'id' == qw counts.index.names[0]:\n",
      "\n",
      "qw counts = qw counts.reset index(level='id', drop=True) qwc = qw counts.reindex(work order, fill value=0).astype('i4') tot = int(qwc.sum()) da = metrics.discount(tot) qwp = qwc.shift(1, fill value=0) qwc s = qwc.cumsum() qwp s = qwp.cumsum() res = pd.Series(\n",
      "\n",
      "[np.mean(da[s:e]) for (s, e) in zip(qwp s, qwc s)], index=qwc.index\n",
      "\n",
      ") return res\n",
      "\n",
      "Well then apply this to each topic, to determine the per-topic target exposures:\n",
      "\n",
      "qw pp target = qwork.groupby('id').apply(qw tgt exposure) qw pp target.name = 'tgt exposure' qw pp target 0.114738 0.087373 0.081146 0.079298 0.078702 ... 0.154202 0.127359 0.120441 0.118827 0.118126\n",
      "\n",
      "Name: tgt_exposure, Length: 636, dtype: float32\n",
      "\n",
      "We can now merge the relevant document work categories with this exposure, to compute the target\n",
      "\n",
      "exposure for each relevant document:\n",
      "\n",
      "36\n",
      "\n",
      "qp exp = qpw.join(qw pp target, on=['id', 'quality']) qp exp = qp exp.set index(['id', 'page id'])['tgt exposure'] qp exp.index.names = ['q id', 'page id'] qp exp\n",
      "\n",
      "q_id page_id 1\n",
      "\n",
      "572 627 903 1193 1542\n",
      "\n",
      "150 63656179 63807245 64614938 64716982 65355704\n",
      "\n",
      "0.081146 0.078438 0.081146 0.079298 0.078702 ... 0.154202 NaN 0.127359 0.127359 0.127359\n",
      "\n",
      "Name: tgt_exposure, Length: 2199077, dtype: float32\n",
      "\n",
      "A.6.2 Geographic Alignment\n",
      "\n",
      "Now that weve computed per-page target exposure, were ready to set up the geographic alignment vectors for computing the per-group expected exposure with geographic data.\n",
      "\n",
      "Were going to start by getting the alignments for relevant documents for each topic: 0.0 0.0 0.0 0.0 0.0 ... 0.0 NaN 0.0 0.0 0.0\n",
      "\n",
      "Latin America and the Caribbean\n",
      "\n",
      "Northern America\n",
      "\n",
      "Oceania\n",
      "\n",
      "q_id page_id 572 1 627 903 1193 1542\n",
      "\n",
      "... 150 63656179 63807245\n",
      "\n",
      "0.0 0.0 0.0 0.0 0.0 ... 0.0 NaN\n",
      "\n",
      "0.0 0.0 0.0 0.0 0.0 ... 0.0 NaN\n",
      "\n",
      "0.0 0.0 0.0 0.0 0.0 ... 0.0 NaN\n",
      "\n",
      "37\n",
      "\n",
      "64614938 64716982 65355704\n",
      "\n",
      "0.0 0.0 0.0\n",
      "\n",
      "0.0 0.0 0.0\n",
      "\n",
      "0.0 0.0 0.0\n",
      "\n",
      "[2199077 rows x 8 columns]\n",
      "\n",
      "Now we need to compute the per-query target exposures. This starst with aligning our vectors: qp geo exp, qp geo align = qp exp.align(qp geo align, fill value=0)\n",
      "\n",
      "And now we can multiply the exposure vector by the alignment vector, and summing by topic - this is\n",
      "\n",
      "equivalent to the matrix-vector multiplication on a topic-by-topic basis.\n",
      "\n",
      "qp aexp = qp geo align.multiply(qp geo exp, axis=0) q geo align = qp aexp.groupby('q id').sum() Now things get a little weird. We want to average the empirical distribution with the world population to compute our fairness target. However, we dont have empirical data on the distribution of articles that do or do not have geographic alignments.\n",
      "\n",
      "Therefore, we are going to average only the known-geography vector with the world population. This\n",
      "\n",
      "proceeds in N steps: pages = qdf['page id'] pages = pages[pages.isin(page xalign.indexes['page'])] q xa = page xalign.loc[pages.values, :, :]\n",
      "\n",
      "# now we need to get the exposure for the pages, and multiply p exp = qp exp.loc[qdf.name] assert p exp.index.is unique p exp = xr.DataArray(p exp, dims=['page'])\n",
      "\n",
      "# and we multiply! q xa = q xa * p exp\n",
      "\n",
      "# normalize into a matrix (this time we don't clear) q am = q xa.sum(axis=0) q am = q am / q am.sum()\n"
     ]
    }
   ],
   "source": [
    "print(cluster_summaries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "all_texts = docs_texts.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Chroma' has no attribute 'from_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming embeddings are in the shape (number_of_documents, embedding_dimension)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create the vector store with Chroma using embeddings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m(embeddings)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Quick check to ensure vectorstore is set up correctly\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# This is a simple check; adjust based on how you can best verify your vectorstore setup\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Chroma' has no attribute 'from_embeddings'"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Assuming embeddings are in the shape (number_of_documents, embedding_dimension)\n",
    "# Create the vector store with Chroma using embeddings\n",
    "vectorstore = Chroma.from_documents(docs_text, embeddings)\n",
    "\n",
    "# Quick check to ensure vectorstore is set up correctly\n",
    "# This is a simple check; adjust based on how you can best verify your vectorstore setup\n",
    "try:\n",
    "    test_query_embedding = embeddings[0]  # Use the first document's embedding as a test query\n",
    "    test_results = vectorstore.similarity_search(test_query_embedding, top_k=1)\n",
    "    print(\"Chroma vectorstore is set up correctly. Test query result:\", test_results)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "# Setup LLaMA\n",
    "n_gpu_layers = 1  # Adjust as needed\n",
    "n_batch = 512     # Adjust based on hardware capabilities\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Retrieval and Prompting setup for RAPTOR\n",
    "prompt_template = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use one sentence maximum and keep the answer CONCISE. Question: {question} \\nContext: {context} \\nAnswer:\"\n",
    "prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=prompt_template))\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "rag_prompt.messages = [prompt]\n",
    "\n",
    "# Convert the vectorstore to a retriever for RAPTOR\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define the document formatting function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc['text'] for doc in docs)\n",
    "\n",
    "# Define the RAPTOR QA chain\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Load questions\n",
    "with open(\"SubmissionData/test/questions.txt\", \"r\") as f:\n",
    "    questions = [line.strip() for line in f]\n",
    "\n",
    "# File to save answers\n",
    "answer_file = \"SubmissionData/system_outputs/answers.txt\"\n",
    "\n",
    "# Initialize or clear the answer file\n",
    "with open(answer_file, \"w\") as f:\n",
    "    pass\n",
    "\n",
    "# Generate and save answers\n",
    "from tqdm import tqdm\n",
    "\n",
    "for question in tqdm(questions):\n",
    "    response = qa_chain.invoke(question)\n",
    "    with open(answer_file, \"a\") as f:\n",
    "        f.write(response + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-rag",
   "language": "python",
   "name": "nlp-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
