{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "model_name = \"BAAI/llm-embedder\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"llm-embedder\", embedding_function=embeddings)\n",
    "model = SentenceTransformer('BAAI/bge-reranker-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vectorstore.similarity_search(\"What is title of the course number 11711?\", k=10):\n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever, RetrieverLike, RetrieverOutputLike\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "  \n",
    "\n",
    "    # vectorstore = Chroma(persist_directory=\"llm-embedder\", embedding_function=embeddings)\n",
    "    # model = SentenceTransformer('BAAI/bge-reranker-base')\n",
    "\n",
    "    vectorstore : RetrieverLike\n",
    "\n",
    "    model : SentenceTransformer\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "\n",
    "        docs = self.vectorstore.get_relevant_documents(query, k=10)\n",
    "\n",
    "        queries = [query]\n",
    "        sentences = []\n",
    "        for i in docs:\n",
    "            sentences.append(i.page_content)\n",
    "\n",
    "        embeddings_1 = self.model.encode(sentences, normalize_embeddings=True)\n",
    "        embeddings_2 = self.model.encode(queries, normalize_embeddings=True)\n",
    "        similarity = embeddings_1 @ embeddings_2.T\n",
    "        results = [(similarity[count][0], i) for count, i in enumerate(docs)]\n",
    "        results = sorted(results, key=lambda x:x[0])\n",
    "\n",
    "        return [i for _,i in results][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = CustomRetriever(vectorstore=vectorstore.as_retriever(), model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "\n",
    "n_gpu_layers = -1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# rag_prompt.messages\n",
    "prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use one sentence maximum and keep the answer CONCISE. Keep the answer CONCISE.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))\n",
    "rag_prompt.messages = [prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "retriever = custom\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"FINALQUESTIONS.txt\", \"r\")\n",
    "questions = f.readlines()\n",
    "f.close()\n",
    "\n",
    "questions = [i.strip() for i in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "answer_file = \"SubmissionData/system_outputs/graham_reranking.txt\"\n",
    "\n",
    "\n",
    "f = open(answer_file, \"w\")\n",
    "f.close()\n",
    "\n",
    "answers = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "  response = qa_chain.invoke(questions[i])\n",
    "\n",
    "  f = open(answer_file, \"a\")\n",
    "  f.write(response.replace(\"\\n\", \"\") + \"\\n\")\n",
    "  f.close()\n",
    "  answers.append(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
