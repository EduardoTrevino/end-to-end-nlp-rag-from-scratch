{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Question Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "config.max_position_embeddings = 8096\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "llm_int8_enable_fp32_cpu_offload=True,\n",
    "bnb_4bit_quant_type='nf4',\n",
    "bnb_4bit_use_double_quant=True,\n",
    "bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "load_in_4bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "model_name_or_path,\n",
    "config=config,\n",
    "trust_remote_code=True,\n",
    "quantization_config=quantization_config,\n",
    "device_map=\"cuda\",\n",
    "offload_folder=\"./offload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = \"\"\"About Scotty\n",
    "\n",
    "The Scottish terrier has long been a familiar figure around Carnegie Mellon's campus. For years students have suited up in an unofficial Scottish terrier costume to excite the fans at athletic events. But the relationship between the Scottish terrier breed and Carnegie Mellon far precedes anybody doing somersaults in a dog costume. Andrew Carnegie, founder of the university, kept a Scottish terrier as his pet.\n",
    "\n",
    "Scotty's road from popular icon to official mascot of the university began in 2006. Carnegie Mellon formed a Mascot Identity Task Force in November 2006, which consisted of students, faculty, staff and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.\n",
    "\n",
    "The mascot selection process included a series of surveys and a university Town Hall meeting. Nearly 78 percent of 2,370 students surveyed in February 2007 voted for the Scottish terrier, and approximately 25 percent of 400 alumni surveyed thought the Scottish terrier was already the mascot.\n",
    "\n",
    "In the spring, the Task Force partnered with SME Branding — a firm with more than 17 years of experience creating mascots for professional sports teams and universities — to develop the graphics for the mascot. During October, students and alumni reviewed potential mascot images in focus groups.\n",
    "\n",
    "Carnegie Mellon's official mascot debuted at the Nov. 10, 2007 home football game. The graphic features a profile of a distinguished, bold Scottish terrier sporting a plaid scarf around his neck. The dog is contained in a shield, representing Carnegie Mellon's Scottish heritage.\n",
    "\n",
    "The Task Force then partnered with a mascot costume company to design our Scottish terrier in the winter of 2007. The official Scotty costume was unveiled at the 2008 Spring Carnival.\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"You are an expert AI assisting us in creating a high quality, diverse synthetic dataset to train Information Retrieval models. Your role is to analyse the document chunk given to you and provide us with high quality potential queries and answers. Make sure answer are concise. \\n\\n Content:{}\".format(information)},\n",
    "    ]\n",
    "\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(\"cuda\")\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "factoid_questions = [\"Is {} the author for {}?\", \n",
    "                     \"Is {} the publication venue for {}\",\n",
    "                     \"Was {} published?\",\n",
    "                     \"Did {} help publish {}\"]\n",
    "\n",
    "descriptive_questions = [\"Describe the contributions of the paper {}\"]\n",
    "\n",
    "reasoning_questions = [\"If I had to find up and coming research on {}, which paper should I refer to?\"]\n",
    "\n",
    "opinion_questions = [\"What is the best method for {}?\"]\n",
    "                     \n",
    "numeric_questions = [\"How many authors did the paper {} have?\",\n",
    "                     \"Which author has the most publications for the year of 2023?\"]\n",
    "\n",
    "list_questions = [\"What are the different types of language tasks for the papers published by LIT faculty?\",\n",
    "                  \"List three papers that are about hallucations for LLMs?\",\n",
    "                  \"What are the publication venues for the papers published by LIT faculty?\",\n",
    "                  \"What are all the publication venues for papers written by author {}?\"]\n",
    "\n",
    "contextual_questions = [\"What are the main findings of {}?\",\n",
    "                        \"What are the key results of {}?\"]\n",
    "\n",
    "multi_turn_questions = [\"What is a method proposed for {} and what makes it effective?\",\n",
    "                        \"List {} unique publication venues for papers from LTI professors at CMU.\"]\n",
    "\n",
    "comparision_questions = [\"Compare {} versus {} for {}\"]\n",
    "\n",
    "temporal_questions = [\"When was the paper {} published?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "import pickle as pkl\n",
    "\n",
    "with open(r\"webscholar_lti_dict.pkl\", \"rb\") as input_file:\n",
    "    data = pkl.load(input_file)\n",
    "\n",
    "\n",
    "    \n",
    "f = open(\"Web Scholar PDFs/metadata.txt\")\n",
    "l = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnswer(question, answer, file, type):\n",
    "    return \"Q: {}\\nA: {}\\nD: {}\\nT: {}\\n\\n\".format(question, answer, file, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "k = list(data.keys())\n",
    "random.shuffle(k)\n",
    "\n",
    "\n",
    "authors = {}\n",
    "for i in k:\n",
    "    authors[i] = set()\n",
    "\n",
    "titles = set()\n",
    "venue = set()\n",
    "paper_venue = {}\n",
    "\n",
    "for i in data:\n",
    "    for j in data[i]:\n",
    "        paper = data[i][j]\n",
    "    \n",
    "        authors[i].add(paper[\"title\"])\n",
    "        titles.add(paper[\"title\"])\n",
    "        venue.add(paper[\"publicationVenue\"])\n",
    "\n",
    "        if paper[\"publicationVenue\"] not in paper_venue:\n",
    "            paper_venue[paper[\"publicationVenue\"]] = set()\n",
    "        \n",
    "        paper_venue[paper[\"publicationVenue\"]].add(paper[\"title\"])\n",
    "\n",
    "        for auth in paper[\"authors\"]:\n",
    "            if auth[\"name\"] not in authors:\n",
    "                authors[auth[\"name\"]] = set()\n",
    "            authors[auth[\"name\"]].add(paper[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "k = list(authors.keys())\n",
    "k = random.sample(k, 5)\n",
    "\n",
    "\n",
    "for i in k:\n",
    "    seen = set()\n",
    "    for j in range(3):\n",
    "\n",
    "        randAuth = random.sample(list(authors.keys()), 1)\n",
    "\n",
    "        if len(authors[i]) > 0:\n",
    "            p1 = random.choice(list(authors[i]))\n",
    "\n",
    "            if p1 in seen:\n",
    "                pass\n",
    "            else:\n",
    "                seen.add(p1)\n",
    "                q1 = createAnswer(factoid_questions[0].format(i, p1), \"Yes.\", \"metadata.txt\", \"1\")\n",
    "                questions.append(q1)\n",
    "            \n",
    "        if len(authors[randAuth[0]]) > 0:\n",
    "            p2 = random.choice(list(authors[randAuth[0]]))\n",
    "            if p2 in seen:\n",
    "                pass\n",
    "            elif p2 in authors[i]:\n",
    "                q2 = createAnswer(factoid_questions[0].format(i, p2), \"Yes.\", \"metadata.txt\", \"1\")\n",
    "                seen.add(p2)\n",
    "                questions.append(q2)\n",
    "            else:\n",
    "                q2 = createAnswer(factoid_questions[0].format(i, p2), \"No.\", \"metadata.txt\", \"1\")\n",
    "                questions.append(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q: Is Giorgio Verdiani the author for The earlier Mona Lisa: creating a tactile physical model for transversal sharing and learning during the exhibition?\\nA: Yes.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Giorgio Verdiani the author for Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Giorgio Verdiani the author for InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Giorgio Verdiani the author for Aligning Large Multimodal Models with Factually Augmented RLHF?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Tianhua Tao the author for SlimPajama-DC: Understanding Data Combinations for LLM Training?\\nA: Yes.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Tianhua Tao the author for Decision Support System for Determining the Best PAUD Teacher Using the MOORA Method?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Tianhua Tao the author for The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Tianhua Tao the author for Hidden firing patterns and memristor initial condition-offset boosting behavior in a memristive Hindmarsh-Rose neuron model?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Athul Paul Jacob the author for AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies?\\nA: Yes.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Athul Paul Jacob the author for Language Model Beats Diffusion - Tokenizer is Key to Visual Generation?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Athul Paul Jacob the author for Understanding the Effect of Model Compression on Social Bias in Large Language Models?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Athul Paul Jacob the author for Quantifying & Modeling Feature Interactions: An Information Decomposition Framework?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Asli Celikyilmaz the author for OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models?\\nA: Yes.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Asli Celikyilmaz the author for Multi-PGS enhances polygenic prediction by combining 937 polygenic scores?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Asli Celikyilmaz the author for Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Asli Celikyilmaz the author for Correction to: Fusion of overexposed and underexposed images using caputo differential operator for resolution and texture based enhancement?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Natalie Shapira the author for Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models?\\nA: Yes.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Natalie Shapira the author for Understanding Masked Autoencoders via Hierarchical Latent Variable Models?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Natalie Shapira the author for 3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n',\n",
       " 'Q: Is Natalie Shapira the author for Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation?\\nA: No.\\nD: metadata.txt\\nT: 1\\n\\n']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
