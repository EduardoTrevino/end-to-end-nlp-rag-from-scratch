{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Question Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "config.max_position_embeddings = 8096\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "llm_int8_enable_fp32_cpu_offload=True,\n",
    "bnb_4bit_quant_type='nf4',\n",
    "bnb_4bit_use_double_quant=True,\n",
    "bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "load_in_4bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "model_name_or_path,\n",
    "config=config,\n",
    "trust_remote_code=True,\n",
    "quantization_config=quantization_config,\n",
    "device_map=\"cuda\",\n",
    "offload_folder=\"./offload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = \"\"\"About Scotty\n",
    "\n",
    "The Scottish terrier has long been a familiar figure around Carnegie Mellon's campus. For years students have suited up in an unofficial Scottish terrier costume to excite the fans at athletic events. But the relationship between the Scottish terrier breed and Carnegie Mellon far precedes anybody doing somersaults in a dog costume. Andrew Carnegie, founder of the university, kept a Scottish terrier as his pet.\n",
    "\n",
    "Scotty's road from popular icon to official mascot of the university began in 2006. Carnegie Mellon formed a Mascot Identity Task Force in November 2006, which consisted of students, faculty, staff and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.\n",
    "\n",
    "The mascot selection process included a series of surveys and a university Town Hall meeting. Nearly 78 percent of 2,370 students surveyed in February 2007 voted for the Scottish terrier, and approximately 25 percent of 400 alumni surveyed thought the Scottish terrier was already the mascot.\n",
    "\n",
    "In the spring, the Task Force partnered with SME Branding — a firm with more than 17 years of experience creating mascots for professional sports teams and universities — to develop the graphics for the mascot. During October, students and alumni reviewed potential mascot images in focus groups.\n",
    "\n",
    "Carnegie Mellon's official mascot debuted at the Nov. 10, 2007 home football game. The graphic features a profile of a distinguished, bold Scottish terrier sporting a plaid scarf around his neck. The dog is contained in a shield, representing Carnegie Mellon's Scottish heritage.\n",
    "\n",
    "The Task Force then partnered with a mascot costume company to design our Scottish terrier in the winter of 2007. The official Scotty costume was unveiled at the 2008 Spring Carnival.\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"You are an expert AI assisting us in creating a high quality, diverse synthetic dataset to train Information Retrieval models. Your role is to analyse the document chunk given to you and provide us with high quality potential queries and answers. Make sure answer are concise. \\n\\n Content:{}\".format(information)},\n",
    "    ]\n",
    "\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(\"cuda\")\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "factoid_questions = [\"Is {} the author for {}?\", \n",
    "                     \"Is {} the publication venue for {}\",\n",
    "                     \"Was {} published?\"]\n",
    "\n",
    "descriptive_questions = [\"Describe the contributions of the paper {}\"]\n",
    "\n",
    "reasoning_questions = [\"If I had to find up and coming research on {}, which paper should I refer to?\"]\n",
    "\n",
    "opinion_questions = [\"What is the best method for {}?\"]\n",
    "                     \n",
    "numeric_questions = [\"How many authors did the paper {} have?\",\n",
    "                     \"Which author has the most publications for the year of 2023?\"]\n",
    "\n",
    "list_questions = [\"What are the different types of language tasks for the papers published by LIT faculty?\",\n",
    "                  \"List three papers that are about hallucations for LLMs?\",\n",
    "                  \"What are the publication venues for the papers published by LIT faculty?\"]\n",
    "\n",
    "contextual_questions = [\"What are the main findings of {}?\",\n",
    "                        \"What are the key results of {}?\"]\n",
    "\n",
    "multi_turn_questions = [\"What is a method proposed for {} and what makes it effective?\",\n",
    "                        \"What are all the publication venues for papers written by author {}?\"]\n",
    "\n",
    "comparision_questions = [\"Compare {} versus {} for {}\"]\n",
    "\n",
    "temporal_questions = [\"When was the paper {} published?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "import pickle as pkl\n",
    "\n",
    "with open(r\"cleaned_webscholar/cleaned_webscholar_lti_dict.pkl\", \"rb\") as input_file:\n",
    "    data = pkl.load(input_file)\n",
    "\n",
    "\n",
    "    \n",
    "f = open(\"Web Scholar PDFs/metadata.txt\")\n",
    "l = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAnswer(question, answer, file, type):\n",
    "    return \"Q: {}\\nA: {}\\nD: {}\\nT: {}\\n\\n\".format(question, answer, file, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "k = list(data.keys())\n",
    "random.shuffle(k)\n",
    "\n",
    "\n",
    "authors = {}\n",
    "for i in k:\n",
    "    authors[i] = set()\n",
    "\n",
    "titles = set()\n",
    "venue = set()\n",
    "paper_venue = {}\n",
    "\n",
    "for i in data:\n",
    "    for j in data[i]:\n",
    "        paper = data[i][j]\n",
    "    \n",
    "        authors[i].add(paper[\"title\"])\n",
    "        titles.add(paper[\"title\"])\n",
    "        venue.add(paper[\"publicationVenue\"])\n",
    "\n",
    "        if paper[\"publicationVenue\"] not in paper_venue:\n",
    "            paper_venue[paper[\"publicationVenue\"]] = set()\n",
    "        \n",
    "        paper_venue[paper[\"publicationVenue\"]].add(paper[\"title\"])\n",
    "\n",
    "        for auth in paper[\"authors\"]:\n",
    "            if auth[\"name\"] not in authors:\n",
    "                authors[auth[\"name\"]] = set()\n",
    "            authors[auth[\"name\"]].add(paper[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFactoidQuestions(d, q):\n",
    "    questions = []\n",
    "\n",
    "    k = list(d.keys())\n",
    "    k = random.sample(k, 5)\n",
    "    for i in k:\n",
    "        seen = set()\n",
    "        for j in range(3):\n",
    "\n",
    "            randK = random.sample(list(d.keys()), 1)\n",
    "\n",
    "            if len(d[i]) > 0:\n",
    "                p1 = random.choice(list(d[i]))\n",
    "\n",
    "                if p1 in seen:\n",
    "                    pass\n",
    "                else:\n",
    "                    seen.add(p1)\n",
    "                    q1 = createAnswer(q.format(i, p1), \"Yes.\", \"metadata.txt\", \"1\")\n",
    "                    questions.append(q1)\n",
    "                \n",
    "            if len(d[randK[0]]) > 0:\n",
    "                p2 = random.choice(list(d[randK[0]]))\n",
    "                if p2 in seen:\n",
    "                    pass\n",
    "                elif p2 in d[i]:\n",
    "                    q2 = createAnswer(q.format(i, p2), \"Yes.\", \"metadata.txt\", \"1\")\n",
    "                    seen.add(p2)\n",
    "                    questions.append(q2)\n",
    "                else:\n",
    "                    q2 = createAnswer(q.format(i, p2), \"No.\", \"metadata.txt\", \"1\")\n",
    "                    questions.append(q2)\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factoid question generation\n",
    "\n",
    "q1 = createFactoidQuestions(authors, factoid_questions[0])\n",
    "q2 = createFactoidQuestions(paper_venue, factoid_questions[1])\n",
    "\n",
    "\n",
    "papers_not_published = [\"Training Compute-Optimal Large Language Models ('Chinchilla')\", \n",
    "                        \"Chain of Thought Prompting Elicits Reasoning in Large Language Models\", \n",
    "                        \"Training Language Models to Follow Instructions with Human Feedback ('InstructGPT')\", \n",
    "                        \"PaLM: Scaling Language Models with Pathways\", \n",
    "                        \"Constitutional AI: Harmless from AI Feedback\", \n",
    "                        \"ST-MoE: Designing Stable and Transferable Sparse Expert Models\", \n",
    "                        \"Solving Quantitative Reasoning Problems with Language Models ('Minerva')\",\n",
    "                        \"Scaling Instruction-Finetuned Language Models ('Flan2')\",\n",
    "                        \"Competition-level code Generation with AlphaCode\",\n",
    "                        \"LaMDA: Language Model for Dialog Applications\",\n",
    "                        \"Emergent Abilities of Large Language Models\",\n",
    "                        \"Multitask Prompted Training Enables Zero-Shot Task Generalization\"]\n",
    "\n",
    "q3 = []\n",
    "t = list(titles)\n",
    "random.shuffle(t)\n",
    "\n",
    "for i in papers_not_published:\n",
    "    q3.append(createAnswer(factoid_questions[2].format(i), \"No.\", \"metadata.txt\", \"1\"))\n",
    "\n",
    "for i in t[0:10]:\n",
    "    q3.append(createAnswer(factoid_questions[2].format(i), \"No.\", \"metadata.txt\", \"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Web Scholar PDFs/annotation.txt\", \"w\")\n",
    "for i in q1:\n",
    "    f.write(i)\n",
    "\n",
    "for i in q2:\n",
    "    f.write(i)\n",
    "\n",
    "for i in q3:\n",
    "    f.write(i)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
