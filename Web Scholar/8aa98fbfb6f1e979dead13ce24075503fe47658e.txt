A Survey on In-context Learning
Qingxiu Dong1, Lei Li1, Damai Dai1, Ce Zheng1, Zhiyong Wu2,
Baobao Chang1, Xu Sun1, Jingjing Xu2, Lei Li3and Zhifang Sui1
1MOE Key Lab of Computational Linguistics, School of Computer Science, Peking University
2Shanghai AI Lab3University of California, Santa Barbara
{dqx,lilei}@stu.pku.edu.cn, wuzhiyong@pjlab.org.cn, lilei@cs.ucsb.edu
{daidamai,zce1112zslx,chbb,xusun,jingjingxu,szf}@pku.edu.cn
Abstract
With the increasing ability of large language
models (LLMs), in-context learning (ICL)
has become a new paradigm for natural
language processing (NLP), where LLMs
make predictions only based on contexts aug-
mented with a few examples. It has been a
new trend to explore ICL to evaluate and ex-
trapolate the ability of LLMs. In this paper,
we aim to survey and summarize the progress
and challenges of ICL. We first present a for-
mal definition of ICL and clarify its corre-
lation to related studies. Then, we organize
and discuss advanced techniques, including
training strategies, demonstration designing
strategies, as well as related analysis. Finally,
we discuss the challenges of ICL and provide
potential directions for further research. We
hope that our work can encourage more re-
search on uncovering how ICL works and
improving ICL.
1 Introduction
With the scaling of model size and corpus size (De-
vlin et al., 2019; Radford et al., 2019; Brown et al.,
2020; Chowdhery et al., 2022), large language
models (LLMs) demonstrate an in-context learn-
ing (ICL) ability, that is, learning from a few ex-
amples in the context. Many studies have shown
that LLMs can perform a series of complex tasks
through ICL, such as solving mathematical reason-
ing problems (Wei et al., 2022c). These strong abil-
ities have been widely verified as emerging abilities
for large language models (Wei et al., 2022b).
The key idea of in-context learning is to learn
from analogy. Figure 1 gives an example describ-
ing how language models make decisions with ICL.
First, ICL requires a few examples to form a demon-
stration context. These examples are usually writ-
ten in natural language templates. Then, ICL con-
catenates a query question and a piece of demon-
stration context together to form a prompt, which
Review: Delicious food! Review: The food is awful. … Review: Terrible dishes!
PositiveLarge Language ModelReview: Good meal!Sentiment:
InputSentiment: PositiveSentiment: Negative…Sentiment: NegativeOutputParameter Freeze kDemonstrationExamplesNewQuery 
TemplateDelicious food!  The food is awful. Terrible dishes!…Review: [Text] Sentiment: [Label]TextLabel100…
Figure 1: Illustration of in-context learning. ICL re-
quires a piece of demonstration context containing a few
examples written in natural language templates. Taking
the demonstration and a query as the input, large lan-
guage models are responsible for making predictions.
is then fed into the language model for prediction.
Different from supervised learning requiring a train-
ing stage that uses backward gradients to update
model parameters, ICL does not conduct parameter
updates and directly performs predictions on the
pretrained language models. The model is expected
to learn the pattern hidden in the demonstration and
accordingly make the right prediction.
As a new paradigm, ICL has multiple attractive
advantages. First, since the demonstration is writ-
ten in natural language, it provides an interpretable
interface to communicate with LLMs (Brown et al.,
2020). This paradigm makes it much easier to in-
corporate human knowledge into LLMs by chang-
ing the demonstration and templates (Liu et al.,
2022; Lu et al., 2022; Wu et al., 2022; Wei et al.,
2022c). Second, in-context learning is similar to
the decision process of human beings by learning
from analogy (Winston, 1980). Third, compared
with supervised training, ICL is a training-free
learning framework. This could not only greatly re-
duce the computation costs for adapting the model
to new tasks, but also make language-model-as-a-
service (Sun et al., 2022) possible and can be easily
applied to large-scale real-world tasks.
Despite being promising, there are also inter-
esting questions and intriguing properties that re-arXiv:2301.00234v3  [cs.CL]  1 Jun 2023In-context LearningTraining Warmup (§4)Supervised
In-context
Training (§4.1)MetaICL (Min et al., 2022b), OPT-IML (Iyer et al., 2022), FLAN (Wei et al., 2022a),
Super-NaturalInstructions (Wang et al., 2022c), Scaling Instruction (Chung et al., 2022),
Symbol Tuning (Wei et al., 2023a)
Self-supervised
In-context
Training (§4.2)Self-supervised ICL (Chen et al., 2022a), PICL (Gu et al., 2023)
InferenceDemonstration
Designing (§5)Organization (§5.1)Selecting
(§5.1.1)KATE (Liu et al., 2022), EPR (Rubin et al., 2022), PPL (Gonen et al., 2022),
SG-ICL (Kim et al., 2022a), Self Adaptive (Wu et al., 2022),MI (Sorensen et al., 2022),
Q-Learning (Zhang et al., 2022a), Informative Score (Li and Qiu, 2023a),
Topic (Wang et al., 2023e), UDR (Li et al., 2023f)
Ordering
(§5.1.2)GlobalE&LocalE (Lu et al., 2022)
Formatting (§5.2)Instruction
(§5.2.1)Instruction Induction (Honovich et al., 2022), APE (Zhou et al., 2022c),
Self-Instruct (Wang et al., 2022b)
Reasoning
Steps
(§5.2.2)CoT (Wang et al., 2022b), Complex CoT (Fu et al., 2022),
AutoCoT (Zhang et al., 2022b), Self-Ask (Press et al., 2022),
MoT(Li and Qiu, 2023b), SuperICL(Xu et al., 2023b)
iCAP (Wang et al., 2022a), Least-to-Most Prompting (Zhou et al., 2022a)
Scoring
Function (§6)Channel prompt tuning (Min et al., 2022a), Structrured Prompting (Hao et al., 2022b),
kNN-Prompting (Xu et al., 2023a)
Figure 2: Taxonomy of in-context learning. The training and the inference stage are two main stages for ICL.
During the training stage, existing ICL studies mainly take a pretrained LLM as backbone, and optionally warmup
the model to strengthen and generalize the ICL ability. Towards the inference stage, the demonstration designing
and the scoring function selecting are crucial for the ultimate performance.
quire further investigation in ICL. While the vanilla
GPT-3 model itself shows promising ICL abilities,
several studies observed that the ability could be
significantly boosted via adaption during pretrain-
ing (Min et al., 2022b; Chen et al., 2022c). In
addition, the performance of ICL is sensitive to spe-
cific settings, including the prompting template, the
selection of in-context examples, and order of ex-
amples, and so on (Zhao et al., 2021). Furthermore,
while intuitively reasonable, the working mecha-
nism of the ICL remains unclear, and few studies
have provided preliminary explanations (Dai et al.,
2022; von Oswald et al., 2022).
With the rapid growth of studies in ICL, our
survey aims to sensitize the community toward the
current progress. Specifically, we present a detailed
paper survey with a paper list that will be continu-
ously updated, and make an in-depth discussion on
related studies of ICL. We highlight the challenges
and potential directions and hope our work may
provide a useful roadmap for beginners interested
in this area and shed light on future research.
2 Overview
The strong performance of ICL relies on two stages:
(1) the training stage that cultivates the ICL ability
of LLMs, and (2) the inference stage where LLMs
predict according to task-specific demonstrations.
In terms of the training stage, LLMs are directly
trained on language modeling objectives, such as
left-to-right generation. Although the models arenot specifically optimized for in-context learning,
they still exhibit the ICL ability. Existing studies on
ICL basically take a well-trained LLM as the back-
bone, and thus this survey will not cover the details
of pretraining language models. Towards the infer-
ence stage, as the input and output labels are all
represented in interpretable natural language tem-
plates, there are multiple directions for improving
ICL performance. This paper will give a detailed
description and comparison, such as selecting suit-
able examples for demonstrations and designing
specific scoring methods for different tasks.
We organize the current progress in ICL follow-
ing the taxonomy above (as shown in Figure 2).
With a formal definition of ICL (§3), we provide a
detailed discussion of the warmup approaches (§4),
the demonstration designing strategies (§5), and the
main scoring functions(§6). §7 provides in-depth
discussions of current explorations on unveiling the
secrets behind the ICL. We further provide useful
evaluation and resources for ICL (§8) and introduce
potential application scenarios where ICL shows
its effectiveness (§10). Finally, we summarize the
challenges and potential directions (§11) and hope
this could pave the way for researchers in this field.
3 Definition and Formulation
Following the paper of GPT-3 (Brown et al., 2020),
we provide a definition of in-context learning: In-
context learning is a paradigm that allows language
models to learn tasks given only a few examplesin the form of demonstration. Essentially, it esti-
mates the likelihood of the potential answer condi-
tioned on the demonstration by using a well-trained
language model.
Formally, given a query input text xand a
set of candidate answers Y={y1, . . . , y m}
(Y could be class labels or a set of free text
phrases), a pretrained language model Mtakes
the candidate answer with the maximum score
as the prediction conditioning a demonstration
setC.Ccontains an optional task instruc-
tionIandkdemonstration examples; there-
fore, C={I, s(x1, y1), . . . , s (xk, yk)}orC=
{s(x1, y1), . . . , s (xk, yk)}, where s(xk, yk, I)is
an example written in natural language texts ac-
cording to the task. The likelihood of a candidate
answer yjcould be represented by a scoring func-
tionfof the whole input sequence with the model
M:
P(yj|x)≜fM(yj, C, x ) (1)
The final predicted label ˆyis the candidate answer
with the highest probability:
ˆy= arg max
yj∈YP(yj|x). (2)
The scoring function festimates how possible the
current answer is given the demonstration and the
query text. For example, we could predict the class
label in a binary sentiment classification by compar-
ing the token probability of Negative andPositive .
There are many fvariants for different applications,
which will be elaborated in §6.
According to the definition, we can see the dif-
ference between ICL and other related concepts.
(1) Prompt Learning: Prompts can be discrete tem-
plates or soft parameters that encourage the model
to predict the desired output. Strictly speaking,
ICL can be regarded as a subclass of prompt tuning
where the demonstration is part of the prompt. Liu
et al. (2021) made a thorough survey on prompt
learning. However, ICL is not included. (2) Few-
shot Learning: few-shot learning is a general ma-
chine learning approach that uses parameter adap-
tation to learn the best model parameters for the
task with a limited number of supervised exam-
ples (Wang and Yao, 2019). In contrast, ICL does
not require parameter updates and is directly per-
formed on pretrained LLMs.
4 Model Warmup
Although LLMs have shown promising ICL ca-
pability, many studies also show that the ICL ca-pability can be further improved through a con-
tinual training stage between pretraining and ICL
inference, which we call model warmup for short.
Warmup is an optional procedure for ICL, which
adjusts LLMs before ICL inference, including mod-
ifying the parameters of the LLMs or adding ad-
ditional parameters. Unlike finetuning, warmup
does not aim to train the LLM for specific tasks but
enhances the overall ICL capability of the model.
4.1 Supervised In-context Training
To enhance ICL capability, researchers proposed
a series of supervised in-context finetuning strate-
gies by constructing in-context training data and
multitask training. Since the pretraining objectives
are not optimized for in-context learning (Chen
et al., 2022a), Min et al. (2022b) proposed a method
MetaICL to eliminate the gap between pretraining
and downstream ICL usage. The pretrained LLM
is continually trained on a broad range of tasks
with demonstration examples, which boosts its few-
shot abilities. To further encourage the model to
learn input-label mappings from the context, Wei
et al. (2023a) propose symbol tuning. This ap-
proach fine-tunes language models on in-context
input-label pairs, substituting natural language la-
bels (e.g., "positive/negative sentiment") with arbi-
trary symbols (e.g., "foo/bar"). As a result, symbol
tuning demonstrates an enhanced capacity to utilize
in-context information for overriding prior seman-
tic knowledge.
Besides, recent work indicates the potential
value of instructions (Mishra et al., 2021) and there
is a research direction focusing on supervised in-
struction tuning. Instruction tuning enhances the
ICL ability of LLMs through training on task in-
structions. Tuning the 137B LaMDA-PT (Thop-
pilan et al., 2022) on over 60 NLP datasets ver-
balized via natural language instruction templates,
FLAN (Wei et al., 2022a) improves both the zero-
shot and the few-shot ICL performance. Compared
to MetaICL, which constructs several demonstra-
tion examples for each task, instruction tuning
mainly considers an explanation of the task and
is more easier to scale up. Chung et al. (2022) and
Wang et al. (2022c) proposed to scale up instruction
tuning with more than 1000+ task instructions.
4.2 Self-supervised In-context Training
Leveraging raw corpora for warmup, Chen et al.
(2022a) proposed constructing self-supervised
training data aligned with ICL formats in down-stream tasks. They transformed raw text into input-
output pairs, exploring four self-supervised objec-
tives, including masked token prediction and classi-
fication tasks. Alternatively, PICL (Gu et al., 2023)
also utilizes raw corpora but employs a simple lan-
guage modeling objective, promoting task infer-
ence and execution based on context while preserv-
ing pre-trained models’ task generalization. Con-
sequently, PICL outperforms Chen et al. (2022a)’s
method in effectiveness and task generalizability.
3Takeaway :(1) Supervised training and self-
supervised training both propose to train the LLMs
before ICL inference. The key idea is to bridge the
gap between pretraining and downstream ICL for-
mats by introducing objectives close to in-context
learning. Compared to in-context finetuning involv-
ing demonstration, instruction finetuning without a
few examples as demonstration is simpler and more
popular. (2) To some extent, these methods all im-
prove the ICL capability by updating the model pa-
rameters, which implies that the ICL capability of
the original LLMs has great potential for improve-
ment. Therefore, although ICL does not strictly
require model warmup, we recommend adding a
warmup stage before ICL inference. (3) The perfor-
mance advancement made by warmup encounters
a plateau when increasingly scaling up the training
data. This phenomenon appears both in supervised
in-context training and self-supervised in-context
training, indicating that LLMs only need a small
amount of data to adapt to learn from the context
during warmup.
5 Demonstration Designing
Many studies have shown that the performance
of ICL strongly relies on the demonstration sur-
face, including demonstration format, the order of
demonstration examples, and so on (Zhao et al.,
2021; Lu et al., 2022). As demonstrations play a vi-
tal role in ICL, in this section, we survey demonstra-
tion designing strategies and classify them into two
groups: demonstration organization and demonstra-
tion formatting, as shown in Table 1.
5.1 Demonstration Organization
Given a pool of training examples, demonstration
organization focuses on how to select a subset of
examples and the order of the selected examples.
5.1.1 Demonstration Selection
Demonstrations selection aims to answer a funda-
mental question: Which examples are good exam-ples for ICL? We classify related studies into two
categories, including unsupervised methods based
on pre-defined metrics and supervised methods.
Unsupervised Method Liu et al. (2022) showed
that selecting the closest neighbors as the in-context
examples is a good solution. The distance metrics
are pre-defined L2 distance or cosine-similarity
distance based on sentence embeddings. They
proposed KATE, a kNN-based unsupervised re-
triever for selecting in-context examples. In addi-
tion to distance metrics, mutual information is also
a valuable selection metric (Sorensen et al., 2022).
Similarly, k-NN cross-lingual demonstrations can
be retrieved for multi-lingual ICL (Tanwar et al.,
2023) to strengthen source-target language align-
ment. The advantage of mutual information is that
it does not require labeled examples and specific
LLMs. In addition, Gonen et al. (2022) attempted
to choose prompts with low perplexity. Levy et al.
(2022) consider the diversity of demonstrations to
improve compositional generalization. They select
diverse demonstrations to cover different kinds of
training demonstrations. Different from these stud-
ies selecting examples from human-labeled data,
Kim et al. (2022a) proposed to generate demonstra-
tions from LLM itself.
Some other methods utilized the output scores
of LMs P(y|C, x)as unsupervised metrics to se-
lect demonstrations. Wu et al. (2022) selected the
best subset permutation of kNN examples based
on the code-length for data transmission to com-
press label ygiven xandC. Nguyen and Wong
(2023) measured the influence of a demonstration
xiby calculating the difference between the av-
erage performance of the demonstration subsets
{C|xi∈C}and{C|xi/∈C}. Furthermore, Li
and Qiu (2023a) used infoscore, i.e., the average
ofP(y|xi, yi, x)−P(y|x)for all (x, y)pairs in a
validation set with a diversity regularization.
Supervised Method Rubin et al. (2022) pro-
posed a two-stage retrieval method to select demon-
strations. For a specific input, it first built an un-
supervised retriever (e.g., BM25) to recall simi-
lar examples as candidates and then built a su-
pervised retriever EPR to select demonstrations
from candidates. A scoring LM is used to eval-
uate the concatenation of each candidate exam-
ple and the input. Candidates with high scores
are labeled as positive examples, and candidates
with low scores are hard negative examples. LiCategory Methods Demonstration Acquisition LLMs Main Tasks
Demonstration SelectionKATE (Liu et al., 2022) Human design GPT-3 SST, table-to-text
SG-ICL (Kim et al., 2022a) LM generated GPT-J SST, NLI
EPR (Rubin et al., 2022) Human design GPT-{J, 3}/CodeX Semantic parsing
Demonstration Ordering GlobalE & LocalE (Lu et al., 2022) Human design GPT-{2, 3} Text classification
Instruction Formatting Self Instruct (Wang et al., 2022b) LM generated GPT-3/InstructGPT SuperNaturalInstruction
Reasoning Steps FormattingCoT (Wei et al., 2022c) Human design GPT-3/CodeX Reasoning tasks
AutoCoT (Zhang et al., 2022b) LM generated GPT-3/PaLM Reasoning tasks
Self-Ask (Press et al., 2022) LM generated GPT-3/InstructGPT MultihopQA
Table 1: Summary of representative demonstration designing methods.
et al. (2023f) further enhanced the EPR by adopt-
ing a unified demonstration retriever to unify the
demonstration selection across different tasks. Ye
et al. (2023a) retrieved the entire set of demonstra-
tions instead of individual demonstrations to model
inter-relationships between demonstrations. They
trained a DPP retriever to align with LM output
scores by contrastive learning and obtained the op-
timal demonstration set with maximum a posteriori
at inference.
Based on prompt tuning, Wang et al. (2023e)
view LLMs as topic models that can infer con-
cepts θfrom few demonstrations and generate to-
kens based on concept variables θ. They use task-
related concept tokens to represent latent concepts.
Concept tokens are learned to maximize P(y|x, θ).
They select demonstrations that are most likely to
infer the concept variable based on P(θ|x, y). Be-
sides, reinforcement learning was introduced by
Zhang et al. (2022a) for example selection. They
formulated demonstration selection as a Markov de-
cision process (Bellman, 1957) and selected demon-
strations via Q-learning. The action is choosing an
example, and the reward is defined as the accuracy
of a labeled validation set.
5.1.2 Demonstration Ordering
Ordering the selected demonstration examples is
also an important aspect of demonstration orga-
nization. Lu et al. (2022) have proven that order
sensitivity is a common problem and always exists
for various models. To handle this problem, pre-
vious studies have proposed several training-free
methods to sort examples in the demonstration. Liu
et al. (2022) sorted examples decently by their dis-
tances to the input, so the rightmost demonstration
is the closest example. Lu et al. (2022) defined the
global and local entropy metrics. They found a pos-
itive correlation between the entropy metric and the
ICL performance. They directly used the entropy
metric to select the best ordering of examples.5.2 Demonstration Formatting
A common way to format demonstrations is con-
catenating examples (x1, y1), . . . , (xk, yk)with a
template Tdirectly. However, in some tasks that
need complex reasoning (e.g., math word problems,
commonsense reasoning), it is not easy to learn the
mapping from xitoyiwith only kdemonstrations.
Although template engineering has been studied in
prompting (Liu et al., 2021), some researchers aim
to design a better format of demonstrations for ICL
by describing tasks with the instruction I(§5.2.1)
and adding intermediate reasoning steps between
xiandyi(§5.2.2).
5.2.1 Instruction Formatting
Except for the well-designed demonstration ex-
amples, good instructions which describe the task
precisely are also helpful to the inference perfor-
mance. However, unlike the demonstration exam-
ples, which are common in traditional datasets, the
task instructions depend heavily on human-written
sentences. Honovich et al. (2022) found that given
several demonstration examples, LLMs can gener-
ate the task instruction. According to the genera-
tion ability of LLMs, Zhou et al. (2022c) proposed
Automatic Prompt Engineer for automatic instruc-
tion generation and selection. To further improve
the quality of the automatically generated instruc-
tions, Wang et al. (2022b) proposed to use LLMs
to bootstrap off its own generations. Existing work
has achieved good results in automatically generat-
ing instructions, which provided opportunities for
future research on combining human feedback with
automatic instruction generation.
5.2.2 Reasoning Steps Formatting
Wei et al. (2022c) added intermediate reasoning
steps between inputs and outputs to construct
demonstrations, which are called chain-of-thoughts
(CoT). With CoT, LLMs predict the reasoning steps
and the final answer. CoT prompting can learncomplex reasoning by decomposing input-output
mappings into many intermediate steps. There are
many pieces of research on CoT prompting strate-
gies (Qiao et al., 2022) including prompt designing
and process optimization. In this paper, we mainly
focus on CoT designing strategies.
Similar to demonstration selection, CoT design-
ing also considers CoT selection. Different from
Wei et al. (2022c) manually writing CoTs, Auto-
CoT (Zhang et al., 2022b) used LLMs with Let’s
think step by step to generate CoTs. In addi-
tion, Fu et al. (2022) proposed a complexity-based
demonstration selection method. They selected
demonstrations with more reasoning steps for CoT
prompting.
As input-output mappings are decomposed into
step-by-step reasoning, some researchers apply
multi-stage ICL for CoT prompting and design
CoT demonstrations for each step. Multi-stage
ICL queries LLMs with different demonstrations in
each reasoning step. Self-Ask (Press et al., 2022) al-
lows LLMs to generate follow-up questions for the
input and ask themselves these questions. Then the
questions and intermediate answers will be added
to CoTs. iCAP (Wang et al., 2022a) proposes a
context-aware prompter that can dynamically ad-
just contexts for each reasoning step. Least-to-
Most Prompting (Zhou et al., 2022a) is a two-stage
ICL including question reduction and subquestion
solution. The first stage decomposes a complex
question into subquestions; in the second stage,
LLMs answer subquestions sequentially, and previ-
ously answered questions and generated answers
will be added into the context.
Xu et al. (2023b) fine-tuned small LMs on spe-
cific task as plug-ins to generate pseudo reasoning
steps. Given an input-output pair (xi, yi), Super-
ICL regarded the prediction y′
iand confidence ci
of small LMs for the input xias reasoning steps by
concatenating (xi, y′
i, ci, yi).
3Takeaway :(1) Demonstration selection
strategies improve the ICL performance, but most
of them are instance level. Since ICL is mainly
evaluated under few-shot settings, the corpus-level
selection strategy is more important yet under-
explored. (2) The output score or probability distri-
bution of LLMs plays an important role in instance
selecting. (3) For kdemonstrations, the size of
search space of permutations is k!. How to find the
best orders efficiently or how to approximate the
optimal ranking better is also a challenging ques-Scoring Function Target Efficiency Task Coverage Stability
Direct M(yj|C, x) +++ + +
PPL PPL(Sj) + +++ +
Channel M(x|C, y j) + + ++
Table 2: Summary of different scoring functions.
tion. (4) Adding chain-of-thoughts can effectively
decompose complex reasoning tasks into intermedi-
ate reasoning steps. During inference, multi-stage
demonstration designing strategies are applied to
generate CoTs better. How to improve the CoT
prompting ability of LLMs is also worth explor-
ing (5) In addition to human-written demonstra-
tions, the generative nature of LLMs can be utilized
in demonstration designing. LLMs can generate
instructions, demonstrations, probing sets, chain-
of-thoughts, and so on. By using LLM-generated
demonstrations, ICL can largely get rid of human
efforts on writing templates.
6 Scoring Function
The scoring function decides how we can transform
the predictions of a language model into an estima-
tion of the likelihood of a specific answer. A direct
estimation method (Direct) adopts the conditional
probability of candidate answers that can be rep-
resented by tokens in the vocabulary of language
models (Brown et al., 2020). The answer with a
higher probability is selected as the final answer.
However, this method poses some restrictions on
the template design, e.g., the answer tokens should
be placed at the end of input sequences. Perplex-
ity (PPL) is another commonly-used metric, which
computes the sentence perplexity of the whole in-
put sequence Sj={C, s(x, yj, I)}consists of the
tokens of demonstration examples C, input query x
and candidate label yj. As PPL evaluates the prob-
ability of the whole sentence, it removes the limi-
tations of token positions but requires extra com-
putation time. Note that in generation tasks such
as machine translation, ICL predicts the answer by
decoding tokens with the highest sentence probabil-
ity combined with diversity-promoting strategies
such as beam search or Top- pand Top- k(Holtzman
et al., 2020) sampling algorithms.
Different from previous methods, which esti-
mate the probability of the label given the input
context, Min et al. (2022a) proposed to utilize chan-
nel models (Channel) to compute the conditional
probability in a reversed direction, i.e., estimating
the likelihood of input query given the label. Inthis way, language models are required to generate
every token in the input, which could boost the per-
formance under imbalanced training data regimes.
We summarize all three scoring functions in Table 2.
As ICL is sensitive to the demonstration (see §5
for more details), normalizing the obtained score
by subtracting a model-dependent prior with empty
inputs is also effective for improving the stability
and overall performance (Zhao et al., 2021).
Another direction is to incorporate informa-
tion beyond the context length constrain to cali-
brate the score. Structured Prompting (Hao et al.,
2022b) proposes to encode demonstration exam-
ples separately with special positional embeddings,
which then are provided to the test examples with
a rescaled attention mechanism. kNN Prompt-
ing (Xu et al., 2023a) first queries LLMs with train-
ing data for distributed representations, then pre-
dicts test instances by simply referring to nearest
neighbors with closing representations with stored
anchor representations.
3Takeaway :(1) We conclude the characteris-
tics of three widely-used scoring functions in Ta-
ble 2. Although directly adopting the conditional
probability of candidate answers is efficient, this
method still poses some restrictions on the tem-
plate design. Perplexity is also a simple and widely
scoring function. This method has universal ap-
plications, including both classification tasks and
generation tasks. However, both methods are still
sensitive to demonstration surface, while Chan-
nel is a remedy that especially works under im-
balanced data regimes. (2) Existing scoring func-
tions all compute a score straightforwardly from
the conditional probability of LLMs. There is lim-
ited research on calibrating the bias or mitigating
the sensitivity via scoring strategies. For instance,
some studies add additional calibration parame-
ters to adjust the model predictions (Zhao et al.,
2021).
7 Analysis
To understand ICL, many analytical studies attempt
to investigate what factors may influence the perfor-
mance and aim to figure out why ICL works. We
summarize the factors that have a relatively strong
correlation to ICL performance in Table 3 for easy
reference.Stage Factor
PretrainingPretraining corpus domain
(Shin et al., 2022a)
Pretraining corpus combination
(Shin et al., 2022a)
Number of model parameters
(Wei et al., 2022b; Brown et al., 2020)
Number of pretraining steps
(Wei et al., 2022b)
InferenceLabel space exposure
(Min et al., 2022c)
Demonstration input distribution
(Min et al., 2022c)
Format of input-label pairing
(Min et al., 2022c; An et al., 2023)
Demonstration input-label mapping
(Min et al., 2022c; Kim et al., 2022b)
(Wei et al., 2023b)
Demonstration sample ordering
(Lu et al., 2022)
Demonstration-query similarity
(Liu et al., 2022)
Demonstration diversity
(An et al., 2023)
Demonstration complexity
(An et al., 2023)
Table 3: Summary of factors that have a relatively strong
correlation to ICL performance.
7.1 What Influences ICL Performance
Pre-training Stage We first introduce influence
factors in the LLM pretraining stage. Shin et al.
(2022a) investigated the influence of the pretrain-
ing corpora. They found that the domain source is
more important than the corpus size. Putting mul-
tiple corpora together may give rise to emergent
ICL ability, pretraining on corpora related to the
downstream tasks does not always improve the ICL
performance, and models with lower perplexity
do not always perform better in the ICL scenar-
ios. Wei et al. (2022b) investigated the emergent
abilities of many large-scale models on multiple
tasks. They suggested that a pretrained model sud-
denly acquires some emergent ICL abilities when it
achieves a large scale of pretraining steps or model
parameters. Brown et al. (2020) also showed that
the ICL ability grows as the parameters of LLMs
increase from 0.1 billion to 175 billion.
Inference Stage In the inference stage, the prop-
erties of the demonstration samples also influence
the ICL performance. Min et al. (2022c) investi-
gated that the influence of demonstration samples
comes from four aspects: the input-label pairingformat, the label space, the input distribution, and
the input-label mapping. They prove that all of the
input-label pairing formats, the exposure of label
space, and the input distribution contribute substan-
tially to the ICL performance. Counter-intuitively,
the input-label mapping matters little to ICL. In
terms of the effect of input-label mapping, Kim
et al. (2022b) drew an opposite conclusion that
correct input-label mapping does impact the ICL
performance, depending on specific experimental
settings. Wei et al. (2023b) further found that when
a model is large enough, it will show an emer-
gent ability to learn input-label mappings, even
if the labels are flipped or semantically-unrelated.
From the compositional generalization perspective,
An et al. (2023) validated that ICL demonstrations
should be diverse, simple, and similar to the test ex-
ample in terms of the structure. Lu et al. (2022) in-
dicated that the demonstration sample order is also
an important factor. In addition, Liu et al. (2022)
found that the demonstration samples that have
closer embeddings to the query samples usually
bring better performance than those with farther
embeddings.
7.2 Understanding Why ICL Works
Distribution of Training Data Concentrating on
the pretraining data, Chan et al. (2022) showed that
the ICL ability is driven by data distributional prop-
erties. They found that the ICL ability emerges
when the training data have examples appearing in
clusters and have enough rare classes. Xie et al.
(2022) explained ICL as implicit Bayesian infer-
ence and constructed a synthetic dataset to prove
that the ICL ability emerges when the pretraining
distribution follows a mixture of hidden Markov
models.
Learning Mechanism By learning linear func-
tions, Garg et al. (2022) proved that Transformers
could encode effective learning algorithms to learn
unseen linear functions according to demonstra-
tion samples. They also found that the learning
algorithm encoded in an ICL model can achieve
a comparable error to that from a least squares
estimator. Li et al. (2023g) abstracted ICL as an
algorithm learning problem and showed that Trans-
formers can implement a proper function class
through implicit empirical risk minimization for
the demonstrations. Pan et al. (2023) decoupled the
ICL ability into task recognition ability and task
learning ability, and further showed how they uti-lize demonstrations. From an information-theoretic
perspective, Hahn and Goyal (2023) showed an er-
ror bound for ICL under linguistically motivated
assumptions to explain how next-token prediction
can bring about the ICL ability. Si et al. (2023)
found that large language models exhibit prior fea-
ture biases and showed a way to use intervention
to avoid unintended features in ICL.
Another series of work attempted to build con-
nections between ICL and gradient descent. Tak-
ing linear regression as a starting point, Akyürek
et al. (2022) found that Transformer-based in-
context learners can implement standard finetun-
ing algorithms implicitly, and von Oswald et al.
(2022) showed that linear attention-only Transform-
ers with hand-constructed parameters and mod-
els learned by gradient descent are highly related.
Based on softmax regression, Li et al. (2023e)
found that self-attention-only Transformers showed
similarity with models learned by gradient-descent.
Dai et al. (2022) figured out a dual form between
Transformer attention and gradient descent and fur-
ther proposed to understand ICL as implicit fine-
tuning. Further, they compared GPT-based ICL
and explicit finetuning on real tasks and found that
ICL indeed behaves similarly to finetuning from
multiple perspectives.
Functional Components Focusing on specific
functional modules, Olsson et al. (2022) found that
there exist some induction heads in Transformers
that copy previous patterns to complete the next
token. Further, they expanded the function of in-
duction heads to more abstract pattern matching
and completion, which may implement ICL. Wang
et al. (2023b) focused on the information flow in
Transformers and found that during the ICL pro-
cess, demonstration label words serves as anchors,
which aggregates and distributes key information
for the final prediction.
3Takeaway :(1) Knowing and considering
how ICL works can help us improve the ICL per-
formance, and the factors that strongly correlate to
ICL performance are listed in Table 3. (2) Although
some analytical studies have taken a preliminary
step to explain ICL, most of them are limited to
simple tasks and small models. Extending analysis
on extensive tasks and large models may be the
next step to be considered. In addition, among ex-
isting work, explaining ICL with gradient descent
seems to be a reasonable, general, and promising
direction for future research. If we build clear con-Benchmark Tasks #Tasks
BIG-Bench
(Srivastava et al., 2022)Mixed tasks 204
BBH
(Suzgun et al., 2022)Unsolved problems 23
PRONTOQA
(Saparov and He, 2022)Question answering 1
MGSM
(Shi et al., 2022)Math problems 1
LLMAS
(Valmeekam et al., 2022)Plan and reasoning tasks 8
OPT-IML Bench
(Iyer et al., 2022)Mixed tasks 2000
Table 4: New challenging evaluation benchmarks for
ICL. For short, we use LLMAS to represent LLM As-
sessment Suite (Valmeekam et al., 2022).
nections between ICL and gradient-descent-based
learning, we can borrow ideas from the history of
traditional deep learning to improve ICL.
8 Evaluation and Resources
8.1 Traditional Tasks
As a general learning paradigm, ICL can be ex-
amined on various traditional datasets and bench-
marks, e.g., SuperGLUE (Wang et al., 2019),
SQuAD (Rajpurkar et al., 2018). Implementing
ICL with 32 randomly sampled examples on Su-
perGLUE, Brown et al. (2020) found that GPT-
3 can achieve results comparable to state-of-the-
art (SOTA) finetuning performance on COPA and
ReCoRD, but still falls behind finetuning on most
NLU tasks. Hao et al. (2022b) showed the po-
tential of scaling up the number of demonstration
examples. However, the improvement brought by
scaling is very limited. At present, compared to
finetuning, there still remains some room for ICL
to reach on traditional NLP tasks.
8.2 New Challenging Tasks
In the era of large language models with in-context
learning capabilities, researchers are more inter-
ested in evaluating the intrinsic capabilities of large
language models without downstream task finetun-
ing (Bommasani et al., 2021).
To explore the capability limitations of LLM on
various tasks, Srivastava et al. (2022) proposed
the BIG-Bench (Srivastava et al., 2022), a large
benchmark covering a large range of tasks, includ-
ing linguistics, chemistry, biology, social behav-
ior, and beyond. The best models have already
outperformed the average reported human-rater
results on 65% of the BIG-Bench tasks throughICL (Suzgun et al., 2022). To further explore tasks
actually unsolvable by current language models,
Suzgun et al. (2022) proposed a more challenging
ICL benchmark, BIG-Bench Hard (BBH). BBH in-
cludes 23 unsolved tasks, constructed by selecting
challenging tasks where the state-of-art model per-
formances are far below the human performances.
Besides, researchers are searching for inverse scal-
ing tasks,1that is, tasks where model performance
reduces when scaling up the model size. Such
tasks also highlight potential issues with the cur-
rent paradigm of ICL. To further probe the model
generalization ability, Iyer et al. (2022) proposed
OPT-IML Bench, consisting of 2000 NLP tasks
from 8 existing benchmarks, especially benchmark
for ICL on held-out categories.
Specifically, a series of studies focus on ex-
ploring the reasoning ability of ICL. Saparov and
He (2022) generated an example from a synthetic
world model represented in first-order logic and
parsed the ICL generations into symbolic proofs
for formal analysis. They found that LLMs can
make correct individual deduction steps via ICL.
Shi et al. (2022) constructed the MGSM bench-
mark to evaluate the chain-of-thought reasoning
abilities of LLMs in multilingual settings, finding
that LLMs manifest complex reasoning across mul-
tiple languages. To further probe more sophisti-
cated planning and reasoning abilities of LLMs,
Valmeekam et al. (2022) provided multiple test
cases for evaluating various reasoning abilities on
actions and change, where existing ICL methods
on LLMs show poor performance.
8.3 Open-source Tools
Noticing that ICL methods are often implemented
differently and evaluated using different LLMs and
tasks, Wu et al. (2023) developed OpenICL, an
open-source toolkit enabling flexible and unified
ICL assessment. With its adaptable architecture,
OpenICL facilitates the combination of distinct
components and offers state-of-the-art retrieval and
inference techniques to accelerate the integration
of ICL into advanced research.
3Takeaway :(1) Due to the restrictions of
ICL on the number of demonstration examples, the
traditional evaluation tasks must be adapted to
few-shot settings; otherwise, the traditional bench-
marks cannot evaluate the ICL capability of LLMs
directly. (2) As ICL is a new paradigm that is dif-
1https://github.com/inverse-scaling/prizeVisual prompt image
OutputInpainting Modelf
x1y1xqvp
Concatenate into single image
x
Edge detectionColorizationInpaintingSegmentationStyle transferTask Input ExampleTask Output ExampleQueryVisual prompt image
OutputInpainting Modelf
x1y1xqvp
Concatenate into single image
x
Edge detectionColorizationInpaintingSegmentationStyle transferTask Input ExampleTask Output ExampleQueryVisual prompt image
OutputInpainting Modelf
x1y1xqvp
Concatenate into single image
x
Edge detectionColorizationInpaintingSegmentationStyle transferTask Input ExampleTask Output ExampleQueryVisual prompt image
OutputInpainting Modelf
x1y1xqvp
Concatenate into single image
x
Edge detectionColorizationInpaintingSegmentationStyle transferTask Input ExampleTask Output ExampleQueryTaskInputImageTaskOutputImageQueryImageVisualPromptGridImageInpaintingModelVisual prompt image
OutputInpainting Modelf
x1y1xqvp
Concatenate into single image
x
Edge detectionColorizationInpaintingSegmentationStyle transferTask Input ExampleTask Output ExampleQueryTaskTextPromptTaskInputImageTaskOutputImage“Segment the horses from the rest of the image and generate a new image where the horse regions are white and the other regions are black.”QueryImageDiffusionModelOutputImage
OutputImageTextVisualPrompt
Figure 3: Image-only and textual augmented prompting
for visual in-context learning.
ferent from traditional learning paradigms in many
aspects, the evaluation of ICL presents new chal-
lenges and opportunities. Toward the challenges,
the results of existing evaluation methods are un-
stable, especially sensitive to the demonstration
examples and the instructions. Chen et al. (2022b)
observed that existing evaluations by accuracy un-
derestimate the sensitivity towards instruction per-
turbation of ICL. It is still an open question to con-
duct consistent ICL evaluation and OpenICL(Wu
et al., 2023) represents a valuable initial attempt to
address this challenge. Toward the opportunities
for evaluation, as ICL only requires a few instances
for the demonstration, it lowers the cost of evalua-
tion data construction.
9 In-Context Learning Beyond Text
The tremendous success of ICL in NLP has in-
spired researchers to explore its potential in differ-
ent modalities, including visual, vision+language
and speech tasks as well.
9.1 Visual In-Context Learning
Bar et al. (2022) employ an image patch infill-
ing task in grid-like images using masked auto-
encoders (MAE) to train their model. At the in-
ference stage, the model generates output images
consistent with provided input-output examples for
a novel input image, showcasing promising ICL
capabilities for unseen tasks such as image segmen-
tation. Painter (Wang et al., 2023c) extends this
approach by incorporating multiple tasks to build
a generalist model, achieving competitive perfor-
mance compared to task-specific models. Build-
ing upon this, SegGPT (Wang et al., 2023d) in-
tegrates diverse segmentation tasks into a unified
framework and investigates ensemble techniquesfrom spatial and feature perspectives to enhance the
quality of prompt examples. Wang et al. (2023f)
propose to utilize an extra text prompt to guide a
generative model in comprehensively producing
the desired image. The resulting Prompt Diffu-
sion model is the first diffusion-based model that
exhibits ICL ability. Figure 3 illustrates the key dif-
ference between the image-only and textual prompt
augmented in-context learning for visual in-context
learning.
Similar to ICL in NLP, the effectiveness of vi-
sual in-context learning is significantly influenced
by the selection of in-context demonstration im-
ages (Zhang et al., 2023a; Sun et al., 2023). To
address this, Zhang et al. (2023a) investigate two
approaches: (1) an unsupervised retriever that se-
lects nearest samples using an off-the-shelf model,
and (2) a supervised method training an additional
retriever model to maximize ICL performance. The
retrieved samples notably enhance performance, ex-
hibiting semantic similarity to the query and closer
contextual alignment regarding viewpoint, back-
ground, and appearance. Except for the prompt
retrieval, Sun et al. (2023) further explore a prompt
fusion technique for improving the results.
9.2 Multi-Modal In-Context Learning
In the vision-language area, Tsimpoukelli et al.
(2021) utilize a vision encoder to represent an im-
age as a prefix embedding sequence that is aligned
with a frozen language model after training on the
paired image-caption dataset. The resulting model,
Frozen, is capable of performing multi-modal few-
shot learning. Further, Alayrac et al. (2022) in-
troduce Flamingo, which combines a vision en-
coder with LLMs and adopts LLMs as the general
interface to perform in-context learning on many
multi-modal tasks. They show that training on
large-scale multi-modal web corpora with arbitrar-
ily interleaved text and images is key to endowing
them with in-context few-shot learning capabili-
ties. Kosmos-1 (Huang et al., 2023b) is another
multi-modal LLMs and demonstrates promising
zero-shot, few-shot, and even multimodal chain-
of-thought prompting abilities. Hao et al. (2022a)
present METALM, a general-purpose interface to
models across tasks and modalities. With a semi-
causal language modeling objective, METALM is
pretrained and exhibits strong ICL performance
across various vision-language tasks.
It is natural to further enhance the ICL abilitywith instruction tuning, and the idea is also ex-
plored in the multi-modal scenarios as well. Re-
cent explorations first generate instruction tuning
datasets transforming existing vision-language task
dataset (Xu et al., 2022; Li et al., 2023a) or with
power LLMs such as GPT-4 (Liu et al., 2023; Zhu
et al., 2023a) , and connect LLMs with powerful vi-
sion foundational models such as BLIP-2 (Li et al.,
2023c) on these multi-modal datasets (Zhu et al.,
2023a; Dai et al., 2023).
9.3 Speech In-Context Learning
In the speech area, Wang et al. (2023a) treated text-
to-speech synthesis as a language modeling task.
They use audio codec codes as an intermediate rep-
resentation and propose the first TTS framework
with strong in-context learning capability. Subse-
quently, V ALLE-X (Zhang et al., 2023b) extend the
idea to multi-lingual scenarios, demonstrating su-
perior performance in zero-shot cross-lingual text-
to-speech synthesis and zero-shot speech-to-speech
translation tasks.
3Takeaway :(1) Recent studies have explored
in-context learning beyond natural language with
promising results. Properly formatted data (e.g.,
interleaved image-text datasets for vision-language
tasks) and architecture designs are key factors
for activating the potential of in-context learning.
Exploring it in a more complex structured space
such as for graph data is challenging and promis-
ing (Huang et al., 2023a). (2) Findings in textual
in-context learning demonstration design and selec-
tion cannot be trivially transferred to other modal-
ities. Domain-specific investigation is required to
fully leverage the potential of in-context learning
in various modalities.
10 Application
ICL manifests excellent performance on traditional
NLP tasks and methods (Kim et al., 2022a; Min
et al., 2022b), such as machine translation (Zhu
et al., 2023b; Sia and Duh, 2023), information ex-
traction (Wan et al., 2023; He et al., 2023) and
text-to-SQL (Pourreza and Rafiei, 2023). Espe-
cially, through demonstrations that explicitly guide
the process of reasoning, ICL manifests remark-
able effects on tasks that require complexity reason-
ing (Wei et al., 2022c; Li et al., 2023b; Zhou et al.,
2022b) and compositional generalization (Zhou
et al., 2022a).
Moreover, ICL offers potential for popular meth-ods such as meta-learning and instruction-tuning.
Chen et al. (2022d) applied ICL to meta-learning,
adapting to new tasks with frozen model parame-
ters, thus addressing the complex nested optimiza-
tion issue. (Ye et al., 2023b) enhanced zero-shot
task generalization performance for both pretrained
and instruction-finetuned models by applying in-
context learning to instruction learning.
Specifically, we explore several emerging and
prevalent applications of ICL, showcasing their
potential in the following paragraphs.
Data Engineering ICL has manifested the po-
tential to be widely applied in data engineering.
Benefiting from the strong ICL ability, it costs 50%
to 96% less to use labels from GPT-3 than using la-
bels from humans for data annotation. Combining
pseudo labels from GPT-3 with human labels leads
to even better performance at a small cost (Wang
et al., 2021). In more complex scenarios, such as
knowledge graph construction, Khorashadizadeh
et al. (2023) has demonstrated that ICL has the po-
tential to significantly improve the state of the art of
automatic construction and completion of knowl-
edge graphs, resulting in a reduction in manual
costs with minimal engineering effort. Therefore,
leveraging the capabilities of ICL in various data
engineering applications can yield significant bene-
fits. Compared to human annotation (e.g., crowd-
sourcing) or noisy automatic annotation (e.g., dis-
tant supervision), ICL generates relatively high
quality data at a low cost. However, how to use ICL
for data annotation remains an open question. For
example, Ding et al. (2022) performed a compre-
hensive analysis and found that generation-based
methods are more cost-effective in using GPT-3
than annotating unlabeled data via ICL.
Model Augmentating The context-flexible na-
ture of ICL demonstrates significant potential to
enhance retrieval-augmented methods. By keep-
ing the LM architecture unchanged and prepend-
ing grounding documents to the input, in-context
RALMRam et al. (2023) effectively utilizes off-
the-shelf general-purpose retrievers, resulting in
substantial LM gains across various model sizes
and diverse corpora. Furthermore, ICL for retrieval
also exhibits the potential to improve safety. In ad-
dition to efficiency and flexibility, ICL also shows
potential in safety (Panda et al., 2023), (Meade
et al., 2023) use ICL for retrieved demonstrations
to steer a model towards safer generations, reduc-ing bias and toxicity in the model.
Knowledge Updating LLMs may contain out-
dated or incorrect knowledge, but ICL demon-
strates the potential for effectively editing and up-
dating this information. In an initial trial, Si et al.
(2022) found that GPT-3 updated its answers 85%
of the time when provided with counterfactual ex-
amples, with larger models performing better at
in-context knowledge updating. However, this ap-
proach may impact other correct knowledge in
LLMs. Compared to knowledge editing for fine-
tuned models (De Cao et al., 2021), ICL has proven
effective for lightweight model editing. Si et al.
(2022) explored the possibility of editing LLMs’
memorized knowledge through in-context demon-
strations, discovering that a larger model scale
and a mix of demonstration examples improved
ICL-based knowledge editing success rates. In a
comprehensive study, Zheng et al. (2023) investi-
gated ICL strategies for editing factual knowledge,
finding that well-designed demonstrations enabled
competitive success rates compared to gradient-
based methods, with significantly fewer side effects.
This underlines the potential of ICL for knowledge
editing.
11 Challenges and Future Directions
In this section, we review some of the existing chal-
lenges and propose possible directions for future
research on ICL.
11.1 New Pretraining Strategies
As investigated by Shin et al. (2022b), language
model objectives are not equal to ICL abilities. Re-
searchers have proposed to bridge the gap between
pretraining objectives and ICL through interme-
diate tuning before inference (Section 4), which
shows promising performance improvements. To
take it further, tailored pretraining objectives and
metrics for ICL have the potential to raise LLMs
with superior ICl capabilities.
11.2 ICL Ability Distillation
Previous studies have shown that in-context learn-
ing for reasoning tasks emerges as the scale of
computation and parameter exceed a certain thresh-
old (Wei et al., 2022b). Transferring the ICL ability
to smaller models could facilitate the model deploy-
ment greatly. Magister et al. (2022) showed that it
is possible to distill the reasoning ability to small
language models such as T5-XXL. The distillationis achieved by finetuning the small model on the
chain-of-thought data (Wei et al., 2022c) generated
by a large teacher model. Although promising per-
formance is achieved, the improvements are likely
task-dependent. Further investigation on improv-
ing the reasoning ability by learning from larger
LLMs could be an interesting direction.
11.3 ICL Robustness
Previous studies have shown that ICL performance
is extremely unstable, from random guess to SOTA,
and can be sensitive to many factors, including
demonstration permutation, demonstration format,
etc. (Zhao et al., 2021; Lu et al., 2022). The robust-
ness of ICL is a critical yet challenging problem.
However, most of the existing methods fall into
the dilemma of accuracy and robustness (Chen
et al., 2022c), or even at the cost of sacrificing
inference efficiency. To effectively improve the
robustness of ICL, we need deeper analysis of the
working mechanism of the ICL. We believe that
the analysis of the robustness of the ICL from a
more theoretical perspective rather than an empir-
ical perspective can highlight future research on
more robust ICL.
11.4 ICL Efficiency and Scalability
ICL necessitates prepending a significant number
of demonstrations within the context. However, it
presents two challenges: (1) the quantity of demon-
strations is constrained by the maximum input
length of LMs, which is significantly fewer com-
pared to fine-tuning (scalability); (2) as the number
of demonstrations increases, the computation cost
becomes higher due to the quadratic complexity of
attention mechanism (efficiency). Previous work in
§5 focused on exploring how to achieve better ICL
performance using a limited number of demonstra-
tions and proposed several demonstration design-
ing strategies. Scaling ICL to more demonstrations
and improving its efficiency remains a challenging
task.
Recently, some works have been proposed to ad-
dress the issues of scalability and efficiency of ICL.
Efforts were made to optimize prompting strate-
gies with structured prompting (Hao et al., 2022b),
demonstration ensembling (Khalifa et al., 2023),
dynamic prompting (Zhou et al., 2023), and itera-
tive forward tuning (Yang et al., 2023). Addition-
ally, Li et al. (2023d) proposed EVaLM with longer
context length and enhanced long-range language
modeling capabilities. This model-level improve-ment aims to improve the scalability and efficiency
of ICL. As LMs continue to scale up, exploring
ways to effectively and efficiently utilize a larger
number of demonstrations in ICL remains an ongo-
ing area of research.
12 Conclusion
In this paper, we survey the existing ICL literature
and provide an extensive review of advanced ICL
techniques, including training strategies, demon-
stration designing strategies, evaluation datasets
and resources, as well as related analytical studies.
Furthermore, we highlight critical challenges and
potential directions for future research. To the best
of our knowledge, this is the first survey about ICL.
We hope this survey can highlight the current re-
search status of ICL and shed light on future work
on this promising paradigm.
References
Ekin Akyürek, Dale Schuurmans, Jacob An-
dreas, Tengyu Ma, and Denny Zhou. 2022.
What learning algorithm is in-context learn-
ing? investigations with linear models. CoRR ,
abs/2211.15661.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Mal-
colm Reynolds, et al. 2022. Flamingo: a vi-
sual language model for few-shot learning. Ad-
vances in Neural Information Processing Sys-
tems, 35:23716–23736.
Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen,
Nanning Zheng, Jian-Guang Lou, and Dong-
mei Zhang. 2023. How do in-context exam-
ples affect compositional generalization? CoRR ,
abs/2305.04835.
Amir Bar, Yossi Gandelsman, Trevor Darrell,
Amir Globerson, and Alexei Efros. 2022. Vi-
sual prompting via image inpainting. Ad-
vances in Neural Information Processing Sys-
tems, 35:25005–25017.
Richard Bellman. 1957. A markovian decision
process. Journal of mathematics and mechanics ,
pages 679–684.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli,
Russ Altman, Simran Arora, Sydney von Arx,Michael S. Bernstein, Jeannette Bohg, Antoine
Bosselut, Emma Brunskill, Erik Brynjolfsson,
S. Buch, Dallas Card, Rodrigo Castellon, Ni-
ladri S. Chatterji, Annie S. Chen, Kathleen A.
Creel, Jared Davis, Dora Demszky, Chris Don-
ahue, Moussa Doumbouya, Esin Durmus, Ste-
fano Ermon, John Etchemendy, Kawin Etha-
yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale,
Lauren E. Gillespie, Karan Goel, Noah D.
Goodman, Shelby Grossman, Neel Guha, Tat-
sunori Hashimoto, Peter Henderson, John He-
witt, Daniel E. Ho, Jenny Hong, Kyle Hsu,
Jing Huang, Thomas F. Icard, Saahil Jain, Dan
Jurafsky, Pratyusha Kalluri, Siddharth Karam-
cheti, Geoff Keeling, Fereshte Khani, O. Khat-
tab, Pang Wei Koh, Mark S. Krass, Ranjay Kr-
ishna, Rohith Kuditipudi, Ananya Kumar, Faisal
Ladhak, Mina Lee, Tony Lee, Jure Leskovec,
Isabelle Levent, Xiang Lisa Li, Xuechen Li,
Tengyu Ma, Ali Malik, Christopher D. Man-
ning, Suvir P. Mirchandani, Eric Mitchell,
Zanele Munyikwa, Suraj Nair, Avanika Narayan,
Deepak Narayanan, Benjamin Newman, Allen
Nie, Juan Carlos Niebles, Hamed Nilforoshan,
J. F. Nyarko, Giray Ogut, Laurel Orr, Isabel
Papadimitriou, Joon Sung Park, Chris Piech,
Eva Portelance, Christopher Potts, Aditi Raghu-
nathan, Robert Reich, Hongyu Ren, Frieda
Rong, Yusuf H. Roohani, Camilo Ruiz, Jack
Ryan, Christopher R’e, Dorsa Sadigh, Shiori
Sagawa, Keshav Santhanam, Andy Shih, Kr-
ishna Parasuram Srinivasan, Alex Tamkin, Ro-
han Taori, Armin W. Thomas, Florian Tramèr,
Rose E. Wang, William Wang, Bohan Wu, Jia-
jun Wu, Yuhuai Wu, Sang Michael Xie, Michi-
hiro Yasunaga, Jiaxuan You, Matei A. Zaharia,
Michael Zhang, Tianyi Zhang, Xikun Zhang,
Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and
Percy Liang. 2021. On the opportunities and
risks of foundation models. ArXiv .
Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christo-
pher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners. In Ad-
vances in Neural Information Processing Sys-
tems 33: Annual Conference on Neural Infor-
mation Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual .
Stephanie C. Y . Chan, Adam Santoro, Andrew K.
Lampinen, Jane X. Wang, Aaditya Singh,
Pierre H. Richemond, Jay McClelland, and Fe-
lix Hill. 2022. Data distributional properties
drive emergent in-context learning in transform-
ers.CoRR , abs/2205.05055.
Mingda Chen, Jingfei Du, Ramakanth Pasunuru,
Todor Mihaylov, Srini Iyer, Veselin Stoyanov,
and Zornitsa Kozareva. 2022a. Improving in-
context few-shot learning via self-supervised
training. In Proceedings of the 2022 Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies , pages 3558–3573, Seattle,
United States. Association for Computational
Linguistics.
Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKe-
own, and He He. 2022b. On the relation between
sensitivity and accuracy in in-context learning.
ArXiv preprint , abs/2209.07661.
Yanda Chen, Chen Zhao, Zhou Yu, Kathleen R.
McKeown, and He He. 2022c. On the relation
between sensitivity and accuracy in in-context
learning. CoRR , abs/2209.07661.
Yanda Chen, Ruiqi Zhong, Sheng Zha, George
Karypis, and He He. 2022d. Meta-learning via
language model in-context tuning. In Proc. of
ACL, pages 719–730, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Aakanksha Chowdhery, Sharan Narang, Jacob De-
vlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, et al. 2022.
Palm: Scaling language modeling with pathways.
ArXiv preprint , abs/2204.02311.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Yunxuan Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha
Brahma, Albert Webson, Shixiang Shane Gu,
Zhuyun Dai, Mirac Suzgun, Xinyun Chen,
Aakanksha Chowdhery, Alex Castro-Ros, Marie
Pellat, Kevin Robinson, Dasha Valter, SharanNarang, Gaurav Mishra, Adams Yu, Vincent
Zhao, Yanping Huang, Andrew Dai, Hongkun
Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob
Devlin, Adam Roberts, Denny Zhou, Quoc V .
Le, and Jason Wei. 2022. Scaling instruction-
finetuned language models.
Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhi-
fang Sui, and Furu Wei. 2022. Why can gpt learn
in-context? language models secretly perform
gradient descent as meta-optimizers. CoRR ,
abs/2212.10559v2.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. 2023.
Instructblip: Towards general-purpose vision-
language models with instruction tuning. arXiv
preprint arXiv:2305.06500 .
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021.
Editing factual knowledge in language models.
InProc. of EMNLP , pages 6491–6506, Online
and Punta Cana, Dominican Republic. Associa-
tion for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding. In Proc. of NAACL-HLT , pages
4171–4186, Minneapolis, Minnesota. Associa-
tion for Computational Linguistics.
Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong
Bing, Shafiq Joty, and Boyang Li. 2022. Is gpt-3
a good data annotator?
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,
and Tushar Khot. 2022. Complexity-based
prompting for multi-step reasoning. CoRR ,
abs/2210.00720.
Shivam Garg, Dimitris Tsipras, Percy Liang, and
Gregory Valiant. 2022. What can transformers
learn in-context? A case study of simple function
classes. CoRR , abs/2208.01066.
Hila Gonen, Srini Iyer, Terra Blevins, Noah A
Smith, and Luke Zettlemoyer. 2022. Demystify-
ing prompts in language models via perplexity
estimation. ArXiv preprint , abs/2212.04037.
Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang.
2023. Pre-training to learn in context. arXiv
preprint arXiv:2305.09137 .Michael Hahn and Navin Goyal. 2023. A theory
of emergent in-context learning as implicit struc-
ture induction. CoRR , abs/2303.07971.
Yaru Hao, Haoyu Song, Li Dong, Shaohan
Huang, Zewen Chi, Wenhui Wang, Shuming
Ma, and Furu Wei. 2022a. Language models
are general-purpose interfaces. arXiv preprint
arXiv:2206.06336 .
Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han,
Yuxian Gu, and Furu Wei. 2022b. Structured
prompting: Scaling in-context learning to 1,000
examples. ArXiv preprint , abs/2212.06713.
Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu,
Xing Xu, and Heng Tao Shen. 2023. Icl-d3ie:
In-context learning with diverse demonstrations
updating for document information extraction.
arXiv preprint arXiv:2303.05063 .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes,
and Yejin Choi. 2020. The curious case of neural
text degeneration. In Proc. of ICLR . OpenRe-
view.net.
Or Honovich, Uri Shaham, Samuel R. Bowman,
and Omer Levy. 2022. Instruction induction:
From few examples to natural language task de-
scriptions. CoRR , abs/2205.10782.
Qian Huang, Hongyu Ren, Peng Chen, Gre-
gor Kržmanc, Daniel Zeng, Percy Liang, and
Jure Leskovec. 2023a. Prodigy: Enabling in-
context learning over graphs. arXiv preprint
arXiv:2305.12600 .
Shaohan Huang, Li Dong, Wenhui Wang, Yaru
Hao, Saksham Singhal, Shuming Ma, Tengchao
Lv, Lei Cui, Owais Khan Mohammed, Qiang
Liu, et al. 2023b. Language is not all you
need: Aligning perception with language models.
arXiv preprint arXiv:2302.14045 .
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pa-
sunuru, Todor Mihaylov, Daniel Simig, Ping
Yu, Kurt Shuster, Tianlu Wang, Qing Liu,
Punit Singh Koura, Xian Li, Brian O’Horo,
Gabriel Pereyra, Jeff Wang, Christopher Dewan,
Asli Celikyilmaz, Luke Zettlemoyer, and Ves
Stoyanov. 2022. Opt-iml: Scaling language
model instruction meta learning through the lens
of generalization.Muhammad Khalifa, Lajanugen Logeswaran,
Moontae Lee, Honglak Lee, and Lu Wang.
2023. Exploring demonstration ensembling for
in-context learning. In ICLR 2023 Workshop on
Mathematical and Empirical Understanding of
Foundation Models .
Hanieh Khorashadizadeh, Nandana Mihindukula-
sooriya, Sanju Tiwari, Jinghua Groppe, and Sven
Groppe. 2023. Exploring in-context learning
capabilities of foundation models for generat-
ing knowledge graphs from text. arXiv preprint
arXiv:2305.08804 .
Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim,
Taeuk Kim, Kang Min Yoo, and Sang-goo
Lee. 2022a. Self-generated in-context learn-
ing: Leveraging auto-regressive language mod-
els as a demonstration generator. ArXiv preprint ,
abs/2206.08082.
Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho,
Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,
Kang Min Yoo, and Taeuk Kim. 2022b. Ground-
truth labels matter: A deeper look into input-
label demonstrations. CoRR , abs/2205.12685.
Itay Levy, Ben Bogin, and Jonathan Berant.
2022. Diverse demonstrations improve in-
context compositional generalization. arXiv
preprint arXiv:2212.06800 .
Bo Li, Yuanhan Zhang, Liangyu Chen, Jing-
hao Wang, Jingkang Yang, and Ziwei Liu.
2023a. Otter: A multi-modal model with
in-context instruction tuning. arXiv preprint
arXiv:2305.03726 .
Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and
Zhi Jin. 2023b. Towards enhancing in-context
learning for code generation. arXiv preprint
arXiv:2303.17780 .
Junnan Li, Dongxu Li, Silvio Savarese, and Steven
Hoi. 2023c. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders
and large language models. arXiv preprint
arXiv:2301.12597 .
Mukai Li, Shansan Gong, Jiangtao Feng, Yi-
heng Xu, Jun Zhang, Zhiyong Wu, and Ling-
peng Kong. 2023d. In-context learning with
many demonstration examples. arXiv preprint
arXiv:2302.04931 .Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi
Zhou. 2023e. The closeness of in-context learn-
ing and weight shifting for softmax regression.
CoRR , abs/2304.13276.
Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei
Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang,
and Xipeng Qiu. 2023f. Unified demonstra-
tion retriever for in-context learning. CoRR ,
abs/2305.04320.
Xiaonan Li and Xipeng Qiu. 2023a. Finding sup-
porting examples for in-context learning. arXiv
preprint arXiv:2302.13539 .
Xiaonan Li and Xipeng Qiu. 2023b. Mot: Pre-
thinking and recalling enable chatgpt to self-
improve with memory-of-thoughts. CoRR ,
abs/2305.05181.
Yingcong Li, M. Emrullah Ildiz, Dimitris S. Papail-
iopoulos, and Samet Oymak. 2023g. Transform-
ers as algorithms: Generalization and implicit
model selection in in-context learning. CoRR ,
abs/2301.07067.
Haotian Liu, Chunyuan Li, Qingyang Wu, and
Yong Jae Lee. 2023. Visual instruction tuning.
arXiv preprint arXiv:2304.08485 .
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill
Dolan, Lawrence Carin, and Weizhu Chen. 2022.
What makes good in-context examples for GPT-
3? In Proceedings of Deep Learning Inside Out
(DeeLIO 2022): The 3rd Workshop on Knowl-
edge Extraction and Integration for Deep Learn-
ing Architectures , pages 100–114, Dublin, Ire-
land and Online. Association for Computational
Linguistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zheng-
bao Jiang, Hiroaki Hayashi, and Graham Neu-
big. 2021. Pre-train, prompt, and predict: A
systematic survey of prompting methods in
natural language processing. arXiv preprint
arXiv:2107.13586 .
Yao Lu, Max Bartolo, Alastair Moore, Sebastian
Riedel, and Pontus Stenetorp. 2022. Fantasti-
cally ordered prompts and where to find them:
Overcoming few-shot prompt order sensitivity.
InProc. of ACL , pages 8086–8098, Dublin, Ire-
land. Association for Computational Linguistics.Lucie Charlotte Magister, Jonathan Mallinson,
Jakub Adamek, Eric Malmi, and Aliaksei Sev-
eryn. 2022. Teaching small language models to
reason. ArXiv preprint , abs/2212.08410.
Nicholas Meade, Spandana Gella, Devamanyu
Hazarika, Prakhar Gupta, Di Jin, Siva Reddy,
Yang Liu, and Dilek Hakkani-Tür. 2023. Using
in-context learning to improve dialogue safety.
arXiv preprint arXiv:2302.00871 .
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2022a. Noisy channel lan-
guage model prompting for few-shot text clas-
sification. In Proc. of ACL , pages 5316–5330,
Dublin, Ireland. Association for Computational
Linguistics.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and
Hannaneh Hajishirzi. 2022b. MetaICL: Learn-
ing to learn in context. In Proceedings of the
2022 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies , pages
2791–2809, Seattle, United States. Association
for Computational Linguistics.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel
Artetxe, Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2022c. Rethinking the role of
demonstrations: What makes in-context learning
work? ArXiv preprint , abs/2202.12837.
Swaroop Mishra, Daniel Khashabi, Chitta Baral,
and Hannaneh Hajishirzi. 2021. Cross-task gen-
eralization via natural language crowdsourcing
instructions. arXiv preprint arXiv:2104.08773 .
Tai Nguyen and Eric Wong. 2023. In-context ex-
ample selection with influences. arXiv preprint
arXiv:2302.11042 .
Catherine Olsson, Nelson Elhage, Neel Nanda,
Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao
Bai, Anna Chen, Tom Conerly, Dawn Drain,
Deep Ganguli, Zac Hatfield-Dodds, Danny Her-
nandez, Scott Johnston, Andy Jones, Jackson
Kernion, Liane Lovitt, Kamal Ndousse, Dario
Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah. 2022. In-
context learning and induction heads. CoRR ,
abs/2209.11895.Johannes von Oswald, Eyvind Niklasson, Ettore
Randazzo, João Sacramento, Alexander Mordv-
intsev, Andrey Zhmoginov, and Max Vladymy-
rov. 2022. Transformers learn in-context by gra-
dient descent. ArXiv preprint , abs/2212.07677.
Jane Pan, Tianyu Gao, Howard Chen, and Danqi
Chen. 2023. What in-context learning "learns"
in-context: Disentangling task recognition and
task learning. CoRR , abs/2305.09731.
Ashwinee Panda, Tong Wu, Jiachen T Wang,
and Prateek Mittal. 2023. Differentially
private in-context learning. arXiv preprint
arXiv:2305.01639 .
Mohammadreza Pourreza and Davood Rafiei. 2023.
Din-sql: Decomposed in-context learning of
text-to-sql with self-correction. arXiv preprint
arXiv:2304.11015 .
Ofir Press, Muru Zhang, Sewon Min, Ludwig
Schmidt, Noah A. Smith, and Mike Lewis. 2022.
Measuring and narrowing the compositionality
gap in language models. CoRR , abs/2210.03350.
Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang
Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan,
Fei Huang, and Huajun Chen. 2022. Reason-
ing with language model prompting: A survey.
arXiv preprint arXiv:2212.09597 .
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Lan-
guage models are unsupervised multitask learn-
ers.
Pranav Rajpurkar, Robin Jia, and Percy Liang.
2018. Know what you don’t know: Unanswer-
able questions for SQuAD. In Proc. of ACL ,
pages 784–789, Melbourne, Australia. Associa-
tion for Computational Linguistics.
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor
Muhlgay, Amnon Shashua, Kevin Leyton-
Brown, and Yoav Shoham. 2023. In-context
retrieval-augmented language models. arXiv
preprint arXiv:2302.00083 .
Ohad Rubin, Jonathan Herzig, and Jonathan Be-
rant. 2022. Learning to retrieve prompts for
in-context learning. In Proceedings of the 2022
Conference of the North American Chapter of
the Association for Computational Linguistics:Human Language Technologies , pages 2655–
2671, Seattle, United States. Association for
Computational Linguistics.
Abulhair Saparov and He He. 2022. Language
models are greedy reasoners: A systematic for-
mal analysis of chain-of-thought. ArXiv preprint ,
abs/2210.01240.
Freda Shi, Mirac Suzgun, Markus Freitag,
Xuezhi Wang, Suraj Srivats, Soroush V osoughi,
Hyung Won Chung, Yi Tay, Sebastian Ruder,
Denny Zhou, et al. 2022. Language models are
multilingual chain-of-thought reasoners. ArXiv
preprint , abs/2210.03057.
Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sung-
dong Kim, HyoungSeok Kim, Boseop Kim,
Kyunghyun Cho, Gichang Lee, Woomyoung
Park, Jung-Woo Ha, and Nako Sung. 2022a. On
the effect of pretraining corpora on in-context
learning by a large-scale language model. In
Proceedings of the 2022 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies , pages 5168–5186, Seattle, United
States. Association for Computational Linguis-
tics.
Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sung-
dong Kim, HyoungSeok Kim, Boseop Kim,
Kyunghyun Cho, Gichang Lee, Woomyoung
Park, Jung-Woo Ha, and Nako Sung. 2022b. On
the effect of pretraining corpora on in-context
learning by a large-scale language model. In
Proceedings of the 2022 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies , pages 5168–5186, Seattle, United
States. Association for Computational Linguis-
tics.
Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng,
Danqi Chen, and He He. 2023. Measuring induc-
tive biases of in-context learning with underspec-
ified demonstrations. CoRR , abs/2305.13299.
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang
Wang, Jianfeng Wang, Jordan Boyd-Graber, and
Lijuan Wang. 2022. Prompting gpt-3 to be reli-
able. ArXiv preprint , abs/2210.09150.
Suzanna Sia and Kevin Duh. 2023. In-
context learning as maintaining coherency: Astudy of on-the-fly machine translation us-
ing large language models. arXiv preprint
arXiv:2305.03573 .
Taylor Sorensen, Joshua Robinson, Christopher
Rytting, Alexander Shaw, Kyle Rogers, Alexia
Delorey, Mahmoud Khalil, Nancy Fulda, and
David Wingate. 2022. An information-theoretic
approach to prompt engineering without ground
truth labels. In Proc. of ACL , pages 819–862,
Dublin, Ireland. Association for Computational
Linguistics.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya
Gupta, Adrià Garriga-Alonso, et al. 2022. Be-
yond the imitation game: Quantifying and ex-
trapolating the capabilities of language models.
ArXiv preprint , abs/2206.04615.
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuan-
jing Huang, and Xipeng Qiu. 2022. Black-box
tuning for language-model-as-a-service. ArXiv
preprint , abs/2201.03514.
Yanpeng Sun, Qiang Chen, Jian Wang, Jingdong
Wang, and Zechao Li. 2023. Exploring effective
factors for improving visual in-context learning.
arXiv preprint arXiv:2304.04748 .
Mirac Suzgun, Nathan Scales, Nathanael Schärli,
Sebastian Gehrmann, Yi Tay, Hyung Won
Chung, Aakanksha Chowdhery, Quoc V Le,
Ed H Chi, Denny Zhou, et al. 2022. Challenging
big-bench tasks and whether chain-of-thought
can solve them. ArXiv preprint , abs/2210.09261.
Eshaan Tanwar, Manish Borthakur, Subhabrata
Dutta, and Tanmoy Chakraborty. 2023. Mul-
tilingual llms are better cross-lingual in-context
learners with alignment. arXiv preprint
arXiv:2305.05940 .
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie
Baker, Yu Du, YaGuang Li, Hongrae Lee,
Huaixiu Steven Zheng, Amin Ghafouri, Marcelo
Menegali, Yanping Huang, Maxim Krikun,
Dmitry Lepikhin, James Qin, Dehao Chen,
Yuanzhong Xu, Zhifeng Chen, Adam Roberts,
Maarten Bosma, Yanqi Zhou, Chung-Ching
Chang, Igor Krivokon, Will Rusch, Marc Pickett,Kathleen S. Meier-Hellstern, Meredith Ringel
Morris, Tulsee Doshi, Renelito Delos Santos,
Toju Duke, Johnny Soraker, Ben Zevenber-
gen, Vinodkumar Prabhakaran, Mark Diaz, Ben
Hutchinson, Kristen Olson, Alejandra Molina,
Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi
Rajakumar, Alena Butryna, Matthew Lamm,
Viktoriya Kuzmina, Joe Fenton, Aaron Cohen,
Rachel Bernstein, Ray Kurzweil, Blaise Aguera-
Arcas, Claire Cui, Marian Croak, Ed H. Chi,
and Quoc Le. 2022. Lamda: Language mod-
els for dialog applications. ArXiv preprint ,
abs/2201.08239.
Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,
SM Eslami, Oriol Vinyals, and Felix Hill. 2021.
Multimodal few-shot learning with frozen lan-
guage models. Advances in Neural Information
Processing Systems , 34:200–212.
Karthik Valmeekam, Alberto Olmo, Sarath Sreed-
haran, and Subbarao Kambhampati. 2022. Large
language models still can’t plan (a bench-
mark for llms on planning and reasoning about
change). ArXiv preprint , abs/2206.10498.
Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying
Liu, Haiyue Song, Jiwei Li, and Sadao Kuro-
hashi. 2023. Gpt-re: In-context learning for re-
lation extraction using large language models.
arXiv preprint arXiv:2305.02105 .
Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. 2019.
Superglue: A stickier benchmark for general-
purpose language understanding systems. In
Advances in Neural Information Processing Sys-
tems 32: Annual Conference on Neural Infor-
mation Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada ,
pages 3261–3275.
Boshi Wang, Xiang Deng, and Huan Sun. 2022a.
Iteratively prompt pre-trained language models
for chain of thought. In The 2022 Conference
on Empirical Methods for Natural Language
Processing .
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang
Zhang, Long Zhou, Shujie Liu, Zhuo Chen,
Yanqing Liu, Huaming Wang, Jinyu Li, et al.
2023a. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint
arXiv:2301.02111 .
Lean Wang, Lei Li, Damai Dai, Deli Chen,
Hao Zhou, Fandong Meng, Jie Zhou, and
Xu Sun. 2023b. Label words are anchors:
An information flow perspective for under-
standing in-context learning. arXiv preprint
arXiv:2305.14160 .
Shuohang Wang, Yang Liu, Yichong Xu, Chen-
guang Zhu, and Michael Zeng. 2021. Want
to reduce labeling cost? GPT-3 can help. In
Findings of the Association for Computational
Linguistics: EMNLP 2021 , pages 4195–4205,
Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Xinlong Wang, Wen Wang, Yue Cao, Chunhua
Shen, and Tiejun Huang. 2023c. Images speak in
images: A generalist painter for in-context visual
learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recog-
nition , pages 6830–6839.
Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen
Wang, Chunhua Shen, and Tiejun Huang. 2023d.
Seggpt: Segmenting everything in context.
arXiv preprint arXiv:2304.03284 .
Xinyi Wang, Wanrong Zhu, and William Yang
Wang. 2023e. Large language models are implic-
itly topic models: Explaining and finding good
demonstrations for in-context learning. arXiv
preprint arXiv:2301.11916 .
Yaqing Wang and Quanming Yao. 2019. Few-shot
learning: A survey.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,
Alisa Liu, Noah A Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. 2022b. Self-instruct:
Aligning language model with self generated
instructions. ArXiv preprint , abs/2212.10560.
Yizhong Wang, Swaroop Mishra, Pegah Alipoor-
molabashi, Yeganeh Kordi, Amirreza Mirzaei,
Anjana Arunkumar, Arjun Ashok, Arut Sel-
van Dhanasekaran, Atharva Naik, David
Stap, Eshaan Pathak, Giannis Karamanolakis,
Haizhi Gary Lai, Ishan Purohit, Ishani Mondal,
Jacob Anderson, Kirby Kuznia, Krima Doshi,
Maitreya Patel, Kuntal Kumar Pal, Mehrad
Moradshahi, Mihir Parmar, Mirali Purohit,Neeraj Varshney, Phani Rohitha Kaza, Pulkit
Verma, Ravsehaj Singh Puri, Rushang Karia,
Shailaja Keyur Sampat, Savan Doshi, Siddhartha
Mishra, Sujan Reddy, Sumanta Patro, Tanay
Dixit, Xudong Shen, Chitta Baral, Yejin Choi,
Noah A. Smith, Hannaneh Hajishirzi, and Daniel
Khashabi. 2022c. Super-naturalinstructions:
Generalization via declarative instructions on
1600+ nlp tasks.
Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong
Shen, Pengcheng He, Weizhu Chen, Zhangyang
Wang, and Mingyuan Zhou. 2023f. In-context
learning unlocked for diffusion models. arXiv
preprint arXiv:2305.01115 .
Jason Wei, Maarten Bosma, Vincent Y . Zhao,
Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du, Andrew M. Dai, and Quoc V . Le. 2022a.
Finetuned language models are zero-shot learn-
ers. In Proc. of ICLR . OpenReview.net.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-
fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald
Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol
Vinyals, Percy Liang, Jeff Dean, and William Fe-
dus. 2022b. Emergent abilities of large language
models. CoRR , abs/2206.07682.
Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. 2022c. Chain of thought prompting elic-
its reasoning in large language models. ArXiv
preprint , abs/2201.11903.
Jerry Wei, Le Hou, Andrew Lampinen, Xiangn-
ing Chen, Da Huang, Yi Tay, Xinyun Chen,
Yifeng Lu, Denny Zhou, Tengyu Ma, et al.
2023a. Symbol tuning improves in-context
learning in language models. arXiv preprint
arXiv:2305.08298 .
Jerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Al-
bert Webson, Yifeng Lu, Xinyun Chen, Hanxiao
Liu, Da Huang, Denny Zhou, and Tengyu Ma.
2023b. Larger language models do in-context
learning differently. CoRR , abs/2303.03846.
Patrick H Winston. 1980. Learning and reason-
ing by analogy. Communications of the ACM ,
23(12):689–703.
Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiang-
tao Feng, Jingjing Xu, Yu Qiao, and ZhiyongWu. 2023. Openicl: An open-source framework
for in-context learning. CoRR , abs/2303.02913.
Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and
Lingpeng Kong. 2022. Self-adaptive in-context
learning.
Sang Michael Xie, Aditi Raghunathan, Percy
Liang, and Tengyu Ma. 2022. An explanation
of in-context learning as implicit bayesian infer-
ence. In Proc. of ICLR . OpenReview.net.
Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan
Lyu, Qiaoqiao She, and Yongdong Zhang. 2023a.
knn prompting: Learning beyond the context
with nearest neighbor inference. In International
Conference on Learning Representations .
Canwen Xu, Yichong Xu, Shuohang Wang, Yang
Liu, Chenguang Zhu, and Julian McAuley.
2023b. Small models are valuable plug-ins
for large language models. arXiv preprint
arXiv:2305.08848 .
Zhiyang Xu, Ying Shen, and Lifu Huang. 2022.
Multiinstruct: Improving multi-modal zero-shot
learning via instruction tuning. arXiv preprint
arXiv:2212.10773 .
Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei
Huang, and Yongbin Li. 2023. Iterative forward
tuning boosts in-context learning in language
models. arXiv preprint arXiv:2305.13016 .
Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu,
and Lingpeng Kong. 2023a. Compositional ex-
emplars for in-context learning. arXiv preprint
arXiv:2302.05698 .
Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang,
Hyeongu Yun, Yireun Kim, and Minjoon Seo.
2023b. In-context instruction learning. arXiv
preprint arXiv:2302.14691 .
Yiming Zhang, Shi Feng, and Chenhao Tan. 2022a.
Active example selection for in-context learning.
CoRR , abs/2211.04486.
Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu.
2023a. What makes good examples for vi-
sual in-context learning? arXiv preprint
arXiv:2301.13670 .
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2022b. Automatic chain of thoughtprompting in large language models. CoRR ,
abs/2210.03493.
Ziqiang Zhang, Long Zhou, Chengyi Wang,
Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen,
Yanqing Liu, Huaming Wang, Jinyu Li, et al.
2023b. Speak foreign languages with your own
voice: Cross-lingual neural codec language mod-
eling. arXiv preprint arXiv:2303.03926 .
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein,
and Sameer Singh. 2021. Calibrate before use:
Improving few-shot performance of language
models. In Proc. of ICML , volume 139 of Pro-
ceedings of Machine Learning Research , pages
12697–12706. PMLR.
Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan
Fan, Zhiyong Wu, Jingjing Xu, and Baobao
Chang. 2023. Can we edit factual knowl-
edge by in-context learning? arXiv preprint
arXiv:2305.12740 .
Denny Zhou, Nathanael Schärli, Le Hou, Jason
Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-
mans, Olivier Bousquet, Quoc Le, and Ed Chi.
2022a. Least-to-most prompting enables com-
plex reasoning in large language models. ArXiv
preprint , abs/2205.10625.
Hattie Zhou, Azade Nova, Hugo Larochelle,
Aaron C. Courville, Behnam Neyshabur, and
Hanie Sedghi. 2022b. Teaching algorithmic
reasoning via in-context learning. CoRR ,
abs/2211.09066.
Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan
Cotterell, and Mrinmaya Sachan. 2023. Effi-
cient prompting via dynamic in-context learning.
arXiv preprint arXiv:2305.11170 .
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen
Han, Keiran Paster, Silviu Pitis, Harris Chan, and
Jimmy Ba. 2022c. Large language models are
human-level prompt engineers. ArXiv preprint ,
abs/2211.01910.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li,
and Mohamed Elhoseiny. 2023a. Minigpt-4: En-
hancing vision-language understanding with ad-
vanced large language models. arXiv preprint
arXiv:2304.10592 .
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing
Xu, Lingpeng Kong, Jiajun Chen, Lei Li, andShujian Huang. 2023b. Multilingual machine
translation with large language models: Em-
pirical results and analysis. arXiv preprint
arXiv:2304.04675 .