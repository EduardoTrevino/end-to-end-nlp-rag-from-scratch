arXiv:2307.03201v1  [cs.LG]  5 Jul 2023ScalingLaws DoNot Scale
FernandoDiaz
Google
Pittsburgh,USA
diazf@acm.orgMichaelMadaio
Google
New York,USA
madaiom@google.com
ABSTRACT
Recentworkhasproposedapowerlawrelationship,referred toas
“scaling laws,” between the performance of artiﬁcial intel ligence
(AI) modelsandaspectsofthosemodels’design(e.g., datas etsize).
Inotherwords,asthesizeofadataset(ormodelparameters, etc)in-
creases, the performance of a given model trained on that dat aset
will correspondingly increase. However, while compelling in the
aggregate, this scaling law relationship overlooks the way s that
metrics usedto measure performancemay beprecarious and co n-
tested,ormaynotcorrespondwithhowdiﬀerentgroupsofpeo ple
mayperceivethequalityofmodels’output.Inthispaper,we argue
thatasthesizeofdatasetsusedtotrainlargeAImodelsgrow s,the
number of distinct communities (including demographic gro ups)
whosedatais includedinagiven datasetis likelytogrow,ea ch of
whommayhavediﬀerent values.Asaresult,thereisanincrea sed
riskthatcommunitiesrepresentedinadatasetmayhavevalu esor
preferencesnotcapturedby(orintheworstcase,atoddswit h)the
metrics used to evaluate model performance for scaling laws . We
end the paper with implications for AI scaling laws—that mod els
maynot,infact,continuetoimproveasthedatasetsgetlarg er—at
least notfor allpeopleorcommunities impactedbythosemod els.
1 INTRODUCTION
Recentstudieshaveinvestigatedtherelationshipbetween machine
learningmodelperformance—asmeasuredbysomeevaluation metric—
and model design variables like dataset size, number of mode l pa-
rameters, and compute, or what is commonly known as scaling
lawsfor AI [91]. In general, these analyses demonstrate a mono-
tonicrelationshipbetweendesignvariablesandmodelperf ormance—
e.g., as the dataset size increases, models will perform bet ter on
some metric computedover an evaluation dataset.This pheno me-
non is often presented graphically where the x-axis indicat es the
design variable and the y-axis indicates model performance , with
linessuggestingthesuperlinearrelationshipfordeeplea rningmod-
els.Thisrelationshiphasbeenusedtojustifythecollecti onofever-
largerdatasets(e.g.,theColossalCleanCrawledCorpus[5 3])used
totrainlargelanguage models[36, 124].
InthecontextofAIsystems thatareusedbyordirectlyimpac t
people, an evaluation metric, the dependent variable under lying
many scaling laws, reﬂects the performance or quality of the sys-
tem for those people. A metric, then, should not be considere d a
decontextualized property of a model, but rather a reﬂectio n of
thequalityofamodelusedforaparticulartask,inaspeciﬁc social
context. On the one hand, the mathematical form of a metric id e-
ally reﬂects what a system designer thinks is important to us ers
or others impacted by the system. The mathematical formula f or
a metric encodes assumptions about user beliefs, values, an d be-
havior. On the other hand, the evaluation data used to comput ethe metric represents—through a sampling procedure—the pa rtic-
ularpopulationofinterest.Incombination,themathemati calform
of a metric and the associated data estimate the performance of a
modelonaspeciﬁcpopulation.Thequantitycomputedbyanev al-
uationmetric,then,isshapedbothbyitsunderlyingmathem atical
assumptionsand samplingprocedures.
Just as a predictive model and its training data can be scruti -
nizedforbiases,wecan—andshould—scrutinizemetrics,es pecially
inlightoftheirroleinscalinglaws.Wecontendthat,whend esign-
ingforasuﬃcientlydiversepopulationofusers,measuring perfor-
mancewithasingle,universalmetricisprecarious.Speciﬁ cally,we
arguethatasthesizeoftheevaluationdatasetincreases, t henum-
ber of subpopulations present in the evaluation set is likel y to in-
creaseaswell.Bysubpopulations,wemeanmembersofpartic ular
identitygroups,includingdemographicgroups(e.g.,gend er,racial
orethnicgroups,religion,caste,nationalidentity,disa bilitystatus),
aswellasotherculturalorsociopoliticalgroups,amongth emany
other ways that human societies have variously organized th em-
selves.1Thesamplesusedtoevaluatemodelperformancewillthus
(intentionally or not) capture some set of communities in th e so-
cialcontextinwhichthedatawascollected.Assamplesizeg rows,
the likelihood of a larger number of communities included in the
evaluation set also grows. While more inclusive, comprehen sive
datasets might be desirable, the diverse communities thems elves
arelikelytoholddiﬀerentuses,behaviors,andvalueswith respect
tothemodelbeing evaluated.
Increasing evaluation dataset size increases the number of sub-
populationspresentandmultipliesthenumberofvaluestha tshould
be considered. Beyond the well-known challenges of develop ing
AI systems to be used by diverse subpopulations—such as eval u-
ating algorithmic unfairness in model performance with res pect
to a ﬁxed evaluation metric (e.g., equalized odds)—we argue that
diversesubpopulationsarelikelytohavediﬀerent and pote ntially
conﬂictingnotionsof‘good’asinstantiatedinboththemat hemat-
ical form and data behind an evaluation metric, which machin e
learning evaluations using single metrics often assume to b e uni-
versal. While evaluations of algorithmic unfairness may su rface
the systematic variation of the truerelationship between f eatures
andtargetsacrosssubpopulations,wecontendthatthetarg etsthem-
selves varyacross subpopulations.2
1In section 4.1, we return to the ways that the groupings of sub populations are not
inherent,butarealwaysimaginedandcreated—forexample, byresearcherscollecting
dataonthosepopulations,bythecommunitiesthemselves,o rbystateorinstitutional
actors,to name justafew.
2Note that prior work in algorithmic fairness has argued that fairness itself is fun-
damentallycontested bydiﬀerentcommunities,wherediﬀer entpopulations maynot
agree on how to best operationalize a construct such as fairn ess in terms of a single
fairnessmetric,suchasequalized odds [71,85]Fernando Diaz andMichael Madaio
Thecurrentlackofattentiononthesubpopulationsandcomm u-
nities represented in scaling lawevaluationdatasets3poses major
challenges forproponentsof AIscaling laws—and presents p oten-
tialrisksforallthoseimpactedbythedeploymentoflargem odels.
Despiteclaimsthatalargertrainingdataset(e.g.,acrawl ofthepre-
dominantly English-speaking Internet [53]) will lead to im proved
model performance, when such models are deployed at scale, t he
larger numbers of people included in the evaluation dataset —and
thusalargernumberofcommunities—mayleadtobreakdowns i n
modelperformancefordiﬀerentcommunities.Diﬀerentcomm uni-
ties of users or impacted stakeholders may necessitate eval uating
with diﬀerent metrics (due to diﬀerent behaviors or values) , each
of which may be in conﬂict with each other, or in conﬂict with
commonlyusedevaluation metrics.
We demonstrate that current AI scaling law analyses overloo k
thelargeanddiversesetofconstructsrequiredtotrulyass ess per-
formance for large and diverse sets of communities. As a resu lt,
scaling laws conceal sub-optimal performance for under-sa mpled
communities of users. In other words, despite claims to the c on-
trary, larger training datasets may not in fact lead to impro ved
modelperformanceforalluserswhen deployed atscale. Wedr aw
on scholarship from the social sciences to questionthe vali dity of
claims made about scaling laws for dataset size and model per for-
mance when considering their use in models deployed at scale ,
on global populations whose values may not be reﬂected in cur -
rent performance metrics or which may be in irreconcilable t en-
sionwitheachother.Becauseevaluationdatacanvaryinsiz eand
composition, we propose that, for a given metric and samplin g
procedure, scaling laws consider, in addition to the two axe s of
(training) dataset size and performance on a given metric, a third
axis indicating the sizeof the evaluation data set.Doing so allows
us to capture the change in compositionof evaluation data as the
sample size increases, be it an artifact of a small sample siz e or a
samplingstrategythatvarieswiththesize.Moreover,stud yingdy-
namicevaluationsetsisconsistentwithproductionsystem swhere
theevaluationsetdependsonthenumberandcompositionofs ys-
tem users,a populationthat technologydesigners hopewill grow
as a given system is morewidely adopted.
These observations suggest that scaling laws, in the pursui t of
general principles, canobscuresystematic under-perform ancefor
subpopulations and communities of people using or impacted by
these models.
2 SCALING LAWS
In order to scrutinize scaling laws from the perspectiveof e valua-
tion metrics, we ﬁrst review the relevant concepts from AI ev alu-
ation and scaling law literature. We will introducesome con cepts
frommeasurementmodeling[74,85]andintroducesomenotat ion
that wewillrefer tothroughoutouranalysis of scalinglaws .
2.1 PerformanceMetrics
EvaluationofAIsystemsinvolvescomputingandcomparingq uan-
titative measures of performance of a system on a task. In oﬄi ne
settings,includinglaboratoryorbenchmarkexperiments, researchers
3See prior work on understanding communities represented in other AI evaluation
datasets[e.g.,6, 7,14].useevaluationmetricsbasedondatalabeledthroughdedica tedan-
notators.Inonlinesettings, includingdeployed AI system s inpro-
ductionenvironments, organizationsuseevaluationmetri csbased
onlogged behavior data.In bothsettings, evaluation metri cs play
a critical role in guiding high-level research and model dev elop-
ment decisions as well as more granular parameter optimizat ion
[38,72,87,98].
In line with modern AI paradigms, we focus on the evaluation
ofmodelstrainedonasetofdata.Werepresentamodeltraine don
adataset Das/u1D70B(D).Forthepurposeofouranalysis,weareinter-
estedintherelationshipbetweendataandmodelperformanc eand
therefore will not specify the input or output space of /u1D70B(D), nor
do we care about the speciﬁc functional form or design of /u1D70B(D).
WeuseΠtorefertothespaceofalltrainedmodels,ofwhich /u1D70B(D)
is one member. For clarity, we will sometimes refer to a model as
/u1D70B, even though it is always the outcome of a training procedure
and data.
Genericmetricslikeaccuracyorsquarederrorareoftenado pted
in machine learning contexts to evaluate the quality of the m odel
output.Thesemetricsaresimple,well-understood,andusu allyamenable
todirectoptimization.FormanyAIsystems,however,gener icmet-
ricsdonotcapturehow peopleusethesystemoutput.Forinstance,
fora language modelusedfor question-answering, how reﬂec tive
isthemeasureof‘accuracyofpredictingthenext word’ofth eun-
derlying user goal of ‘understanding the answer to a question’?
For a recommender system, how reﬂective is the measure of ‘ac -
curacy of predicting a rating’ of the underlying user goal of ‘dis-
coveryofnew,relevant content’?Thereissubtletyinhowth esys-
tem output is used by people, which is lost when performance i s
measured with generic metrics. This is why, in applied conte xts,
domain-speciﬁc metrics are usually developed. Indeed, a va riety
of areas of research, including natural language processin g (e.g.,
BLEU [121, 127], ROUGE [62]), search and recommendation [30 ],
andmore[e.g.,125],haveadoptedfamiliesofmetricsinfor medby
technologyuse.
Evenwithinagivendomain,thequalityofmodels’outputmay
betightlycoupledwithauser’sspeciﬁctask;forexample,t hequal-
ity of a predictive typing application may be related to how u se-
ful users ﬁnd it in eﬀectively completing a writing task—but this
qualitymaydiﬀergreatlybetweendiﬀerentusecasesforthe same
task, such as (for instance) informal messages and creative writ-
ing tasks compared to professional communication or scient iﬁc
writingtasks.ForAIsystems,wearethusinterested ineval uation
metrics that measure—explicitly or not—the quality of a sys tem’s
output(e.g., predictions, decisions, recommendations) f or a given
populationof people for a given task.
Drawing on the measurement modeling literature [e.g., 74, 8 5],
we refer to the unobservable domain- and task-speciﬁc notio n of
model output quality as a construct, or, the true performance of
a system when used by a speciﬁc population for a speciﬁc task.
While in many cases, a construct is completely unobservable, in
others it may merely be very expensive to collect or more accu -
ratelyestimate. We deﬁne /u1D707∗(U,/u1D70B(D))as thelatentscalar value
associatedwiththequalityof /u1D70B(D)forapopulation U.Thispop-
ulation could be compact and well-deﬁned (e.g., ‘coworkers in aScaling Laws DoNot Scale
speciﬁcacademicdepartment’)orvagueandgeneral(e.g.,‘ anyper-
sonwith access toa computer,regardless of location,race, ethnic-
ity, age, religion, gender identity, sexual orientation, d isability, or
economicstatus’).Althoughitmaybediﬃcultorimpossible toob-
serve the latent construct, /u1D707∗, we can “operationalize” it using an
observable evaluation metric as a proxy [cf. 85, 145]. For example,
we can evaluate the qualityof a text prediction model’s outp utin
terms of its usefulness for a given writing task by, for insta nce,
measuring composition time, number of edits, the number of a c-
ceptedtextpredictionsuggestions,orusingdelayeduserf eedback
onthequalityof thetextpredictions [cf.129]—butall ofth ese are
onlyeverproxies(somebetterthanothers)forthelatentco nstruct
of interest, /u1D707∗.
To makethis clear,let /u1D707(/u1D448,/u1D70B(D))betheevaluationmetric for
a sample of user-associated data /u1D448∼ U(or, a sample /u1D448from a
population U).User-associateddata,oftenreferredtoas‘testdata,’
may consist of individual data (i.e., each data point or inst ance is
a person) or derived data (e.g., each data point is a text docu ment
writtenbya person).So,whenwerefer toanevaluationmetri c,it
approximates both thefunctional form of /u1D707∗as well as the associ-
ated target population U. In Section 4, we discuss the ways that
populations might impact the validity of evaluation metric s. Let
Mbethesetof allevaluationmetrics,(includingbetterandworse
proxies for the latent construct), that can be used for measu ring
systemperformance.Frommeasurement modeling,thevalidi tyof
an evaluation metric /u1D707is based on theextent to which it captures
the salient aspects of the latent construct /u1D707∗it is designed to op-
erationalize [85]. The better the variable captures the con struct,
the better able that metric is to assess the performance of a g iven
model.
Precisely howametricisrelatedtoaconstructcanbemeasured
in a variety of ways [85]. One approach measures the extent to
which a metric orders models consistently with the construc t.As-
sumewehaveasubset ˜Π⊂Πofmodelsusedtomeasurethemetric
quality.Ifwe coulddirectlyobservetheconstruct(whichwecanin
situationswhereitisobservablebutexpensive),wecouldc ompute
a/u1D707and/u1D707∗foreach modelin ˜Πinordertoconstructtworankings
of models,one ranking by /u1D707and another by /u1D707∗.We canthen mea-
sure the consistency in ranking models by /u1D707and/u1D707∗[94]. This is
related to Skalse et al. [141]’s proposal to deﬁne a pair of me trics
as incompatible if there is anydiscordance. Or, in the context of
binary-valued constructs, we might consider a metric incom pati-
ble with a construct if their values disagree; this is the app roach
adopted in analysis of fairness metrics [35, 59, 96]. Even in cases
where a metric and a construct show some correlation in gener al,
their relationship can still exhibit heteroscedasticity. For instance,
a metric may agree with a construct in some systematic way (e. g.
distinguishing a high quality system from a low quality syst em)
anddisagreeinothersystematicways(e.g.distinguishing between
twohigh qualitysystems).
Allofthissuggests thattherelationship betweenwhatwema y
wanttomeasureaboutAIsystems(e.g., /u1D707∗(U,/u1D70B(D)))andtheway
wemeasurethat(e.g., /u1D707(/u1D448,/u1D70B(D))) maynotbestraightforward.In
Section3, weunpack theways that this relationship breaks d own
in practiceand theimplications for AIscaling laws.2.2 ScalingLaws
Scaling law analyses use “learning curves” to represent the rela-
tionship between system performance and training set size ( i.e.,
thex-axis is asequenceoftraining datasets orderedbyincr easing
size|D/u1D456|<|D/u1D456+1|). In general, proponents of AI scaling laws ar-
gue that/u1D707∗(U,/u1D70B(D/u1D456))</u1D707∗(U,/u1D70B(D/u1D456+1)). However, this is based
on observations from proxy metrics ( /u1D707) instead of the construct
(/u1D707∗)andpopulationsamples( /u1D448)insteadofthetruetargetdistribu-
tion (U), as constructs and the full populationmay be diﬃcult or
impossibletodirectlyobserve.
Neuralscalinglawsstatethattherelationshipbetween |D/u1D456|and
/u1D707(/u1D448,/u1D70B(D/u1D456))follows a power law. In other words, as long as it is
notbottleneckedbyitscapacityoraccess tocomputeresour ces, a
model’sperformanceimproves superlinearlyas afunctiono fdata
[91, 131]. Although initial results demonstrated scaling l aws for
natural language processing tasks, similar laws have been d evel-
opedformultimodaland reinforcement learning models[33, 81].
Theevidenceofscalinglawshasmotivatedtheirusageinmod el
design.Theyhavebeenusedasmotivationforscrapingever- larger
trainingdatasets[53],todevelopquantizationmethods[4 9],toex-
trapolateperformancefromsmallerdatasets[84],andtode termine
dataminimization policies[137].
Despite their popularity, however, insuﬃcient attention t o the
relationship between latent constructs of model quality an d their
operationalizationinperformancemetrics forparticularcommuni-
ties included in evaluation datasets poses serious questions to the
validityofrelyingonscalinglawswhenevaluatingdeploye dmod-
els atscale.
3 THE PRECARITY OF METRICS
In order to understand precisely how scaling laws might be co m-
promised, we ﬁrst need to review the various ways in which met -
ricsarefarfromthe‘groundtruth’theyareoftenconsidere dtore-
ﬂect.Through a discussionand review of existing observati ons in
thecomputerscienceandsocialscienceliterature,wedemo nstrate
that evaluation metrics are inherently contestable and pre carious
andthat,atanyonepoint,theremaybemultiplemetricsand c on-
structs in tension. This echoes recent work in the responsib le AI
andcomputationalsocialsciencecommunitiesthatsimilar lyrecog-
nizesthat,althoughpresentedasareliableproxyfortheco nstruct,
evaluationmetrics areoftentenuous [59,85, 149,153].
3.1 Metric(In)compatibility
As discussed in Section 2.1, an evaluation metric an that app roxi-
mationoftheconstructofinterest.Ageneric metric likeac curacy
is often adoptedbecauseit can beused withoutmodiﬁcation. But,
foranygiventask,aconstructismorecomplicatedthansugg ested
byagenericmetric.Itmightbeimpactedbyanindividualuse r’sex-
pectations,interface constraints, or normative social va lues, none
of which are captured by a generic metric like accuracy. Beca use
metrics are errorful proxies for constructs, any two metric s, even
if they largely agree with the construct, may disagree with e ach
other.Moreover,becauseconstructsandhowtheyareoperat ional-
ized may be contested, two metrics may disagree because each is
modeling a diﬀerent (though related) underlying construct (e.g.,Fernando Diaz andMichael Madaio
twofairnessmetricsmaybeoperationalizingdiﬀerentunde rstand-
ings of fairness [35]).
As a result of this potential incompatibility between metri cs,
adopting any single metric for evaluating a model—includin g for
scaling law analyses—requires careful understanding of th e valid-
ityofthemetricwithrespecttotheconstructofinterest,a ndhow
that relationship may be more or less stable for various use c ases,
socialcontexts,orevendiﬀerentrangesofthemetric’sval ue.More-
over,understandingtherelationship betweenmultiplemetricsand
how this may also vary for diﬀerent ranges of the metrics’ val ues
helps designers avoid situations where a metric becomes unr eli-
able as model performance improves. While the concept of con -
vergent validity(i.e., themutualalignmentofasetofmetr icswith
a construct) is a desirable property, the inherent friction between
metricsmeansthatthisrelationshipmaybelessrigidthani mplied
byliteratureonscalinglaws,posingseriousquestionsfor whether
the performance metrics used in scaling law analyses adequa tely
reﬂectmodeloutputqualityforusersandothersimpactedby such
systems.
3.2 MetricNonstationarity
In addition to evaluation metrics being potentially incomp atible
with each other, their usefulness (in terms of their ability to cap-
ture a construct of interest) may also change over time. This non-
stationarityarises forthreereasons.
First, dueto the complexity of sociotechnical systems made up
ofdeployedAIsystemsusedbyorimpactingpeople,metricde velopment—
like modeldevelopment—is iterative, based on our understa nding
of how peopleand socialgroupsengage with thetechnology[3 1].
Inwebsearchevaluation,increasinglysophisticatedmode lsofuser
interaction inform evaluation metric development [27]. Si milarly,
in multidocument summarization, a better understanding of how
summaries are used by peoplehas led also to the development o f
new evaluationmetrics [138].Assuch,at anypointin time,a spe-
ciﬁc metric captures the researcher’s or practitioner’s be st under-
standing of how to model the construct. But, because our unde r-
standing of users and the world is changing, this metric will also
change over time.
Second, as suggested by the possible heteroscedastic relat ion-
shipbetweenmetricsandconstructs(Section2.1),the‘app ropriate
metric’ at any point in time may depend on the set of systems be -
ingcompared.Inpractice,thismeansthatthemostreliable evalua-
tionmetricmaychange dependingontheeﬀectiveness ofthes ys-
tems being evaluated. Because models conceivably improve o ver
time,metricswillbecomestaleandrequirereplacementwit hother,
more appropriate metrics [152]. Relatedly, some AI researc h, in-
spiredbyGoodhart’slaw,focusesonthefallibilityofprox ymetrics
withrespecttoatargetmetricorconstruct.Forinstance,G aoetal.
[64]usesimulationstodemonstratethatproxymetrics,whe nopti-
mized for by reinforcement learning systems, will initiall y be cor-
related with the construct, but this eventually degrades as the AI
system over-optimizes for the proxy. Similarly, Skalse et a l. [141]
showthatevensmalldiﬀerencesbetweenproxyandtargetmet rics
can result in over-optimization and degradation of perform ance
with respecttothetarget metric.Third,modeldevelopmentoftenexistsinabroadersociotec hni-
cal context, which itself might change over time. As an examp le,
considerametricsuchas‘consumptiontime,’ usedinanumbe rof
mediaplatformstomodelrelevancebymeasuringhowlongsom e-
one spends consuming media. The higher the consumption time
of a particular piece of media, the more relevant the item to t hat
user.Improvements to bandwidth and rendering times ondevi ces
can result in substantive changes in the relationship betwe en the
consumptiontimeandrelevance.Similarly,acuteshocksto theen-
vironment (e.g., unexpected exogenous events, such as disa sters,
orregulatorypolicychanges) canchangehowpeople,techno logy,
and their measured values behave and, as a result, relate to a con-
structofinterest[cf.136].Insomecases,themodelitself maylead
tobehavioralorenvironmentalchangesthatimpacttheusef ulness
ofthemetric.Forexample,usersofasystemmay,overtime,a dapt
theirbehaviorinwaysthatcompromiseassumptionsintheme tric
(e.g.,howpeopleexpressqueriesonmobilesearchengines[ 89,90]).
Or,so-calledadversariesmayattempttomanipulatethesys temby
gaming the metric (e.g., clickbait). We return to how metric s may
beimpactedbysociotechnical systems in section3.5.
3.3 MetricStaging
Multiplemetrics mayariseforpragmatic reasons. Forexamp le, in
the context of model development, diﬀerent metrics are used at
diﬀerent stages due to the costs involved in metric computat ion.
In production systems, behavioral metrics based on real int erac-
tions with end users (e.g., user engagement metrics) maybem ore
accurate reﬂections of a latent construct (e.g., due to face validity
[85]) but more expensive because—in addition to potential c osts
ofdeploymentordatacollection—theyincurariskofharmin gthe
users,losingtheirtrust,andhavingthemabandonthetechn ology
altogether.Onthe otherhand, in oﬄine evaluation, metrics based
on data labeled by annotators (e.g. precision, recall) are l ess ac-
curate reﬂections of the construct than behavioral metrics . That
said, they are extremely fast to compute, often reusable, an d can
beusedinanisolatedenvironmentwhereanyunexpectedorha rm-
fuloutputsmaybeinsulatedfromimpactingusers.4Unfortunately,
oﬄine evaluation requires an annotation budget and the deve lop-
mentofguidelinesforannotators,bothofwhichcanbeprohi bitive.
In practice, model development involves a coordination of t hese
metrics:usingoﬄineevaluationandassociatedmetricsfor alarge
set of models before selecting a subset for production evalu ation
and a diﬀerent set ofmetrics.
3.4 MetricVariationAcross Subtasks
Although an AI system intended for a speciﬁc task or domain is
oftenevaluatedusingasingleconstructormetric,inreali ty,there
are often a diversity of specialized subtasks or subdomains . For
example,asearchenginesupportsawidevarietyofpossible infor-
mationneeds,includingnavigatingtoaspeciﬁcwebpage,le arning
aboutatopic,ormakingapurchase[22].Question-answerin gsim-
ilarly can be decomposed into multiple classes of questions [109].
In this case, each subtask or subdomain has a unique construc t.
4Although suchharmfuloutputsarenot,crucially,prevente dfromimpactingannota-
tors[cf.70].Scaling Laws DoNot Scale
Thismaymanifestfromsomepropertyofthelabels(e.g.cont extu-
alizing labeled data on the subtask or subdomain) or the form ula
for the metric itself (e.g. in the search context, reciproca l rank for
navigationalintentsversusrecallforliteraturereview) .Insomesit-
uations,subtaskmetricsmaybecompatibleor‘compatiblee nough’
(see Section 3.1), allowing for shared information in learn ing and
evaluation [160]. In other situations, two subtasks may be h ave
quite diﬀerent constructs in tension [5]. While a single, un iversal
metric may be one way to resolve inconsistency amongst subta sk
or subdomain constructs or metrics, in a reinforcement lear ning
context, Skalse and Abate [140] demonstrate that jointly op timiz-
ing a single policyby reducingmultiplerewards (i.e. metri cs) toa
single number is onlypossiblefor linearlyinterpolatedre wards.
However, despite this variation across tasks, the dominant par-
adigm used in scaling law analyses (and in training large-sc ale
pre-trained models) is to optimize and evaluate for a single per-
formance metric.
3.5 MetricPower
Beyond the technical and pragmatic reasons for metric preca rity,
whenmetricsareusedtoguideresearchinacademiccommunit ies,
make decisions in industry,and evaluateperformanceof mac hine
learningsystems,theybecomesocialobjectsamongstresea rchers,
fundingagencies, engineers, productmanagers, andotheri ndivid-
ualsinvolvedintheprocessesofmachinelearningresearch ,devel-
opment,deployment,anduse.Asaresult,metricsusedtoeva luate
AIsystemsarealwayssubjectto(andinturn,shape)thesoci aldy-
namics of thesocioculturalsystems inwhich they are embedd ed.
When metrics are used to measure phenomena in social con-
texts,itis well-establishedthattheynotonlyenableonet ounder-
standaparticularphenomena,buttheyalsohavethepower[c f.13]
to change social actors’ behaviors. In other words, metrics do not
simplyreﬂecttheworld,buttheyshapeit,andareshapedbyi t[cf.
119] as well, as part of the previously mentioned Goodhart’s Law
[26, 66, 144]. This phenomena has been identiﬁed across nume r-
ousﬁelds,includingeducation[144],economics[107],and organi-
zationalstudies[69].Forinstance,whenschoolsusestude nts’test
scoresasametrictoevaluateteacherquality,someteacher sandad-
ministratorshaverespondedbyteachingtothetestorinthe worst
case,alteringstudents’testscores[61].Intheworkplace moregen-
erally, companies have long adopted performance metrics of var-
ious sorts to measure their employees’ productivity [126] a nd at-
tempt to incentivize them to be more productive [159]—metri cs
that are increasingly based on ﬁne-grained data from those e m-
ployees,andwhichmayshapeemployees’behaviorsinotherw ays
(e.g.,employeesusingapplicationstomovetheircursorto simulate
productivity)[17].
However, thespeciﬁcways thatmetrics impactpeople’sbeha v-
ior is itself shaped by the norms, culture, and organization al dy-
namicsofthecontextinwhichtheyareused.Forinstance,Ch ristin
[37] found that two newsrooms in France and the United States
thathadaccesstosimilarwebtraﬃcdataabouttheirnews sto ries
diﬀered greatly in how the metrics they derived from those da ta
shaped their approach to journalistic decision-making, in ways
impacted bytheir respective organizational, professiona l, and cul-
turalcontexts.In additionto the culturallyspeciﬁc ways that metrics may i m-
pact particular social worlds, metrics may take on a social l ife of
theirown.Withinresearch communities,metricscanhaveas tick-
iness when entire research programs develop around them. Th e
canonical example here is the evaluation and optimization o f rec-
ommender systems. For more than a decade, the majority of re-
search evaluated performance using various rating predict ion ac-
curacymeasures, rootmean squarederror(RMSE) beingcommo n
[80]. This was mathematically convenient and amenable to di rect
optimization. However, Cremonesi et al. [40] conducted a st udy
demonstrating that accuracy metrics were poorlycorrelate d with
usersatisfactionandarguedfortheirabandonment(cf.Sec tion3.1).
Despite these issues, RMSE continues to be used for some reco m-
mender system evaluation[128].
Allof thissuggests thattheperformancemetrics usedtoeva lu-
ate AI systems may be inherently unstable or precarious in wa ys
thatraiseseriousquestionsforthevalidityand robustnes s ofscal-
ing laws for AI, which rely on metrics that are believed to be d i-
vorced fromany particularsocialcontext.
4 SCALING LAWS DONOTSCALE
Wenowturntoananalysisofscalinglawsinlightofthepreca rity
of evaluation metrics. While there are existing initiative s to iden-
tify ‘inverse scaling laws’ for particular tasks [155], our claim is
that, when we consider training and evaluating with human da ta,
scaling laws,as currentlyposed,are, at best,incomplete, and may
befundamentally ﬂawed.
Our claim is divided into four parts. First, that evaluation met-
ricsreﬂectthecompositionoftheevaluationdataset,whic hisshaped
bythesamplingapproachusedtocollectthatdata;second,t hatthe
numberofsub-groupswithinagivendatasetgrowswithdatas ize;
third, those sub-groups can have incompatible values and pr efer-
ences for appropriateevaluation metrics; and fourth,that therisk
ofthatmetric incompabilitygrows withdataset size.
4.1 SamplingApproaches Shapewhois
Included in Evaluation Datasets
As we mentioned in Section 2.1, evaluations provide estimat es of
the performance of a model when used by an intended popula-
tion of users. Ideally both training and evaluation data are drawn
fromthesamepopulationusingthesamesamplingdistributi on(al-
thoughinpracticethismaynotbethecase).If Uisthepopulation,
then/u1D703deﬁnesthesamplingdistributionfromwhichwedrawboth
thetrainingset Dandtheevaluationset /u1D448.Andso,whendescrib-
inganevaluationmetric,wecandescribeit—or,parameteri zeit—in
termsofthesamplesize |/u1D448|.Forinstance,benchmarkingoroﬄine
experimentshaveaﬁxedsample(andthereforesamplesize)d eter-
minedbythecollected,staticevaluationdata.Indeployed systems,
the sample size varies with the number of users (and the train ing
datasize |D|).
Whilewehave talkedaboutmodelsevaluated whenusedbyor
impacting peoplefor speciﬁc tasks, we have avoided the ques tion
ofwhich people an AI system is designed for, used by, or impacts.
Given a current set of users (e.g., in a deployed system), we c an
answerthisquestionnarrowlybyevaluatingamodelwithres pect
to the existing set of users of a system or application that a g ivenFernando Diaz andMichael Madaio
model is embedded in; in this case, we say that the sampling di s-
tributionhas“support”constrainedtothesubpopulations reﬂected
bythecurrentsetofusers.Or,wecananswerthisaspiration allyby
evaluatingwithrespecttosomefuturepopulationofusers; inthis
case, we assume that the sampling distribution has support t hat
boundsallsubpopulationspresentinthecompletepopulati on.For
the purpose of our analysis, because of the aspirational nat ure of
claims made about AI scaling laws [e.g., 91], we assume that t he
supportofthesamplingdistributioniscomplete.Thisimpl iesthat
every possible user has a nonzero probability of occurring i n the
training or evaluationdata.
However,regardlessofthesamplingdistribution,inanysp eciﬁc
sample,werarely(ifever)havereliabledataforeveryposs ibleuser
in every context. Nevertheless, the desire for such a datase t has
led researchers toseek outlarger sources of data onhumanpo pu-
lations [3, 76, 95, 158]. Debates about whether and how diﬀer ent
groups of people may be represented by or within large datase ts
have persisted for decades, across multiple ﬁelds [cf. 32]. Social
scientists conducting public opinion surveys have long wre stled
with what it means for their samples to be representative (an d of
whom they might be representative), including, for instanc e, ex-
amining representativeness of survey respondents from var ious
demographics across geographic scales (e.g., from cities t o states
to national populations) [45]. Many theories and methods in the
socialscienceshavebeendevelopedtograpplewiththefund amen-
tal heterogeneity of human populations,as groups of people may
vary based ongender, age, race/ethnicity, disability stat us,as well
asbehavior,physiology,language,religion,culture,and numerous
other dimensions [56]. These dimensions of diﬀerence may th em-
selves be more or less stable, as peoplemay claim membership in
numerous communities at diﬀerent points in their lives, as s uch
identitiesbecomemoreorlesssalient(e.g.,apersonleavi ngorjoin-
ing a religion, moving toa new cityorcountry,joining orlea ving
the military, and numerous other examples). In other cases, the
community itself may spring into existence in response to a p ar-
ticularpoliticalissue(e.g.,DREAMers,NIMBYs,climatec hangeor
electiondeniers, and more) [cf52].
Depending on one’s question of interest, diﬀerent approach es
may be needed to understand how a sample captures variation
across a larger population [56]. That is, the means we use to c ap-
turethesediﬀerentdimensionsorcommunitiesareoftendes igned
based on the goal: political surveys may be designed to captu re
population-level variation across politically salient de mographic
categories, such as gender, race/ethnicity, education lev el, and in-
come,whilemarketingresearchsurveysmaybedesignedtosp ecif-
icallytargetconsumer-relevantdemographicsandbehavio rs,such
as familysize, technologyuse,spending behaviors,and mor e[45].
While many datasets about people, such as opinion polls, are
intentionally designed to answer speciﬁc research questio ns, the
advent oflarge-scaledatacollectionenabledbydatatrace sondig-
italplatformshasledcomputationalsocialscientistsand othersto
explore how such datasets may enable them to better understa nd
people[95,158].Despiteclaimsformassivedatasets(inth eformof
‘bigdata’)tousherin“theendoftheory”[3]viadatasetswh ere,al-
legedly,“n=all” [95],subsequentresearch has demonstratedthat
large-scaleplatformdatasetsarealwaysreﬂectionsofbeh aviorsofparticulargroups ofpeople(rather thanbeing somehow incl usive
ofeveryone) [21].
Aslargedatasetsfromsocialmediaplatforms(e.g.,Twitte r,Face-
book, Reddit, etc) are used to investigate research questio ns not
onlyaboutplatformuse,butaboutsocialdynamicsmoregene rally,
theseplatformsmaysuﬀerfroma‘modelorganism’problem[1 50],
where claims are made about the world in general, based on dat a
fromonespeciﬁcorganism,suchasmiceorfruitﬂies(orhere :one
speciﬁc social media platform). In reality, there is a non-r andom
selection of users into social media platforms [76]; for exa mple,
Twitter is used by less than 20% of the US population [106]. An
empiricalanalysisofwhoisleftoutofso-called“bigdata” fromso-
cialmediafoundthatsocialmediauserstendtobemoreeduca ted,
higher-income, and more technologically savvy than non-us ers,
withsubstantialdiﬀerencesingenderandrace/ethnicitya crossdif-
ferent platforms.[77].
Similarly, massive datasets used to train large models, suc h as
theColossalCleanCrawledCorpus(C4),trainedonacrawlof the
web, or others such as LAION [e.g., 19, 100], are not represen ta-
tiveofeveryone ontheplanet,butcontainparticularsnaps hotsof
particular populations (and not others). For instance, som e com-
munitiesmaynotbeincludedatallintheC4corpusorotherla rge
web-basedcorpora:low-literacycommunitiesorthosewhor elyon
radio for information and communication [e.g., 93]; commun ities
with low orno technologyuse—or thosewhose technologyusei s
primarilyonmobiledevices, and doesnotinvolve producing web-
basedcontentlegibletowebcrawlers[e.g.,1,112];orcomm unities
whospeak low-resourcelanguages [102],amongmany others.
As such, it is an empirical question as to who, precisely, is r ep-
resented in massive datasets used to train and evaluate mode ls
for scaling laws [cf. 14]. The composition of a dataset depen ds
onthesampling approach usedtocreateit—whether that is a r an-
dom populationsample, a sample stratiﬁed by some dimension (s)
(e.g., randomized within diﬀerent state populations), or a conve-
nience or platform sample (i.e., a sample shaped by the natur e
of the platform used to collect the data, such as Twitter, Red dit,
or the web). For all of these approaches, we cannot say with ce r-
taintypreciselyhowmanycommunitiesorsub-populationsa rere-
ﬂected in a given dataset without access to information abou t its
sampling or data collection methodology. Even then, becaus e the
deﬁnition of a given community and an individual’s membersh ip
init maybeﬂuid over timeorpotentiallyoverlapping orinte rsec-
tional[cf.39,41,114],thenumberofcommunitiesrepresen tedina
dataset may depend onthe research questions used toinvesti gate
thatquestion.
4.2 The Number ofSub-Groups in Evaluation
DatasetsGrowswithDatasetSize
Sinceevaluationmetricsareinﬂuencedbythesampledpopul ation
/u1D448,whichitselfmaybenon-representative foravarietyofrea sons,
oftenrelatedtothesamplingapproach,wenowaskwhathappe ns
as we increase the sample size. For this, we can introduce a th ird
‘z-axis’toscalinglawanalysis reﬂecting thesizeoftheev aluation
data,which,wewillshow,reﬂects thenumberofsub-populat ions
present.Scaling Laws DoNot Scale
Consideratheoreticalsamplingdistributionthataimstob erep-
resentative ofthepopulation U.Inthiscase,thesequenceofsam-
ples/u1D4480,/u1D4481,...will, although initially missing out on many sub-
groups, converge toward covering a broad, global set of peop le,
ostensibly representing all sub-groups in U, which is consistent
with the aspirational, though unrealistic, claims for larg e models
tobeneﬁt “all ofhumanity” or“thehumanrace” [2].
Asdiscussedintheprevioussectionthough,thesamplingfr ames
typicallyusedtoevaluateAIsystemsare notintentionallycollected
to be representative of any particular sub-group or communi ty—
nor is it clear what it would even mean for a sampling frame to
be representative of all sub-groups of people. This means th at al-
thoughsmallersamplesarelikelytocontainfewersub-grou psand
alargeenoughsamplemaytheoreticallyconvergetowardabr oad,
globalsetofsub-groups,therateatwhichthenumberofsub- groups
areencountered isdependent onpropertiesofthesamplingd istri-
butions, and the broader sampling approach taken. Given the ten-
dency for data collection practices to be biased toward comm uni-
tiesthatareeasier fortheresearchers collectingthedata toaccess
[cf. 101], or datasets that are easier to scrape [e.g., 53], t he rate
of observing new sub-groups willbe slower thana representa tive
sample.
More realistically, if sampling is done by ‘organic user gro wth,’
as is typical in production settings, the sampling distribu tion it-
self is changing as sample size increases. Consider deploye d sys-
tems, where early adopters of the technology will not be repr e-
sentative of later users. For example, user growth on social me-
dia platforms tends to occur non-uniformly within and acros s na-
tional boundaries [118, 122, 157]. Assuming consistent ado ption
and growth, we canuse a model ofnested subpopulationsuppor t,
supp(/u1D7030) ⊆supp(/u1D7031) ⊆...⊆ U. As suggested by social media
adoption,thisgrowthinsupportisstructured:wetendtose esome
populationsadoptbeforeothers duetohomophily.
Theseobservationssuggestthat,regardlessofthesamplin gstrat-
egy or the way we might represent a sequence of samples (e.g.,
the model of nested support), the number of unique sub-group s
present willgrow withsamplesize. However, cruciallyfors caling
laws, the natureof that growth—and thus the particular composi-
tion of sub-groups and communities contained in the evaluat ion
dataset—is heavily impacted by the sampling strategy used t ocol-
lecttheevaluation dataset.
4.3 Sub-GroupsCanHaveIncompatibleMetrics
Alongwiththesubstantialheterogeneityofpopulationsco meshet-
erogeneityofpreferencesandvalues.Althoughdiﬀerentsu b-populations
mayhavediﬀerentrelationshipswithasingleevaluationme tric(as
in disaggregated evaluations of algorithmic fairness [12, 24, 130]),
we are particularly interested in diﬀerent, incompatible m etrics
andconstructsthemselves[cf.71,85],asubtlebutimporta ntdiﬀer-
ence. For instance, it is well-established via decades of th e World
ValuesSurveythattherearetensionsinvaluesacrossinter national
populations[73]. In addition,there is large sub-national variation
in public opinion; for example, in the US, public opinion diﬀ ers
greatly on topics such as support for gay marriage [28], the N ew
Deal[29],andthedeathpenalty[139],amongothers[16],in ways
that areshapedbyvarious culturaland politicalfactors[2 8].InAIethics,priorworkhas identiﬁedsubstantialdiﬀerenc es in
how various populations’values manifest in terms of prefer ences
for AI systems. For instance, Jobin et al. [88] analyzed AI et hics
principles statements from nearly a hundred diﬀerent insti tutions
acrossvariouscountries,ﬁndingthatwhiletherewasconve rgence
in high-level values such as fairness and transparency, the re was
high divergence between countries in the speciﬁc ways those val-
ues are operationalized in AI principles statements, the pr actices
that enact those principles, and the mechanisms used to enfo rce
them.Inaddition,Awadetal.[9]collecteddataonmillions ofpeo-
ple’spreferencesforwhichoftwopersonasanautonomousve hicle
shouldkillinacaraccident,viaanonlinetoolusedin233co untries
and territories. They found substantial cross-cultural va riation in
preferences [9], and they attempted to explain that variati on by
drawing on various economic and culturalindicators, such a s the
World Values Survey [83], Gini coeﬃcient scores [54], and ot her
cultural frameworks [82, 104].5Relatedly, Jakesch et al. [86] con-
ductedasurveyofhowdiﬀerentgroupsprioritizeethicalva luesin
AIdevelopment,ﬁndingstatisticallysigniﬁcantdiﬀerenc esinhow
members ofdiﬀerent occupationsand demographic groups pri ori-
tizevaluessuchasfairness,privacy,andtransparencyinp articular
AI deployment scenarios.
Recent work has explored the relationship between diﬀerent
groups’responsestopublicopinionpolls(e.g.,PewAmeric anTrends
and the World Values Survey) and the output of large language
models, ﬁnding that large models’ output is more similar to t he
averageresponses from survey respondents in the USA, Canada,
andAustralia,comparedtorespondents fromothercountrie s[55]
andwithintheUS,languagemodels’outputreﬂectscertaing roups’
opinionpollresponses moreoftenthanothers[134].
As one example of how cultural diﬀerences in values may im-
pact AI development and evaluation, Sambasivan et al. [133] has
identiﬁed how algorithmic (un)fairness in the Indian conte xt op-
erates along diﬀerent axes than those identiﬁed in Western c on-
texts. For instance, they found that algorithmic fairness i n India
entails diﬀerent sets of sub-groups, frameworks, and metho ds, in-
cluding how algorithmic harms are shaped by the forces of cas te
and religion, among others. They discuss how popular fairne ss
measurements are informed byspeciﬁc culturaland historic al cir-
cumstances, such as approaches for measuring disparate imp act
ordisparatetreatmentarisingfromUSanti-discriminatio nlaw[cf.
154],andbyWesternphilosophicalframeworksandapproach esto
justicemoregenerally [133].
This suggests that large-scale datasets (which may contain nu-
merous communities or sub-groups) may thus inadvertantly c ol-
lapse meaningful diﬀerences in those sub-groups’ preferen ces for
how values in AI are operationalized—leading to what someha ve
referred to as “aggregation bias” [146]. There is substanti al em-
pirical evidence for how such aggregate approaches to evalu at-
ing models may hide disparities between (or within) subpopu la-
tions for a ﬁxed evaluation metric. To uncover these dispari ties,
5Although itisimportantto note thatthedatacollection met hod—a platformsample
basedonusersaccessinganinteractiveMITwebsite—isnotl ikelytoleadtonationally
representativesamples.Fernando Diaz andMichael Madaio
“dis-aggreggated evaluations” [12] of model performance a re con-
ductedby disaggregating a single performance metric acros s mul-
tiple groups. Such approaches have been the foundation of hi gh-
proﬁle evaluations of machine learning failures, such as ev alua-
tionsofhowgenderrecognitionsystemsarelessaccuratefo rwomen
ofcolorthanothers[23],orhowspeechrecognitionsystems have
higherworderrorratesforspeakersofAfrican-AmericanLa nguage
[97],amongother examples [e.g., 46,110, 111].
However,priorempiricalworkfoundthatwhenAIproducttea ms
are deploying models “at scale” (i.e., across numerous geog raphic
andculturalcontexts),choicesaboutprecisely howtodis-aggregate
evaluations of model performance—in terms of which evaluat ion
metrictouse,oralongwhichdemographicdimensionstodis- aggregate—
posed major obstacles to AI teams’ ability to eﬀectively con duct
fairness evaluations atscale[101].Indeed,concerns with aggrega-
tion bias are only ampliﬁed when we move from ﬁxed evaluation
metrics to subpopulation-speciﬁc metrics, which may be in t en-
sion.
Furthercontributingtopotentialaggregationharms,duri ngthe
dataannotationprocess,priorworkhasfoundsubstantiald isagree-
mentbetweenannotatorsfromdiﬀerentdemographicgroupsw hen
determining what constitutes hate speech or toxic language [44,
135, 148], with other work suggesting that it is crucial to un der-
stand the subjective identities of crowdworkers [48, 51], d evelop-
ing methods for handling disagreement between groups of ann o-
tatorsinsituationswheretheremaynotbeasingle“groundt ruth”
in annotationlabels[43, 67,68].
Scaling laws, which are, in essence, aggregate evaluations of
models’ performance across the entire evaluation dataset /u1D448, may
similarlyhidefailuresorinverserelationshipsamongstc onstructs
andvalueswhenevaluatedwithdiﬀerentsub-populationsco ntained
withintheevaluationdata.
4.4 Risk ofMetricIncompatibility Growswith
DataSize
Given that the number of sub-groups or communities within th e
evaluationdatasetgrowswiththesizeofthatdata,andthes egroups
may have incompatible values (i.e., constructs for model ou tput
quality)andrelationshipstoperformancemetricsthatope rational-
izethoseconstructs,weturntoourﬁnalclaim:thattherisk offail-
ures or harms ofAI systems grows as evaluationdata sizegrow s.
We assumethatan AIsystem is trained and evaluated ona sin-
gle, ‘dominant’ metric, the proxy for a construct of interes t, such
as model outputquality. Notwithstanding the fact that this proxy
may be insuﬃcient to model the construct, when used in social
contexts,weknowthatthevarietyofvaluesofdiﬀerentcomm uni-
ties or sub-groups means that many such values (e.g., constr ucts)
are not considered when AI system are evaluated with a single
evaluation metric,capturinga single construct.
Goodhart’s Law suggests that using a proxy metric can lead to
over-optimization and degradation of the actual performan ce on
theconstruct,somethingconﬁrmedintheAI alignment liter ature
[64,141].Similarly,usingasingledominantmetriccanlea dtoover-
optimization and degradation of the actual performance for other
constructsandvaluesofdiﬀerentcommunities,especially becausethey are more likely to be incompatible with the dominant con -
struct.Indeed,workin multi-tasklearning has demonstrat edthat,
whenoptimizingforonetask,moredatacandegradeperforma nce
for other tasks [5]. In addition, Solaiman and Dennison [143 ] ﬁnd
evidence for a scaling law between model size and toxicity—t hat
is, as model size increases, the models were morelikely to gener-
ate toxic language. Similarly, Lin et al. [99] found evidenc e for an
inverse scaling law for model size and truthfulness in a ques tion-
answering(QA)task(i.e.,modelswerelesstruthfulthelar gerthey
were), and Parrish etal.[116]foundthatlargermodelsperf ormed
worseonthetaskofdetectingbiasedlanguage,usingabiasb ench-
mark dataset they developed for QA. This phenomena has also
beenshownasthetrainingdatasetsizeisincreased,inaddi tionto
the model size. When analyzing the LAION datasets for the pre s-
ence of hateful content in images and alt-text, Birhane et al . [20]
found that as the dataset size increased, the likelihood for models
trainedonthosedatasetstolabelimagesofBlackpeople’sf acesas
criminals alsoincreased.
Sincethenumberofdistinctsub-populations(and thusthei rre-
spectivelatentconstructsformodelquality)represented inaneval-
uation set are likely to grow as a function of dataset size, th ere is
an increasing chance of a dramatically misaligned evaluati on of
model quality—leading to potential impacts or harms for com mu-
nities whose values are not represented by the dominant perf or-
mancemetric usedfor modelevaluation [e.g., 50,57,113].
Given that we might observe a disparity in ‘true performance ’
across populations in the evaluation data or, more generall y, in
thetarget population,we need toquantify theseverity of th is dis-
parity.Thesystematicunder-performanceandexclusionof values
fromsomesub-populationsinscalinglawanalysesraisesis suesof
(un)fairness and justice. While we emphasize that our claim s are
diﬀerent fromthoseintheexistingalgorithmicbiaslitera turethat
evaluate a ﬁxed metric for diﬀerent populations [e.g., 12, 2 3]—as
we are interested here in values tensions that might result i n dif-
ferent communities valuing diﬀerent constructs entirely o r diﬀer-
entproxymeasurestocapturethoseconstructs—wecanstill adopt
methodsfromthatcommunitytomeasuredisparity[11].From the
perspectiveofrobustness orRawlsianfairness, wecanlook atthe
worst case true performance of a system on a sub-population i n
termsofitsownvalues[65,79,92].Poorperformanceislike lytobe
ampliﬁed by the fact that the worst oﬀ sub-populations are li kely
to be from groups that historically have not been considered , rep-
resented, or participated in AI development processes [e.g ., 123]
(Section 4.2). Given thecatastrophic deteriorationof per formance
accordingtoGoodhart’slaw,othernotionsofunfairness (i nterms
ofincompatibilityofvaluesandthemetricsusedtooperati onalize
thosevalues)arealsomorelikelytooccurasmoresub-popul ations
manifestinevaluationdataandmodelperformanceaccordin gtoa
targetmetric grows.
5 DISCUSSION
5.1 Implications for existingapproaches to
scalinglaws
Proponents of scaling laws for AI systems argue for the exist ence
ofapowerlawrelationship betweenthesizeofamodel(i.e., num-
berofparameters,datasetsize,compute)anditsperforman ce(alongScaling Laws DoNot Scale
somemetric).Whilethisnarrativehasledtoincreasedinve stment
in collection of large datasets [19, 100] and in ever-larger models
andcomputepower—alongwithsupportinganarrativeofprog ress
akin to Moore’s Law—recent work has demonstrated that scali ng
laws maynot holdforparticulartasks [25,99,103,116,155] .
However, while the current work on exploring the limitation s
of scaling laws (e.g., inverse scaling laws [103], etc) has l argely
kept the same parameters—the relationship between some asp ect
of model size and model performance—and just uses a diﬀerent
task(e.g., generating truthful text, negation, bias detection , etc),
wearguethattheremaybeotherrelevantdimensionsalongwh ich
scalinglawsmaynothold.Forinstance,wearguethatscalin glaws
should incorporate a third dimension—a z-axis—drawing on t he
evaluationdataset(inadditiontothetypicalx-axis ofthe training
dataset).
Inaddition,whatwoulditlookliketoevaluatescalinglaws for
AIsystemswhereourproposed‘z-axis’,insteadofevaluati ondata
size,mightreﬂectthediﬀerentgroupsofpeoplewhosevalue s(and
thus latent metrics: /u1D707∗) may fundamentally diﬀer? This might in-
clude some combination of modeling scaling laws on the basis of:
1) the number of countries in which an AI system is deployed or
used;2)thenumberoflanguagesinwhichtheoutputisgenera ted;
3) the number of users; 4) the number of unique communities or
sub-populationsrepresented in thedataset.
Inaddition,givenourargumentthatdiﬀerentcommunitiesr ep-
resentedwithinadataset(orimpactedbyaparticularsyste m)may
have fundamentally diﬀerent values and metrics, what might it
look like to evaluate scaling laws where the y-axis, instead of de-
contextualizedmodelperformancemetricslikeaccuracy(o rF1score,
RMSE, ROUGE, etc), were instead chosen for particular use ca ses
or system deployment contexts, or were chosen by particular im-
pacted communities in participatory ways [e.g., 47, 147, 15 3] to
betterreﬂect theirvalues.
Aspreviouslydiscussed,substantialworkonmodelevaluat ions
hasshownthataggregatemetricsofmodelperformancemayhi de
worseperformanceforparticularsub-groupsthatcanbeobs erved
whenmodelperformanceis dis-aggregated bysomedemographic
categories [e.g., 23, 97, 110, 111]. Analogously, the curre nt para-
digm of evaluating scaling laws on aggregations of performa nce
metricsevaluatedonasingletrainingdatasetislikelytoh idesimi-
lardivergencesinvaluesandpreferencesformetricsforsu b-groups
withinanevaluationdataset.Forinstance,arecentpaperp roposed
abenchmarkforevaluatingbiasinQA,andevaluateditonsev eral
large language models; however, they caveat at the end of the pa-
per that “ the data in BBQ is only designed to test biases associated
withUSEnglish-speakingculturalcontexts,anditshouldn otbeused
asevidencethatamodelwouldstilllookunbiasedforcontex tsfroma
diﬀerent culture ” [116]. What would it look like for evaluations of
scalinglawstobedis-aggregatedfordatasetscollectedfr om(oride-
allycollected,curated,orannotated by) diﬀerent communities [cf.
153], be that linguistic communities, cultural communitie s, coun-
tries, or other set of sub-populations? In other words, woul d an
observed scaling lawrelationship for performancemetric /u1D707evalu-
ated with dataset /u1D448still hold if that dataset were collectedfrom a
diﬀerent context or collected or annotated by a diﬀerent com mu-
nity?5.2 Broader questions forscaling laws
While the previous section suggests some shorter-term mean s to
investigate thelimitsofscalinglaws,this workraisesmor efunda-
mentalquestionsforscalinglawsforAI.Forinstance,whil eanaly-
sesliketheonesweproposedinsection5.1mayreveal broken ,in-
verse, orothernon-monotonicallyincreasing functionalf ormsfor
scaling laws across diﬀerent communities within a given dat aset
(orwhomightuseorbeimpactedbyaparticularsystem),what to
doaboutthatisamuchthornierquestion.Inmanyways,tensi ons
in values (sometimes referred to as values pluralism [15, 42 , 151])
are a fundamental challenge of political systems. Methods h ave
been developed from participatory democracy [cf. 120], del ibera-
tion theory [58], and value-sensitive design [105] (among o ther
areas) to identify and resolve tensions in values. However, these
methods have largely been designed for smaller-scales: e.g ., town
halls,focusgroupdiscussions,andparticipatorydesignw orkshops
[108]. As such, it is not clear how such approaches may be able
to address value tensions at the scale of modern AI systems [c f.
75, 142].
In the ﬁeld of AI, this has manifested in tensions in how high-
levelprinciplesforethics(e.g.,fairness)areoperation alizedinterms
of speciﬁc practices or metrics, despite widespread agreem ent on
thehigh-level principleitself[e.g., 88].Foundationalw orkonbias
inrecidivismprediction[4]highlightedtheconsequences ofthis—
the original COMPAS algorithm was evaluated using predicti ve
parity across demographic groups, but subsequent research iden-
tiﬁeddisparities in falsepositives and false negatives fo r diﬀerent
demographic groups despite equivalent predictive parity ( due in
part to systemic injustice in the US criminal justice system ) [34].
How might such tensions in values be resolved when investiga t-
ingscaling laws forlargemodels?
Recent work on AI “alignment” has attempted to develop ap-
proachestoaligningAIwithhumanvalues[8,10,63,143].Ga briel
[60] discusses how values pluralism may impact the goal of al ign-
ing AI systems with “human values” (in all of their multiplic ity
and tensions), and he discusses tradeoﬀs in several potenti al ap-
proaches to resolving those tensions. Some currently adopt ed ap-
proaches include red-teaming [63], reinforcement learnin g from
humanfeedback(RLHF)[10]orcreating“values-targetedda tasets”
[143]. However, in recent work attempting to incorporate va lues
into training large models, people involved in red-teaming and
RLHF appear to largely be US-based and may not be representa-
tiveevenoftheUSpopulation[10,63]—despitepriorworkﬁn ding
eﬀects of annotators’ identity on their AI safety annotatio ns [6].
Meanwhile, in this example, the “values-targeted datasets ” were
createdbytheresearchers, whoacknowledgethislimitatio n,writ-
ing that “ creating many values-targeted datasets to reﬂect the cul-
tures of the many peoples impacted by language models is a diﬃ -
cult feat” [143]. However, this is not simply a limitation of their
work,butamorefundamentalchallengeforvaluestensionsi nscal-
ing laws—what might it mean to not only create values-target ed
datasetsfromdiﬀerentcommunitiesorculturalcontexts,b uttore-
solvepotentiallyirreconcilablediﬀerences invaluesbet weensuch
communities,at scale?
While we proposed a simple worst-case approach to quantify
therobustnessofamodelacrosssubpopulationsinSection4 .4,theFernando Diaz andMichael Madaio
realityofdealingwithmultiple,conﬂictingvaluesismore complex.
The evidence from Birhane et al. [20] demonstrates that valu es of
emergentsubpopulationscanbetoxic,suggestingthatsimp lylook-
ingattheworst-oﬀsubpopulationrisksbuttressingtoxicb ehavior.
Although we do not oﬀer answers to the questions we have
proposed in this paper, we suggest that, in part, what is need ed
are interdisciplinary, mixed-methods approaches to theor etically
andempiricallyinvestigatethequestionsweraise.Therea reexist-
ing theories and methods from numerous ﬁelds—across thesoc ial
sciences (including computational social science) and com puter
science—that have been developed to explorequestions rela ted to
whetherandinwhatwaysvariouscommunitiesorsub-groupsa re
represented bydata[e.g., 158]aswellasidentifying andre solving
values tensions among communities [e.g., 105]. For instanc e, vari-
ousmethodshavebeendevelopedforidentifying sub-popula tions
withinlargedatasetsaboutpeople,inpoliticalscience(e .g.,forpub-
lic opinion polling) [56], demography [115], healthcare [1 56], ge-
netics[117],andmore.However,workfromthesocialscienc essug-
geststhatsomecommunitiesmaybehiddeninwaysnotlegible to
data available for computational community detection(e.g ., injec-
tion drugusers; sex workers) [132], requiring approaches s uch as
qualitativeorethnographicresearchtoidentifysuchcomm unities—
whichmaybediﬃcultandprohibitivelyexpensiveincurrent prac-
ticeatthescalesimplicatedbyAIscalinglaws,nottomenti onthat
manysuchcommunitiesmaynotwanttohavedatacollectedata ll,
as it mayputthem atrisk.
Inaddition,recent workdemonstratingthelackofrelevanc e of
Western AI fairness frameworks for other cultural contexts , such
asIndia,hasdrawnonapproachesfrombothqualitative[133 ]and
quantitative research paradigms, including natural langu age pro-
cessing[18].Wearguethatsuchinterdisciplinary,mixed- methods
approaches such as these, involving deep partnerships with com-
munity members or community organizations, such as partici pa-
tory or community-based research methods [e.g., 47, 78, 108 , 133,
153] may be one way to empirically investigate our claims and
grapplewith theinherent tensions and limitations of scali ng laws
for AI.
5.3 Conclusion
AnalysesofAIscalinglawspaintanimpressivepictureofpr ogress
inAI drivenbyincreasing scales ofdata,modelsize,and com pute
power. We argue that these results are not the wholepicture. We
drawtogetherworkfromcomputerscienceandthesocialscie nces
toidentifythewaysthatmetricsusedtoevaluateAI systems may
beunstableorprecarious;howtheincreasingscalesofdata usedto
train AI systems entail increasing numbers of sub-populati ons or
communities of people; and how those groups’ values and pref er-
encesforAIsystemsandthemetricsusedtoevaluatethemmay be
incompatible or fundamentally at odds with each other. We cl ose
by discussing how these insights pose fundamental challeng es to
the paradigm of AI scaling laws, and we raise openquestions a nd
opportunities for the ﬁeld to investigate whether, when, an d forwhomscaling laws may (ormaynot) hold.Wesuggest opportuni -
ties for interdisciplinary, participatory, and community -based re-
search to better understand which sub-populations or commu ni-
ties may be represented in a given dataset (or impacted by a pa r-
ticular model); which evaluation metrics might best reﬂect their
values;andhowtoconductsuchevaluationsorresolvetensi onsin
thosevalues.Theresultsofthisworkmaysuggestthatprogr essin
large AI models is not quite as straightforward as it ﬁrst app ears.
Inotherwords,one might ask: progress for whom?
REFERENCES
[1] Jenny C Aker and Isaac M Mbiti. Mobile phones and economic development
inafrica. Journal of economicPerspectives ,24(3):207–232, 2010.
[2] SamAltman. Planning for agiand beyond. OpenAI Blog,February ,2023.
[3] Chris Anderson. The end of theory: The data deluge makes t he scientiﬁc
method obsolete. Wired magazine , 16(7):16–07, 2008.
[4] JuliaAngwin,JeﬀLarson,SuryaMattu,andLaurenKirchn er. Machinebias. In
Ethicsof data and analytics , pages254–264.AuerbachPublications,2016.
[5] VamsiAribandi,YiTay,TalSchuster,JinfengRao,Huaix iuStevenZheng, San-
ketVaibhavMehta,Honglei Zhuang, VinhQ. Tran,DaraBahri, JianmoNi,Jai
Gupta,Kai Hui, SebastianRuder,and Donald Metzler. Ext5: T owardsextreme
multi-taskscalingfortransferlearning.In InternationalConferenceonLearning
Representations ,2022.
[6] LoraAroyo, Mark Díaz,Christopher Homan, Vinodkumar Pr abhakaran,Alex
Taylor, and Ding Wang. The reasonable eﬀectiveness of diver se evaluation
data.arXiv preprintarXiv:2301.09406 ,2023.
[7] Lora Aroyo, Alex S Taylor, Mark Diaz, Christopher M Homan , Alicia Par-
rish, Greg Serapio-Garcia,Vinodkumar Prabhakaran, and Di ng Wang. Dices
dataset: Diversity in conversational ai evaluation for saf ety.arXiv preprint
arXiv:2306.11247 ,2023.
[8] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ga nguli, Tom
Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSa rma, et al.
A general language assistant as a laboratory for alignment. arXiv preprint
arXiv:2112.00861 ,2021.
[9] Edmond Awad, SohanDsouza,RichardKim,JonathanSchulz ,JosephHenrich,
Azim Shariﬀ,Jean-FrançoisBonnefon, and Iyad Rahwan. The m oral machine
experiment. Nature,563(7729):59–64, 2018.
[10] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, A nna Chen, Nova
DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Hen ighan, et al.
Training a helpful and harmless assistant with reinforceme nt learning from
humanfeedback. arXiv preprint arXiv:2204.05862 ,2022.
[11] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and
Machine Learning: Limitations and Opportunities . fairmlbook.org, 2019.
http://www.fairmlbook.org.
[12] Solon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones , Meredith Ringel
Morris, Jennifer Wortman Vaughan, W Duncan Wadsworth, and H anna Wal-
lach. Designing disaggregated evaluations of ai systems: C hoices, considera-
tions, and tradeoﬀs. In Proceedings of the 2021 AAAI/ACM Conference on AI,
Ethics,and Society , pages368–378,2021.
[13] DavidBeer. Metricpower . Springer,2016.
[14] AStevieBergman,LisaAnneHendricks,MaribethRauh,B oxiWu,WilliamAg-
new, Markus Kunesch, Isabella Duan, Iason Gabriel, and Will iam Isaac. Rep-
resentation in ai evaluations. In Proceedings of the 2023 ACM Conference on
Fairness,Accountability,and Transparency ,pages519–533,2023.
[15] IsaiahBerlin. Four essaysonliberty. 1969.
[16] Matthew Berman and Robert W Orttung. Measuring progres s toward urban
sustainability:Do global measuresworkfor arcticcities? Sustainability , 12(9):
3708,2020.
[17] Ethan S Bernstein. Making transparencytransparent: T he evolution of obser-
vationinmanagementtheory. AcademyofManagementAnnals ,11(1):217–266,
2017.
[18] ShailyBhatt,SunipaDev,ParthaTalukdar,ShachiDave ,andVinodkumarPrab-
hakaran. Cultural re-contextualization of fairness resea rch in language tech-
nologies inindia. arXiv preprint arXiv:2211.11206 ,2022.
[19] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembw e. Multimodal
datasets: misogyny, pornography, and malignant stereotyp es.arXiv preprint
arXiv:2110.01963 ,2021.
[20] AbebaBirhane,VinayPrabhu,SangHan,andVishnuNares hBoddeti. Onhate
scalinglawsfor data-swamps,2023.
[21] danah boyd and Kate Crawford. Critical questions for bi g data: Provocations
foracultural,technological, andscholarlyphenomenon. Information,commu-
nication& society ,15(5):662–679, 2012.
[22] AndreiBroder. A taxonomy of websearch. SIGIR Forum ,36(2):3–10, 2002.Scaling Laws DoNot Scale
[23] Joy Buolamwini and Timnit Gebru. Gender shades: Inters ectional accuracy
disparities in commercial gender classiﬁcation. In Conference on fairness, ac-
countability and transparency ,pages77–91. PMLR,2018.
[24] Ryan Burnell, Wout Schellaert, John Burden, Tomer D. Ul lman, Fernando
Martinez-Plumed,JoshuaB.Tenenbaum,DanajaRutar,LucyG .Cheke,Jascha
Sohl-Dickstein, Melanie Mitchell, Douwe Kiela, Murray Sha nahan, Ellen M.
Voorhees, Anthony G. Cohn, Joel Z. Leibo, and Jose Hernandez -Orallo. Re-
think reporting of evaluationresultsinai. Science,380(6641):136–138, 2023.
[25] Ethan Caballero,Kshitij Gupta, Irina Rish, and David K rueger. Broken neural
scaling laws,2022. URLhttps://arxiv.org/abs/2210.1489 1.
[26] DonaldTCampbell.Assessingtheimpactofplannedsoci alchange. Evaluation
and programplanning , 2(1):67–90,1979.
[27] BenCarterette,EvangelosKanoulas,andEmineYilmaz. Advancesonthedevel-
opment of evaluation measures. In Proceedings of the 35th International ACM
SIGIR Conference on Research and Development in Informatio n Retrieval , SIGIR
’12,pages1200–1201,NewYork,NY, USA, 2012.ACM.
[28] DevinCaugheyand ChristopherWarshaw. Public opinion insubnationalpol-
itics.Journal of Politics ,81(1), 2019.
[29] Devin Caughey, Michael Dougal, and Eric Schickler. The policy bases of the
newdealrealignment:Evidencefrompublicopinionpolls,1 936–1952. Journal
of Politics ,2018.
[30] Praveen Chandar, Fernando Diaz, and Brian St. Thomas. B eyond accuracy:
Grounding evaluation metrics for human-machine learning s ystems. In Ad-
vances in Neural Information ProcessingSystems ,2020.
[31] Praveen Chandar, Fernando Diaz, and Brian St. Thomas. B eyond accuracy:
Grounding evaluation metrics for human-machine learning s ystems. In Ad-
vances in Neural Information ProcessingSystems ,2020.
[32] Kyla Chasalow and Karen Levy. Representativeness in st atistics, politics, and
machine learning. In Proceedings of the 2021 ACM Conference on Fairness, Ac-
countability, and Transparency ,pages77–89, 2021.
[33] MehdiCherti,RomainBeaumont,RossWightman,Mitchel lWortsman,Gabriel
Ilharco,CadeGordon,ChristophSchuhmann,LudwigSchmidt ,andJeniaJitsev.
Reproduciblescalinglawsforcontrastivelanguage-image learning,2022. URL
https://arxiv.org/abs/2212.07143.
[34] Alexandra Chouldechova. Fair prediction with dispara te impact: A study of
biasinrecidivismpredictioninstruments. Big data, 5(2):153–163, 2017.
[35] Alexandra Chouldechova. Fair prediction with dispara te impact: A study
of bias in recidivism prediction instruments. Big Data, 5(2):153–163, 2017.
doi: 10.1089/big.2016.0047. URL https://doi.org/10.108 9/big.2016.0047. PMID:
28632438.
[36] AakankshaChowdhery,SharanNarang,JacobDevlin,Maa rtenBosma,Gaurav
Mishra,Adam Roberts, Paul Barham,Hyung Won Chung, Charles Sutton, Se-
bastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashc henko, Joshua
Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, V inodkumar
Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pop e, James Brad-
bury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Y in, Toju Duke,
AnselmLevskaya,SanjayGhemawat,SunipaDev,HenrykMicha lewski,Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zho u, Daphne Ip-
polito, DavidLuan,Hyeontaek Lim,BarretZoph, AlexanderS piridonov,Ryan
Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andr ew M. Dai,
Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Er-
ica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee , Zongwei Zhou,
Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Ja-
son Wei, Kathy Meier-Hellstern, Douglas Eck, Jeﬀ Dean, Slav Petrov, and
Noah Fiedel. Palm: Scaling language modeling with pathways , 2022. URL
https://arxiv.org/abs/2204.02311.
[37] Angèle Christin. Metrics at Work: Journalism and the Contested Meaning of
Algorithms . Princeton UniversityPress,2020.
[38] Paul R.Cohen. Empiricalmethods forartiﬁcial intelligence . MITPress,1995.
[39] PatriciaHill Collins. Intersectionality as criticalsocial theory . DukeUniversity
Press,2019.
[40] Paolo Cremonesi, Yehuda Koren, and Roberto Turrin. Per formance of
recommender algorithms on top-n recommendation tasks. In Proceed-
ings of the Fourth ACM Conference on Recommender Systems , RecSys ’10,
pages 39–46, New York, NY, USA, 2010. Association for Comput ing Ma-
chinery. ISBN 9781605589060. doi: 10.1145/1864708.18647 21. URL
https://doi.org/10.1145/1864708.1864721.
[41] KimberleCrenshaw. Mapping the margins:Intersection ality, identity politics,
and violence againstwomen ofcolor. Stan. L.Rev. ,43:1241, 1990.
[42] GeorgeCrowder. Liberalismandvaluepluralism . BloomsburyPublishing,2002.
[43] AidaMostafazadehDavani,MarkDíaz,andVinodkumarPr abhakaran.Dealing
with disagreements: Looking beyond the majority vote in sub jective annota-
tions.Transactionsof the Associationfor Computational Linguis tics,10:92–110,
2022.
[44] Thomas Davidson, Dana Warmsley,Michael Macy, and Ingm ar Weber. Auto-
matedhatespeechdetectionandtheproblemofoﬀensivelang uage.InProceed-
ings of the international AAAI conference on web and social m edia, volume 11,
pages512–515, 2017.[45] Scott de Marchi and Scotte E Page. Computational social science: Discovery
and prediction. edited by r. michael alvarez. new york: Camb ridge university
press,2016.337p. 34.99paper. Perspectives on Politics ,14(4):1169–1170, 2016.
[46] TerranceDeVries,IshanMisra,ChanghanWang,andLaur ensVanderMaaten.
Does object recognition work for everyone? In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition work shops, pages 52–59,
2019.
[47] FernandoDelgado,StephenYang,MichaelMadaio,andQi anYang.Stakeholder
participationin ai: Beyond" add diversestakeholders and s tir".arXiv preprint
arXiv:2111.01122 ,2021.
[48] Emily Denton, Mark Díaz, Ian Kivlichan, Vinodkumar Pra bhakaran, and
Rachel Rosen. Whose ground truth? accounting for individua l and collective
identitiesunderlyingdatasetannotation. arXivpreprintarXiv:2112.04554 ,2021.
[49] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit pr ecision: k-bit infer-
encescaling laws,2022. URL https://arxiv.org/abs/2212. 09720.
[50] SunipaDev,MasoudMonajatipoor,AnaeliaOvalle,Arju nSubramonian,JeﬀM
Phillips, and Kai-Wei Chang. Harms of gender exclusivity an d challenges
in non-binary representation in language technologies. arXiv preprint
arXiv:2108.12084 ,2021.
[51] MarkDíaz,IanKivlichan,RachelRosen,DylanBaker,Ra zvanAmironesei,Vin-
odkumarPrabhakaran,and EmilyDenton. Crowdworksheets:A ccounting for
individual and collective identities underlying crowdsou rced dataset annota-
tion. In2022 ACM Conference on Fairness, Accountability, and Trans parency,
pages2342–2351,2022.
[52] CarlDiSalvo.Designandtheconstructionofpublics. Designissues ,25(1):48–63,
2009.
[53] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew , Gabriel Ilharco,
Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Docu menting large
webtext corpora: A case study on the colossal clean crawled c orpus.arXiv
preprintarXiv:2104.08758 ,2021.
[54] Robert Dorfman. A formula for the gini coeﬃcient. The review of economics
and statistics ,pages146–149,1979.
[55] Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas Sch iefer, Amanda
Askell, Anton Bakhtin, Carol Chen, Zac Hatﬁeld-Dodds, Dann y Hernandez,
Nicholas Joseph, et al. Towards measuring the representati on of subjective
globalopinions inlanguagemodels. arXiv preprint arXiv:2306.16388 ,2023.
[56] EricFeczko, OscarMiranda-Dominguez,Mollie Marr,Al ice M Graham,Joel T
Nigg,and DamienA Fair. The heterogeneity problem: approac hesto identify
psychiatricsubtypes. Trendsin cognitivesciences ,23(7):584–601,2019.
[57] VirginiaKFelkner,Ho-ChunHerbertChang,Eugene Jang ,and JonathanMay.
Towardswinoqueer: Developing abenchmarkfor anti-queerb iasinlargelan-
guagemodels. arXiv preprint arXiv:2206.11484 ,2022.
[58] JamesS Fishkinand RobertCLuskin. Experimenting with ademocratic ideal:
Deliberativepolling and publicopinion. Acta politica , 40:284–298,2005.
[59] Sorelle A. Friedler, Carlos Scheidegger, and Suresh Ve nkatasubramanian.
On the (im)possibility of fairness. CoRR, abs/1609.07236, 2016. URL
http://arxiv.org/abs/1609.07236.
[60] Iason Gabriel. Artiﬁcial intelligence, values, and al ignment. Minds and Ma-
chines,30(3):411–437,2020.
[61] TripGabriel. Under pressure,teacherstamperwithtes ts.TheNewYorkTimes ,
10, 2010.
[62] KavitaGanesan. Rouge2.0:Updated andimprovedmeasur esforevaluationof
summarizationtasks. arXiv preprintarXiv:1803.01937 ,2018.
[63] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda As kell, Yuntao Bai,
Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse,
et al. Red teaming languagemodels to reduce harms:Methods, scalingbehav-
iors,and lessons learned. arXiv preprint arXiv:2209.07858 ,2022.
[64] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws f or reward model
overoptimization,2022. URL https://arxiv.org/abs/2210 .10760.
[65] Avijit Ghosh, Lea Genuit, and Mary Reagan. Characteriz ing intersectional
groupfairnesswith worst-casecomparisons. InDeepti Lamb aandWilliamH.
Hsu,editors, Proceedings of 2nd Workshopon Diversity in ArtiﬁcialIntell igence
(AIDBEI),volume142of ProceedingsofMachineLearningResearch ,pages22–34.
PMLR,09Feb 2021. URLhttps://proceedings.mlr.press/v14 2/ghosh21a.html.
[66] Charles AE Goodhart. Problems of monetary management: the UK experience .
Springer,1984.
[67] Mitchell L. Gordon, Kaitlyn Zhou, Kayur Patel, Tatsuno ri Hashimoto, and
Michael S. Bernstein. The disagreement deconvolution: Bri nging machine
learning performance metrics in line with reality. In Proceedings of the 2021
CHI Conference on Human Factors in Computing Systems , CHI ’21, New York,
NY, USA, 2021.Association forComputing Machinery.
[68] MitchellL.Gordon,MichelleS.Lam,JoonSungPark,Kay urPatel,JeﬀHancock,
TatsunoriHashimoto,andMichaelS.Bernstein. Jurylearni ng:Integratingdis-
senting voices into machine learning models. In Proceedings of the 2022 CHI
Conference on Human Factors in Computing Systems , CHI ’22, New York, NY,
USA, 2022.Association forComputing Machinery.
[69] Dina Gray, Pietro Micheli, and Andrey Pavlov. Measurement Madness: Recog-
nizing and avoiding the pitfalls of performance measuremen t. John Wiley &Fernando Diaz andMichael Madaio
Sons, 2015.
[70] Mary L Gray and Siddharth Suri. Ghost work: How to stop Silicon Valley from
building anew global underclass . EamonDolanBooks, 2019.
[71] BenGreen. Thecontestationoftechethics:Asociotech nicalapproachtotech-
nology ethics inpractice. Journal of Social Computing ,2(3):209–225,2021.
[72] Artem Grotovand Maartende Rijke. Online learning to ra nk for information
retrieval: Sigir 2016 tutorial. In Proceedings of the 39th International ACM SI-
GIR Conference on Research and Development in Information R etrieval, SIGIR
’16, pages 1215–1218, New York, NY, USA, 2016. Association f or Computing
Machinery.
[73] Christian Haerpfer, Ronald Inglehart, Alejandro More no, Christian Welzel,
Kseniya Kizilova, Jaime Diez-Medrano, Marta Lagos, Pippa N orris, Eduard
Ponarin, BiPuranen,et al. World valuessurvey:roundseven –country-pooled
dataﬁle.Madrid, Spain & Vienna, Austria: JD Systems Institute & WVSA Secre-
tariat, 2020.
[74] DavidJ.Hand. MeasurementTheoryandPractice:TheWorldThroughQuantiﬁ-
cation. Wiley,2010.
[75] Alex Hanna and TinaM. Park. Against scale:Provocation s and resistancesto
scalethinking, 2020. URL https://arxiv.org/abs/2010.08 850.
[76] Eszter Hargittai. Is bigger always better? potential b iases of big data derived
from social network sites. The ANNALS of the American Academy of Political
and Social Science , 659(1):63–76, 2015.
[77] Eszter Hargittai. Potential biases in big data: Omitte d voices on social media.
Social Science ComputerReview , 38(1):10–24, 2020.
[78] Christina Harrington, Sheena Erete, and Anne Marie Pip er. Deconstructing
community-basedcollaborativedesign:Towardsmoreequit able participatory
designengagements. Proceedings ofthe ACMon Human-Computer Interaction ,
3(CSCW):1–25, 2019.
[79] Tatsunori Hashimoto, Megha Srivastava, Hongseok Namk oong, and Percy
Liang. Fairness without demographics in repeated loss mini mization. In Jen-
niferDyandAndreasKrause,editors, Proceedingsofthe35thInternationalCon-
ference on Machine Learning , volume 80 of Proceedings of Machine Learning
Research, pages 1929–1938, Stockholmsmässan, Stockholm Sweden, 10 –15 Jul
2018. PMLR.
[80] JonathanL.Herlocker,JosephA.Konstan,LorenG.Terv een,andJohnT.Riedl.
Evaluatingcollaborativeﬁlteringrecommendersystems. ACMTrans.Inf.Syst. ,
22(1):5–53, jan2004.
[81] JacobHilton, JieTang,and JohnSchulman. Scalinglaws for single-agentrein-
forcement learning,2023. URL https://arxiv.org/abs/230 1.13442.
[82] Geert Hofstede. Dimensionalizing cultures: T he hofst ede model in context.
Online readingsinpsychology and culture ,2(1):2307–0919, 2011.
[83] Ronald Inglehart, Miguel Basanez, Jaime Diez-Medrano , Loek Halman, and
Ruud Luijkx. World values surveys and european values surve ys, 1981-1984,
1990-1993, and 1995-1997. Ann Arbor-Michigan, Institute for Social Research,
ICPSR version ,2000.
[84] Maor Ivgi, Yair Carmon, and Jonathan Berant. Scaling la ws under the micro-
scope:Predictingtransformerperformancefromsmallscal eexperiments,2022.
URL https://arxiv.org/abs/2202.06387.
[85] Abigail Z. Jacobsand Hanna Wallach. Measurementand fa irness. In Proceed-
ingsof the 2021ACMConference onFairness,Accountability ,and Transparency ,
FAccT ’21, page 375–385, New York, NY, USA, 2021. Associatio n for Comput-
ing Machinery. ISBN 9781450383097. doi: 10.1145/3442188. 3445901. URL
https://doi.org/10.1145/3442188.3445901.
[86] Maurice Jakesch, Zana Buçinca, Saleema Amershi, and Al exandra Olteanu.
How diﬀerentgroupsprioritizeethical valuesforresponsi bleai. In 2022ACM
Conference onFairness,Accountability,and Transparency ,pages310–323, 2022.
[87] Thorsten Joachims. Optimizing search engines using cl ickthrough data. In
KDD ’02: Proceedings of the eighth ACM SIGKDD international c onference on
Knowledge discoveryand data mining ,pages133–142, 2002.
[88] AnnaJobin,MarcelloIenca,andEﬀyVayena. Thegloball andscapeofaiethics
guidelines. NatureMachine Intelligence , 1(9):389–399, 2019.
[89] Maryam Kamvar and Shumeet Baluja. A large scale study of wireless search
behavior: Google mobile search. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems , CHI ’06, page 701–709, New York, NY,
USA, 2006. Association for Computing Machinery. ISBN 15959 33727. doi: 10.
1145/1124772.1124877. URLhttps://doi.org/10.1145/112 4772.1124877.
[90] Maryam Kamvar and Shumeet Baluja. Deciphering trends i n mobile search.
Computer , 40(8):58–62, aug 2007. ISSN 0018-9162. doi: 10.1109/MC.2 007.270.
URL https://doi.org/10.1109/MC.2007.270.
[91] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brow n, Ben-
jamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeﬀrey W u, and
Dario Amodei. Scaling laws for neural language models, 2020 . URL
https://arxiv.org/abs/2001.08361.
[92] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steve n Wu. Prevent-
ing fairness gerrymandering: Auditing and learning for sub group fairness.
In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th Inter-
national Conference on Machine Learning , volume 80 of Proceedings of Ma-
chine Learning Research , pages 2564–2572. PMLR, 10–15 Jul 2018. URLhttps://proceedings.mlr.press/v80/kearns18a.html.
[93] Md Anowarul Arif Khan, Md Mostaﬁzur Rahman Khan, Mahmud ul Hassan,
Firoz Ahmed, and Shah Md Rauful Haque. Role of community radi o for com-
munitydevelopmentinbangladesh. TheInternationalTechnologyManagement
Review,6(3):94–102, 2017.
[94] Jinyoung Kim, Gabriella Kazai, and Imed Zitouni. Relev ance dimensions in
preference-based ir evaluation. In Proceedings of the 36th International ACM
SIGIR Conference on Research and Development in Informatio n Retrieval , SIGIR
’13,pages913–916,New York,NY, USA, 2013.Association for Computing Ma-
chinery.
[95] Rob Kitchin. Big data, new epistemologies and paradigm shifts.Big data &
society,1(1):2053951714528481, 2014.
[96] JonM.Kleinberg,SendhilMullainathan,andManishRag havan.Inherenttrade-
oﬀs in the fair determination of risk scores. In Christos H. P apadimitriou,
editor,8th Innovations in Theoretical Computer Science Conferenc e, ITCS 2017,
January 9-11, 2017, Berkeley, CA, USA , volume 67 of LIPIcs, pages 43:1–43:23.
Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2017. d oi: 10.4230/LIPIcs.
ITCS.2017.43. URLhttps://doi.org/10.4230/LIPIcs.ITCS .2017.43.
[97] AllisonKoenecke,AndrewNam,EmilyLake,JoeNudell,M innieQuartey,Zion
Mengesha, Connor Toups, John R Rickford, Dan Jurafsky, and S harad Goel.
Racialdisparitiesinautomatedspeechrecognition. ProceedingsoftheNational
Academyof Sciences ,117(14):7684–7689, 2020.
[98] Ron Kohavi, Diane Tang, and Ya Xu. Trustworthy Online Controlled Experi-
ments:APractical Guide toA/B Testing . CambridgeUniversityPress,2020.
[99] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthful qa: Measuring how
models mimichumanfalsehoods. arXiv preprint arXiv:2109.07958 ,2021.
[100] Alexandra Luccioni and Joseph Viviano. What’s in the b ox? an analysis of
undesirable content in the common crawl corpus. In Proceedings of the 59th
Annual Meeting of the Association for Computational Lingui stics and the 11th
International JointConferenceonNatural LanguageProcess ing(Volume2:Short
Papers), pages182–189,2021.
[101] Michael Madaio, Lisa Egede, Hariharan Subramonyam, J ennifer Wort-
man Vaughan, and Hanna Wallach. Assessing the fairness of ai systems: Ai
practitioners’processes,challenges, and needs for suppo rt.Proceedings of the
ACMon Human-Computer Interaction ,6(CSCW1):1–26, 2022.
[102] Alexandre Magueresse, Vincent Carles, and Evan Heetd erks. Low-resource
languages: A review of past work and future challenges. arXiv preprint
arXiv:2006.07264 ,2020.
[103] Ian McKenzie, Alexander Lyzhov, Alicia Parrish, Amey a Prabhu, Aaron
Mueller, Najoung Kim, Sam Bowman, and Ethan Perez. The inver se scaling
prize,2022. URL https://github.com/inverse-scaling/pr ize.
[104] Brendan McSweeney. Hofstede’s model of national cult ural diﬀerences and
their consequences: A triumph of faith-a failureof analysi s.Human relations ,
55(1):89–118,2002.
[105] JessicaKMiller,BatyaFriedman,GavinJancke,and Br ianGill. Valuetensions
indesign:thevaluesensitivedesign,development, andapp ropriationof acor-
poration’s groupware system. In Proceedings of the 2007 international ACM
conferenceon Supporting group work ,pages281–290, 2007.
[106] Amy Mitchell and Paul Hitlin. Twitter reaction to even ts often at odds with
overallpublicopinion. 2013.
[107] DanielMügge.Economicstatisticsaspoliticalartef acts.Reviewofinternational
political economy ,29(1):1–22, 2022.
[108] Michael J Muller and Allison Druin. Participatory des ign: The third space in
human–computerinteraction. In TheHuman–ComputerInteractionHandbook ,
pages1125–1153.CRCPress,2012.
[109] VanessaMurdockandW.BruceCroft. Taskorientationi nquestionanswering.
InSIGIR’02:Proceedingsofthe25thannualinternationalACMS IGIRconference
onResearchanddevelopmentininformationretrieval ,pages355–356,NewYork,
NY, USA, 2002.ACM.
[110] Mei Ngan, Mei Ngan, Patrick Grother, Kayee Hanaoka, an d Jason Kuo. Face
recognitionvendortest(frvt)part4:Morph-performanceo fautomatedfacemorph
detection. US Department of Commerce, National Institute of Standard s and
Technology, 2020.
[111] Ziad Obermeyer, Brian Powers, Christine Vogeli, and S endhil Mullainathan.
Dissectingracialbiasinanalgorithmusedtomanagethehea lthofpopulations.
Science,366(6464):447–453, 2019.
[112] Kenechi Okeleke and S Suardi. The mobile economy sub-s aharanafrica2021.
GSMAIntelligence Report , 2019.
[113] AnaeliaOvalle,PalashGoyal,JwalaDhamala,Zachary Jaggers,Kai-WeiChang,
Aram Galstyan, Richard Zemel, and Rahul Gupta. “i’m fully wh o i am”: To-
wardscenteringtransgenderandnon-binaryvoicestomeasu rebiasesinopen
language generation. In Proceedings of the 2023 ACM Conference on Fairness,
Accountability,and Transparency ,pages1246–1266, 2023.
[114] Anaelia Ovalle, Arjun Subramonian, Vagrant Gautam, G ilbert Gee, and Kai-
WeiChang. Factoringthematrixofdomination:Acriticalre viewandreimagi-
nationof intersectionality inaifairness. arXiv preprint arXiv:2303.17555 ,2023.
[115] Scott E Page. Where diversity comes from and why it matt ers?European
Journal of Social Psychology ,44(4):267–279, 2014.Scaling Laws DoNot Scale
[116] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishak h Padmakumar, Jason
Phang, JanaThompson, Phu MonHtut, and Samuel RBowman. Bbq: A hand-
built biasbenchmarkfor question answering. arXiv preprint arXiv:2110.08193 ,
2021.
[117] NickPatterson,AlkesLPrice,andDavidReich. Popula tionstructureandeige-
nanalysis. PLoS genetics ,2(12):e190, 2006.
[118] AndrewPerrin.Socialmediausage:2005-2015.Techni calreport,PewResearch
Center,October2015.
[119] TrevorJ Pinch and Wiebe E Bijker. The social construct ion of facts and arte-
facts: Or how the sociology of science and the sociology of te chnology might
beneﬁteach other. Social studiesof science ,14(3):399–441, 1984.
[120] Francesca Polletta. Freedom is an endless meeting. In Freedom Is an Endless
Meeting. Universityof ChicagoPress,2012.
[121] Matt Post. A call for clarity in reporting bleu scores. arXiv preprint
arXiv:1804.08771 ,2018.
[122] JacobPoushter,CaldwellBishop,andHanyuChwe.Soci almediausecontinues
to rise in developing countries but plateaus acrossdevelop ed ones. Technical
report, Pew ResearchCenter,June 2018.
[123] OrganizersOf Queerinai, Anaelia Ovalle, Arjun Subra monian, Ashwin Singh,
Claas Voelcker, Danica J Sutherland, Davide Locatelli, Eva Breznik, Filip Klu-
bicka,HangYuan,etal. Queerinai:Acasestudyincommunity -led participa-
tory ai. In Proceedings of the 2023ACM Conference on Fairness,Accounta bility,
and Transparency ,pages1882–1895,2023.
[124] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Mil lican, Jordan Hoﬀ-
mann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Al bin Cassirer,
Richard Powell, George van den Driessche, Lisa Anne Hendric ks, Maribeth
Rauh,Po-SenHuang,AmeliaGlaese,JohannesWelbl,Sumanth Dathathri,Saf-
fron Huang, Jonathan Uesato, John Mellor, Irina Higgins, An tonia Creswell,
NatMcAleese,AmyWu,ErichElsen,SiddhantJayakumar,Elen aBuchatskaya,
David Budden, Esme Sutherland, Karen Simonyan, Michela Pag anini, Lau-
rent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kunco ro, Aida Ne-
matzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Laz aridou, Arthur
Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikol ai Grigorev, Doug
Fritz, Thibault Sottiaux, Mantas Pajarskas,Toby Pohlen, Z hitao Gong, Daniel
Toyama,CypriendeMassond’Autume, YujiaLi,TayfunTerzi, VladimirMiku-
lik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurel ia Guy, Chris
Jones, James Bradbury, Matthew Johnson, Blake Hechtman, La ura Weidinger,
Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell,
Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeﬀ Stanway, Lorra yne Bennett,
Demis Hassabis, Koray Kavukcuoglu, and Geoﬀrey Irving. Sca ling language
models: Methods, analysis & insights from training gopher, 2021. URL
https://arxiv.org/abs/2112.11446.
[125] Inioluwa Deborah Raji, Emily M Bender, Amandalynne Pa ullada, Emily Den-
ton, and Alex Hanna. Ai and the everything in the whole wide wo rld bench-
mark.arXiv preprint arXiv:2111.15366 ,2021.
[126] Aruna Ranganathan and Alan Benson. A numbers game: Qua ntiﬁcation of
work, auto-gamiﬁcation, and worker productivity. American Sociological Re-
view, 85(4):573–609, 2020.
[127] Ehud Reiter. A structured review of the validity of ble u.Computational Lin-
guistics,44(3):393–401, 2018.
[128] Steﬀen Rendle, Li Zhang, and Yehuda Koren. On the diﬃcu lty of evaluating
baselines:Astudyonrecommendersystems. CoRR,abs/1905.01395,2019. URL
http://arxiv.org/abs/1905.01395.
[129] Ronald E Robertson,Alexandra Olteanu, Fernando Diaz ,MiladShokouhi, and
Peter Bailey. “i can’t replywith that”: Characterizingpro blematicemailreply
suggestions. In Proceedings of the 2021 CHI Conference on Human Factors in
ComputingSystems ,pages1–18,2021.
[130] Esther Rolf, Ben Packer, Alex Beutel, and Fernando Dia z. Striving for data-
modeleﬃciency:Identifyingdataexternalitiesongrouppe rformance.In Work-
shop on Trustworthy and Socially Responsible Machine Learn ing, NeurIPS 2022 ,
2022.
[131] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belin kov, and Nir
Shavit. A constructive prediction of the generalization er ror across
scales. In International Conference on Learning Representations , 2020. URL
https://openreview.net/forum?id=ryenvpEKDr.
[132] Matthew J Salganik and Douglas D Heckathorn. Sampling and estimation in
hidden populations using respondent-driven sampling. Sociological methodol-
ogy, 34(1):193–240,2004.
[133] Nithya Sambasivan,Erin Arnesen, Ben Hutchinson, Tul see Doshi, and Vinod-
kumarPrabhakaran.Re-imaginingalgorithmicfairnessini ndiaandbeyond.In
Proceedings of the 2021ACMConferenceon Fairness,Accounta bility,and Trans-
parency, FAccT ’21,pages 315–328, New York, NY, USA, 2021. Associat ion for
Computing Machinery. ISBN 9781450383097. doi: 10.1145/34 42188.3445896.
URL https://doi.org/10.1145/3442188.3445896.
[134] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and
Tatsunori Hashimoto. Whose opinions do language models reﬂ ect?arXiv
preprint arXiv:2303.17548 ,2023.[135] MaartenSap,DallasCard,SaadiaGabriel,YejinChoi, and NoahA Smith. The
risk of racial bias in hate speech detection. In Proceedings of the 57th annual
meetingof the associationforcomputational linguistics ,pages1668–1678,2019.
[136] AndrewDSelbst,danahboyd,SorelleAFriedler,Sures hVenkatasubramanian,
and Janet Vertesi. Fairness and abstractionin sociotechni cal systems. In Pro-
ceedings of the conference on fairness, accountability, an d transparency , pages
59–68,2019.
[137] Divya Shanmugam, Fernando Diaz, Samira Shabanian, Mi chele Finck, and
Asia Biega. Learning to limit data collection via scaling la ws: A com-
putational interpretation for the legal principle of data m inimization. In
2022 ACM Conference on Fairness, Accountability, and Trans parency, FAccT
’22, pages 839–849, New York, NY, USA, 2022. Association for Computing
Machinery. ISBN 9781450393522. doi: 10.1145/3531146.353 3148. URL
https://doi.org/10.1145/3531146.3533148.
[138] Ori Shapira, Ramakanth Pasunuru, Hadar Ronen, Mohit B ansal, Yael Ams-
terdamer, and Ido Dagan. Extending multi-document summari zation evalu-
ation to the interactive setting. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computationa l Linguistics: Hu-
man Language Technologies , pages 657–677, Online, June 2021. Association
for Computational Linguistics. doi: 10.18653/v1/2021.na acl-main.54. URL
https://aclanthology.org/2021.naacl-main.54.
[139] Kenneth E Shirley and Andrew Gelman. Hierarchical mod els for estimating
state and demographic trends in us death penalty public opin ion.Journal of
theRoyal StatisticalSociety. Series A(Statisticsin Soci ety), pages1–28,2015.
[140] JoarMaxViktor SkalseandAlessandro Abate. Therewar dhypothesis isfalse.
InNeurIPS ML Safety Workshop ,2022.
[141] Joar Max Viktor Skalse, Nikolaus H. R. Howe, Dmitrii Kr asheninnikov,
and David Krueger. Deﬁning and characterizing reward gamin g. In Al-
ice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun C ho, ed-
itors,Advances in Neural Information Processing Systems , 2022. URL
https://openreview.net/forum?id=yb3HOXO3lX2.
[142] MonaSloane, Emanuel Moss,OlaitanAwomolo, and Laura Forlano. Participa-
tion is not a design ﬁx for machine learning. arXiv preprint arXiv:2007.02423 ,
2020.
[143] IreneSolaimanandChristyDennison.Processforadap tinglanguagemodelsto
society(palms) with values-targeteddatasets. Advances in Neural Information
ProcessingSystems ,34:5861–5873,2021.
[144] MarilynStrathern. ‘improvingratings’:audit in the britishuniversitysystem.
European review ,5(3):305–321,1997.
[145] ArjunSubramonian,XingdiYuan,HalDauméIII,andSuL inBlodgett. Ittakes
twototango:Navigatingconceptualizationsofnlptasksan dmeasurementsof
performance. arXiv preprint arXiv:2305.09022 ,2023.
[146] Harini Suresh and John Guttag. A framework for underst anding sources of
harmthroughout the machinelearning lifecycle. In Equityand accessin algo-
rithms,mechanisms,and optimization , pages1–9.2021.
[147] Harini Suresh, Rajiv Movva, Amelia Lee Dogan, Rahul Bh argava, Isadora
Cruxen,ÁngelesMartinezCuba,GuiliaTaurino,WonyoungSo ,andCatherine
D’Ignazio. Towardsintersectional feministand participa toryml:A casestudy
in supporting feminicide counterdata collection. In 2022 ACM Conference on
Fairness,Accountability,and Transparency ,pages667–678,2022.
[148] DiasOlivaThiago, Antonialli Dennys Marcelo,and Ale ssandra Gomes. Fight-
inghatespeech,silencingdragqueens?artiﬁcialintellig enceincontentmoder-
ationand risksto lgbtq voicesonline. Sexuality &culture ,25(2):700–732, 2021.
[149] Rachel L. Thomas and David Uminsky. Reliance on metric s is a fundamental
challenge forai. Patterns,3(5), 2022.
[150] Zeynep Tufekci. Big questions for social media big dat a: Representativeness,
validity and other methodological pitfalls. In Proceedings of the international
AAAIconferenceon weband social media , volume8, pages505–514,2014.
[151] ZegerVanderWalandEThJVanHout. Ispublicvalueplur alismparamount?
the intrinsic multiplicity and hybridity of public values. Intl Journal of Public
Administration ,32(3-4):220–231, 2009.
[152] Ellen M. Voorhees, Nick Craswell, and Jimmy Lin. Too ma ny relevants:
Whithercranﬁeldtestcollections? In Proceedingsofthe45thInternationalACM
SIGIR Conference on Research and Development in Informatio n Retrieval , SIGIR
’22, pages 2970–2980, New York, NY, USA, 2022. Association f or Computing
Machinery.
[153] Claudia Wagner, Markus Strohmaier, Alexandra Oltean u, Emre Kıcıman,
Noshir Contractor, and Tina Eliassi-Rad. Measuring algori thmically infused
societies. Nature, 595(7866):197–204, 2021. doi: 10.1038/s41586-021-0366 6-1.
URLhttps://doi.org/10.1038/s41586-021-03666-1.
[154] ElizabethAnne Watkins,Michael McKenna, and JiahaoC hen. The four-ﬁfths
ruleisnot disparateimpact:awoeful tale of epistemictres passinginalgorith-
micfairness. arXiv preprintarXiv:2202.09519 ,2022.
[155] JasonWei,YiTay,andQuocV.Le. Inversescalingcanbe comeu-shaped,2022.
URLhttps://arxiv.org/abs/2211.02011.
[156] James Wick, David JT Campbell, Finlay A McAlister, Bra den J Manns, Mar-
celloTonelli,ReedFBeall,BrendaRHemmelgarn,AndrewSte wart,andPaulEFernando Diaz andMichael Madaio
Ronksley. Identifying subgroupsof adult high-cost health care users: a retro-
spective analysis. Canadian Medical Association Open Access Journal , 10(2):
E390–E399, 2022.
[157] David Wilkinson and Mike Thelwall. Social network sit e changes over time:
The case of myspace. Journal of the American Society for Information Science
and Technology , 61(11):2311–2323, 2010.
[158] Youngjin Yoo. It is not about size: a further thought on big data. Journal of
Information Technology , 30(1):63–65, 2015.[159] VivianaA Zelizer. Payments and social ties. In Sociological forum ,volume 11,
pages481–495.Springer,1996.
[160] Pan Zhou, Yingtian Zou, Xiao-Tong Yuan, Jiashi Feng, C aiming Xiong, and
StevenHoi.Tasksimilarityawaremetalearning:theory-in spiredimprovement
onMAML. InCassiodeCamposandMarloesH.Maathuis,editors ,Proceedings
oftheThirty-SeventhConferenceonUncertaintyinArtiﬁci alIntelligence ,volume
161ofProceedingsofMachineLearningResearch ,pages23–33.PMLR,27–30Jul
2021. URL https://proceedings.mlr.press/v161/zhou21a. html.